<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>编程语言 on 元视角</title>
    <link>http://localhost:1313/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/</link>
    <description>Recent content in 编程语言 on 元视角</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sun, 09 Mar 2025 20:42:23 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Semantic Kernel × MCP：智能体的上下文增强探索</title>
      <link>http://localhost:1313/posts/semantic-kernel-mcp-agent-context-enhanced-exploration/</link>
      <pubDate>Sun, 09 Mar 2025 20:42:23 +0000</pubDate>
      <guid>http://localhost:1313/posts/semantic-kernel-mcp-agent-context-enhanced-exploration/</guid>
      <description>时光飞逝，转眼间已步入阳春三月，可我却迟迟未曾动笔写下 2025 年的第一篇 AI 博客。不知大家心中作何感想，从年初 DeepSeek 的爆火出圈，到近期 Manus 的刷屏热议，AI 领域的发展可谓是日新月异。例如，DeepSeek R1 的出现，让人们开始接受慢思考，可我们同样注意到，OpenAI 的 Deep Research 选择了一条和 R1 截然不同的路线，模型与智能体之间的界限开始变得模糊。对于这一点，使用过 Cursor Composer 或者 Deep Research 的朋友，相信你们会有更深刻的感悟。有人说，Agent 会成为 2025 年的 AI 主旋律。我不知道大家是否清楚 AutoGPT 与 Manus 的差别，对我个人而言，最重要的事情是在喧嚣过后找到 “值得亲手去做的事情”。所以，今天这篇博客，我想分享一个 “熟悉而陌生” 的东西：MCP，即：模型上下文协议，并尝试将这个协议和 Semantic Kernel 连接起来。&#xA;MCP 介绍 [TL;DR] MCP 是由 Anthropic 设计的开放协议，其定位类似于 AI 领域的 USB 接口，旨在通过统一接口解决大模型连接不同数据源和工具的问题。该协议通过 JSON-RPC 规范定义了 Prompt 管理、资源访问和工具调用三大核心能力，使得任何支持 Function Calling 的模型都能无缝对接外部系统，从而帮助大语言模型实现 “万物互联”。&#xA;什么是 MCP? MCP（Model Context Protocol）是由 Anthropic 设计的一种开放协议，旨在标准化应用程序向大语言模型（LLMs）提供上下文的方式，使大模型能够以统一的方法连接各种数据源和工具。你可以将其理解为 AI 应用的 USB 接口，为 AI 模型连接到不同的数据源和工具提供了标准化的方法。架构设计上，MCP 采用了经典的 C/S 架构，客户端可以使用该协议灵活地连接多个 MCP Server，从而获取丰富的数据和功能支持，如下图所示：</description>
    </item>
    <item>
      <title>基于 K-Means 聚类分析实现人脸照片的快速分类</title>
      <link>http://localhost:1313/posts/face-photo-fast-classification-using-k-means-clustering/</link>
      <pubDate>Tue, 14 Jan 2025 12:52:10 +0000</pubDate>
      <guid>http://localhost:1313/posts/face-photo-fast-classification-using-k-means-clustering/</guid>
      <description>注：本文在创作过程中得到了 ChatGPT、DeepSeek、Kimi 的智能辅助支持，由作者本人完成最终审阅。&#xA;在 “视频是不能 P 的” 系列文章中，博主曾先后分享过人脸检测、人脸识别等相关主题的内容。今天，博主想和大家讨论的是人脸分类问题。你是否曾在人群中认错人，或是盯着熟人的照片却一时想不出对方的名字？这种 “脸盲症” 的困扰，不仅在生活中令人感到尴尬，在整理照片时更是让人头疼不已。想象一下，某次聚会结束后，你的手机里存了上百张照片——有你的笑脸、朋友的自拍，甚至还有一部分陌生面孔混杂其中。手动将这些照片按人物分类，不仅费时费力，还可能会因为 “脸盲” 而频繁出错。此时，你是否期待有一种技术，可以像魔法一样，自动将这些照片按人物分类？事实上，这种 “魔法” 已经存在，它的名字叫做 K-Means 聚类分析。作为一种经典的无监督学习算法，K-Means 能够通过分析人脸特征，自动将相似的面孔归类到一起，完全无需人工干预。接下来，为了彻底根治 “脸盲症”，我们将详细介绍如何使用 K-Means 聚类分析来实现这一目标，哈利·波特拥有魔法，而我们则拥有科技。&#xA;实现过程 如图所示，我们将按照下面的流程来达成 “自动分类人脸” 这一目标。其中，Dlib 负责提取人脸特征向量、Scikit-Learn 中的 K-Means 负责聚类分析、Matplotlib 负责结果的可视化：&#xA;基于 K-Means 聚类分析实现人脸照片的快速分类示意图&#xD;K-Means 简介 K-Means 是一种广泛应用的聚类算法，其基本原理是将数据集分成 K 个簇，目标是让每个簇内的数据点尽可能相似，而不同簇之间的数据点尽可能差异明显。K-Means 的执行过程如下：&#xA;随机选取 K 个初始中心点。&#xA;将每个数据点分配到距离最近的中心点所对应的簇。&#xA;更新每个簇的中心点，通常取簇内所有数据点的均值。&#xA;重复步骤 2 和 3，直到中心点不再发生变化或达到预设的最大迭代次数。&#xA;如下图所示，图中展示了四种不同的聚类数据分布情况，按照从左到右、自上而下的顺序：&#xA;图一：簇划分不正确或者簇数量假设错误 图二：数据分布具有各向异性，簇的形状是一个拉长的椭圆形，而不是对称的圆形 图三：各个簇之间的方差不同，绿色簇分布更紧密，而黄色簇分布更稀疏 图四：簇的大小不均匀，黄色簇数据点较少，而紫色簇数据点较多 四种不同的聚类数据分布情况&#xD;因此，适用于 K-Means 的数据通常满足：&#xA;簇是球状且分布均匀 簇的大小相近 簇无明显噪声点或者离群点 数据是各向同性分布 簇的数量已知 数据维度适中 如何确定 K 值 在使用 K-Means 之前，我们需要确定 K 值，即簇的数量。下面是三种常用的确定 K 值的方法：</description>
    </item>
    <item>
      <title>基于 LLaMA 和 LangChain 实践本地 AI 知识库</title>
      <link>http://localhost:1313/posts/practice-local-ai-knowledg-base-based-on-llama-and-langchain/</link>
      <pubDate>Thu, 29 Feb 2024 10:30:47 +0000</pubDate>
      <guid>http://localhost:1313/posts/practice-local-ai-knowledg-base-based-on-llama-and-langchain/</guid>
      <description>有时候，我难免不由地感慨，真实的人类世界，本就是一个巨大的娱乐圈，即使是在英雄辈出的 IT 行业。数日前，Google 正式对外发布了 Gemini 1.5 Pro，一个建立在 Transformer 和 MoE 架构上的多模态模型。可惜，这个被 Google 寄予厚望的产品并未激起多少水花，因为就在同一天 OpenAI 发布了 Sora，一个支持从文字生成视频的模型，可谓是一时风光无二。有人说，OpenAI 站在 Google 的肩膀上，用 Google 的技术疯狂刷屏。此中曲直，远非我等外人所能预也。我们唯一能确定的事情是，通用人工智能，即：AGI（Artificial General Intelligence）的实现，正在以肉眼可见的速度被缩短，以前在科幻电影中看到的种种场景，或许会比我们想象中来得更快一些。不过，等待 AGI 来临前的黑夜注定是漫长而孤寂的。在此期间，我们继续来探索 AI 应用落地的最佳实践，即：在成功部署本地 AI 大模型后，如何通过外挂知识库的方式为其 “注入” 新的知识。&#xA;从 RAG &amp;amp; GPTs 开始 在上一期博客中，博主曾经有一个困惑，那就是当前阶段 AI 应用的最佳实践到底是什么？站在 2023 年的时间节点上，博主曾经以为未来属于提示词工程(Prompt Engineering)，而站在 2024 年的时间节点上，博主认为 RAG &amp;amp; GPTs 在实践方面或许要略胜一筹。在过去的一年里，我们陆陆续续看到像 Prompt Heroes、PromptBase、AI Short&amp;hellip;等等这样的提示词网站出现，甚至提示词可以像商品一样进行交易。与此同时，随着 OpenAI GPT Store 的发布，我们仿佛可以看到一种 AI 应用商店的雏形。什么是 GPTs 呢？通常是指可以让使用者量身定做 AI 助理的工具。譬如，它允许用户上传资料来丰富 ChatGPT 的知识库，允许用户使用个性化的提示词来指导 ChatGPT 的行为，允许用户整合各项技能(搜索引擎、Web API、Function Calling)&amp;hellip;等等。我们在上一期博客中提到人工智能的 “安卓时刻”，一个重要的契机是目前产生了类似应用商店的 GPT Store，如下图所示：</description>
    </item>
    <item>
      <title>为你的服务器集成 LDAP 认证</title>
      <link>http://localhost:1313/posts/integrate-ldap-authentication-for-your-server/</link>
      <pubDate>Tue, 15 Nov 2022 12:49:47 +0000</pubDate>
      <guid>http://localhost:1313/posts/integrate-ldap-authentication-for-your-server/</guid>
      <description>回顾我这些年的工作经历，面向企业(2B)和面向用户(2C)的项目都曾接触过。我个人觉得，面向企业的项目更注重业务，参与决策的人数多、周期长，目的是为企业提供生产经营价值，如缩减成本、提升效率等等，而面向用户的项目更注重体验，参与决策的人数少、周期短，目的是为消费者提供更多的使用价值，本质上是为了圈揽用户和抢夺流量。我在参与这些项目的过程中发现，企业级应用的研发更注重与第三方软件如 SAP、金蝶、用友、ERP 等等的整合，因此，类似单点登录、数据同步这样的需求非常普遍。每当这个时候，我就不由地想起一位前辈。&#xA;时间就像沙漏里的沙一样流逝&#xD;当我还在 Automation 打杂的时候，前辈总是一脸得意地问我：“听说过 AD Domain 吗？”。那时，初出茅庐的我年少轻狂，不好意思说我不会，立马敷衍道：“当然听说过，只是一直没用过”。前辈目光如炬，大抵是看出我心虚，立马不屑一顾地回应道：“那就是不会”。过了几秒钟，前辈不紧不慢地接着说道：“只有学会了 AD Domain，你才算是一只脚踏进了企业级应用开发这个领域，知道吗？”，我点了点头，心道：“这不就和茴香豆的茴字有五种写法一样无聊吗？”。多年后，当 LDAP 这个字眼再次映入眼帘的时候，我内心终于清楚地知道：我错了。&#xA;为什么需要 LDAP 认证 我错在哪里了呢？我想，要回答这个问题，还是需要从企业管理的角度来着手。一个面向用户(2C)的产品，其用户基本上是不受地域因素限制的，而对于一个面向企业(2B)的产品，其用户基本上是在一个层次分明、有着明显边界的范围内。运营一个企业，除了业务系统以外，可能还需要 OA、财务、ERP 等等外围软件的支持，如果是一家互联网公司，可能还需要 DevOps、监控、协作等等方面的支撑。此时，从企业的角度自然是希望可以统一账号体系，这样就衍生出了各种各样的单点登陆和认证方案，单单是博主接触过的就有：OAuth2、CAS、Keycloak、IdentityServer4，这些方案可以说是各有千秋，此中曲折我们按下不表。&#xA;运行在 Windows Server 上的 AD&#xD;这里博主想说的是，一旦企业通过 AD Domain 或者说 Active Directory 来管理用户，就自然而然地牵扯出域登录或者域账号登录的问题。这类围绕 AD Domain 或者说域的问题，我们都可以考虑使用 LDAP 认证或者 Kerberos 认证，特别是后者，主流的软件如 Kafka、Zookeeper、MySQL 等等均支持这一协议，它可以实现在登录本地账户后，免登录打开一个网站的效果。可想而知，这是一个对企业而言极具诱惑力的特性，一个账号打通所有基础设施。当然，我承认 Kerberos 这个协议是非常复杂的，绝非三言两语可以厘清其中的千丝万缕，所以，我们今天只是聊聊 LDAP 认证这个话题。&#xA;通过 LDAP Browser 访问 AD&#xD;可能大家会纠结，LDAP 和 Active Directory 这两者间的关系，事实上， LDAP 是指轻量目录访问协议(Lightweight Directory Access Protocol)，而 Active Directory 则是微软针对该协议的一种实现。当然，微软为了解决域控的问题，利用 LDAP 存储了一部分私有的数据。所以，两者的关系就像是接口和实现类，我们这里只需要 Active Directory 当成一台 LDAP 服务器即可。关于 Active Directory 的基础知识，这里不再做更多的科普。总而言之，通过 LDAP 我们可以对某个网站实现认证，从而达到保护资源的目的。譬如博主目前参与的前端项目，它是没有常规的登录、注册页面的，它采用的就是域账号登录的形式。下面，我们来看看如何集成 LDAP 认证。</description>
    </item>
    <item>
      <title>视频是不能 P 的系列：OpenCV 和 Dlib 实现表情包</title>
      <link>http://localhost:1313/posts/make-memes-with-opencv-and-dlib/</link>
      <pubDate>Fri, 01 Jul 2022 22:49:47 +0000</pubDate>
      <guid>http://localhost:1313/posts/make-memes-with-opencv-and-dlib/</guid>
      <description>2020 年年底的时候，博主曾心血来潮地开启过一个系列：视频是不能 P 的，其灵感则是来源于互联网上的一个梗，即：视频不能 P 所以是真的。不过，在一个美颜盛行的时代，辨别真伪实在是一件奢侈的事情，在各种深度学习框架光环的加持下，在视频中实现“改头换面”已然不再是新鲜事儿，AI 换脸风靡一时的背后，带来是关乎隐私和伦理的一系列问题，你越来越难以确认，屏幕对面的那个到底是不是真实的人类。古典小说《红楼梦》里的太虚幻境，其牌坊上有幅对联写道，“假作真时真亦假，无为有处有还无”。果然，在这个亦真亦幻的世界里，哪里还有什么东西是不能 PS 的呢？在“鸽”了很久很久之后，博主决定要来更新这个系列啦，让我们继续以 OpenCV 作为起点，来探索那些好玩、有趣的视频/图像处理思路，这一次呢，我们来聊聊 OpenCV、Dlib 和 表情包，希望寓教于乐的方式能让大家感受到编程的快乐！&#xA;环境准备 python -m pip install opencv-python python -m pip install opencv-contrib-python python -m pip install Pillow python -m pip install numpy python -m pip install imutils python -m pip install dlib 请注意，如果通过 pip 安装 dlib 不大顺利，你可以到 https://github.com/sachadee/Dlib 这个仓库中下载对应的 .whl 文件。例如，博主使用的是 64 位的 Windows 系统，而我的 Python 版本是 3.7，因此，我下载的是 dlib-19.22.99-cp37-cp37m-win_amd64.whl 这个文件。此时，我们可以用下面的方式来安装 dlib：&#xA;python -m pip install dlib-19.22.99-cp37-cp37m-win_amd64.whl 除此以外，我们还需要下载 dlib 所需的模型文件，下载地址为：http://dlib.</description>
    </item>
    <item>
      <title>ASP.NET Core 搭载 Envoy 实现微服务的监控预警</title>
      <link>http://localhost:1313/posts/1519021197/</link>
      <pubDate>Sat, 10 Jul 2021 14:41:24 +0000</pubDate>
      <guid>http://localhost:1313/posts/1519021197/</guid>
      <description>在构建微服务架构的过程中，我们会接触到服务划分、服务编写以及服务治理这一系列问题。其中，服务治理是工作量最密集的一个环节，无论是服务发现、配置中心、故障转移、负载均衡、健康检查……等等，这一切的一切，本质上都是为了更好地对服务进行管理，尤其是当我们面对数量越来越庞大、结构越来越复杂的集群化环境的时候，我们需要一种科学、合理的管理手段。博主在上一家公司工作的时候，每次一出现线上故障，研发都要第一时间对问题进行排查和处理，而当时的运维团队，对于微服务的监控止步于内存和CPU，无法系统而全面的掌握微服务的运行情况，自然无法从运维监控的角度给研发部门提供方向和建议。所以，今天这篇文章，博主想和大家聊聊，如何利用Envoy来对微服务进行可视化监控。需要说明的是，本文的技术选型为Envoy + ASP.NET Core + Prometheus + Grafana，希望以一种无侵入的方式集成到眼下的业务当中。本文源代码已上传至 Github ，供大家学习参考。&#xA;从 Envoy 说起 在介绍 Envoy 的时候，我们提到了一个词，叫做可观测的。什么叫可观测的呢？官方的说法是， Envoy 内置了stats模块，可以集成诸如prometheus/statsd等监控方案，可以集成分布式追踪系统，对请求进行追踪。对于这个说法，是不是依然有种云里雾里的感觉？博主认为，这里用Metrics这个词会更准确点，即可度量的，你可以认为， Envoy 提供了某种可度量的指标，通过这些指标我们可以对 Envoy 的运行情况进行评估。如果你使用过 Elastic Stack 中的 Kibana，就会对指标(Metrics)这个词汇印象深刻，因为 Kibana 正是利用日志中的各种指标进行图表的可视化的。庆幸的是，Grafana 中拥有与 Kibana 类似的概念。目前， Envoy 中支持三种类型的统计指标：&#xA;Counter：即计数器，一种只会增加不会减少的无符号整数。例如，总请求数 Gauge：即计量，一种可以同时增加或者同时减少的无符整数。例如，状态码为200的有效请求数 Timer/Hitogram：即计时器/直方图，一种无符号整数，最终将产生汇总百分位值。Envoy 不区分计时器（通常以毫秒为单位）和 原始直方图（可以是任何单位）。 例如，上游请求时间（以毫秒为单位）。 在今天的这篇文章中，除了 Envoy 以外，我们还需要两位新朋友的帮助，它们分别是Prometheus 和 Grafana。其中，Prometheus 是一个开源的完整监控解决方案，其对传统监控系统如 Nagios、Zabbix 等的测试和告警模型进行了彻底的颠覆，形成了基于中央化的规则计算、统一分析和告警的新模型。可以说，Prometheus 是完整监控解决方案中当之无愧的后起之秀，它最为人所称道的是它强大的数据模型，在 Prometheus 中所有采集到的监控数据吗，都以指标(Metrics)的形式存储在时序数据库中。和传统的关系型数据库中使用的 SQL 不同，Prometheus 定义一种叫做 PromQL 的查询语言，来实现对监控数据的查询、聚合、可视化、告警等功能。&#xA;Prometheus &amp;amp;amp; Grafana 的奇妙组合&#xD;目前，社区中提供了大量的第三方系统的采集功能的实现，这使得我们可以轻易地对MySQL、PostgresSQL、Consul、HAProxy、RabbitMQ， Redis等进行监控。而 Grafana 则是目前主流的时序数据展示工具，正是因为这个原因， Grafana 总是和 Prometheus 同时出现， Prometheus 中采集到监控数据以后，就可以由 Grafana 赖进行可视化。相对应地，Grafana 中有数据源的概念，除了 Prometheus 以外，它还可以使用来自 Elasticsearch 、InfluxDB 、MySQL 、OpenTSDB 等等的数据。基于这样一种思路，我们需要 Envoy 提供指标信息给 Prometheus ，然后再由 Grafana 来展示这些信息。所以，我们面临的主要问题，其实是怎么拿到 Envoy 中的指标信息，以及怎么把这些指标信息给到 Prometheus 。</description>
    </item>
    <item>
      <title>ASP.NET Core 搭载 Envoy 实现微服务的负载均衡</title>
      <link>http://localhost:1313/posts/3599307336/</link>
      <pubDate>Mon, 05 Jul 2021 22:49:47 +0000</pubDate>
      <guid>http://localhost:1313/posts/3599307336/</guid>
      <description>如果说，我们一定要找出一个词来形容这纷繁复杂的世界，我希望它会是熵。有人说，熵增定律是宇宙中最绝望的定律，所谓熵，即是指事物混乱或者无序的程度。在一个孤立系统下，熵是不断在增加的，当熵达到最大值时，系统就会出现严重混乱，直至最终走向死亡。从某种意义上来讲，它揭示了事物结构衰退的必然性，甚至于我们的人生，本来就是一场对抗熵增的旅程。熵增的不可逆性，正如时光无法倒流一般，古人说，“覆水难收”正是这个道理。同样地，当我们开始讨论微服务的划分/编写/治理的时候，当我们使用服务网格来定义微服务架构的时候……我们是否有意或者无意的增加了系统中的熵呢？**一个孤立的系统尚且会因为熵增而最终走向死亡，更何况是相互影响和制约的复杂系统呢？**现代互联网企业都在追求4个9(即99.99%)的高可用，这意味着年平均停机时长只有52.56分钟。在此之前。我们介绍过重试和熔断这两种故障转移的策略，而今天我们来介绍一种更朴素的策略：负载均衡。&#xA;什么是负载均衡 负载均衡，即Load Banlancing，是一种计算机技术，用来在多个计算机(计算机集群)、网络连接、CPU、磁盘驱动器或其它资源中分配负载，以达到最优化资源使用、最大化吞吐率、最小化响应时间、避免过载的目的。&#xA;我们可以注意到，在这个定义中，使用负载均衡技术的直接原因是避免过载，而根本原因则是为了优化资源使用，确保最大吞吐量、最小响应时间。所以，这本质上是一个局部最优解的问题，而具体的手段就是&amp;quot;多个&amp;quot;。有人说，技术世界不过是真实世界的一个镜像，联系生活中实际的案例，我们发现负载均衡比比皆是。譬如车站在面对春运高峰时增加售票窗口，银行通过多个业务窗口来为客户办理业务……等等。这样做的好处显而易见，可以大幅度地减少排队时间，增加&amp;quot;窗口&amp;quot;这个行为，在技术领域我们将其称为：水平扩展，因为有多个&amp;quot;窗口&amp;quot;，发生单点故障的概率就会大大降低，而这正是现在软件追求的三&amp;quot;高&amp;quot;：高性能、高可用、高并发。&#xA;银行柜员窗口示意图&#xD;每次坐地铁经过小寨，时常听到地铁工作人员举着喇叭引导人们往不同的出口方向走动。此时，工作人员就是一个负载均衡器，它要做的就是避免某一个出口人流量过载。**从熵的角度来看，人流量过载，意味着无序/混乱状态加剧，现代社会通过道德和法律来对抗熵增，人类个体通过自律来对抗熵增。**有时候，我会忍不住去想，大人与小孩儿愈发内卷的恶性竞争，除了给这个世界带来更多的熵以外，还能带来什么？如果参考社会达尔文主义的理论，在这个弱肉强食的世界里，增加熵是人为的选择，而同样的，你亦可以选择&amp;quot;躺平&amp;quot;。&#xA;负载均衡器示意图&#xD;OK，将思绪拉回到负载均衡，它所做的事情，本质上就是控制信息或者说流量流动的方向。一个网站，以集群的方式对外提供服务，你只需要输入一个域名，它就可以把请求分发到不同的机器上面去，而这就是所谓的负载均衡。目前，负载均衡器从种类上可以分为：基于DNS、基于MAC地址(二层)、基于IP(三层)、基于IP和Port(四层)、基于HTTP(七层)。&#xA;OSI七层模型与TCP/IP五层模型&#xD;譬如，博主曾经参与过伊利的项目，它们使用的就是一个四层的负载均衡器：F5。而像更常见Nginx、HAProxy，基本都是四层和七层的负载均衡器，而Envoy就厉害了，它可以同时支持三/四/七层。负载均衡器需要配合负载均衡算法来使用，典型的算法有：轮询法、随机法、哈希法、最小连接数法等等，而这些算法都可以结合加权算法引出新的变式，这里就不再一一列举啦。&#xA;Envoy中的负载均衡 通过上一篇博客，我们已经了解到，Envoy中一个HTTP请求的走向，大致会经历：客户端、侦听器(Listeners)、集群(Clusters)、终结点(Endpoints)、服务(ervices)这样几个阶段。其中，一个集群可以有多个终结点(Endpoints)。所以，这里天然地就存在着负载均衡的设计。因为，负载均衡本质上就是告诉集群，它应该选择哪一个终结点(Endpoints)来提供服务。而之所以我们需要负载均衡，一个核心的原因，其实是因为我们选择了分布式。&#xA;Envoy架构图：负载均衡器连接集群和服务&#xD;如果类比RabbitMQ、Kafka和Redis，你就会发现，这些产品中或多或少地都会涉及到主(Leader)、从(Follower)以及推举Leader的实现，我个人更愿意将其看作是更广义的负载均衡。最直观的，它可以分担流量，简称分流，不至于让某一台服务器满负荷做运行。其次，它可以作为故障转移的一种方案，人生在世，多一个B计划，就多一种选择。同样地，多一台服务器，就多一分底气。最后，它可以指导某一个产品或者功能的推广，通过给服务器设置不同的权重，在必要的时候，将流量局部地导入某一个环境，腾讯和阿里这样的大厂，经常利用这种方式来做灰度测试。&#xA;Envoy中支持常用的负载均衡算法，譬如：ROUND_ROBIN(轮询)、LEAST_REQUEST(最少请求)、RING_HASH(哈希环)、RANDOM(随机)、MAGLEV(磁悬浮)、CLUSTER_PROVIDED等等。因为一个集群下可以有多个终结点，所以，在Envoy中配置负载均衡，本质上就是在集群下面增加终结点，而每个终结点则会对应一个服务，特殊的点在于，这些服务可能是通过同一个Dockerfile或者Docker镜像来构建的。所以，一旦理解了这一点，Envoy的负载均衡就再没有什么神秘的地方。例如，下面的代码片段展示了，如何为WeatherService这个集群应用负载均衡：&#xA;clusters: # Weather Service - name: weatherservice connect_timeout: 0.25s type: STRICT_DNS # ROUND_ROBIN(轮询） # LEAST_REQUEST(最少请求) # RING_HASH(哈希环) # RANDOM(随机) # MAGLEV(磁悬浮) # CLUSTER_PROVIDED lb_policy: LEAST_REQUEST load_assignment: cluster_name: weatherservice endpoints: - lb_endpoints: - endpoint: address: socket_address: address: weatherservice1 port_value: 80 - endpoint: address: socket_address: address: weatherservice2 port_value: 80 是不是觉得特别简单？我想说，也许是Envoy更符合人的直观感受一些，理解起来本身没有太大的心智负担。最近看到一个缓存设计，居然还要依赖Kafka，使用者为了使用缓存这个功能，就必须先实现三个丑陋的委托，这就是所谓的心智负担，违背人类的直觉，使用缓存为什么要了解Kafka？到这里，你大概就能了解利用Envoy实现负载均衡的思路，首先是用同一个Dockerfile或者Docker镜像启动多个不同容器(服务)，然后将指定集群下面的终结点指定不同的服务，再告诉集群要用哪一种负载均衡策略即可。&#xA;邂逅 ASP.NET Core OK，说了这么多，这里我们还是用ASP.NET Core写一个例子。可以预见到的是，我们需要一个Envoy网关，一个ASP.NET Core的服务。这里，我们还是用Docker-Compose来编排这些服务，下面是对应的docker-compose.yaml文件：</description>
    </item>
    <item>
      <title>ASP.NET Core 搭载 Envoy 实现微服务的反向代理</title>
      <link>http://localhost:1313/posts/3599307335/</link>
      <pubDate>Thu, 01 Jul 2021 22:49:47 +0000</pubDate>
      <guid>http://localhost:1313/posts/3599307335/</guid>
      <description>回想起来，博主第一次接触到Envoy，其实是在微软的示例项目 eShopOnContainers，在这个示例项目中，微软通过它来为Ordering API、Catalog API、Basket API 等多个服务提供网关的功能。当时，博主并没有对它做深入的探索。此刻再度回想起来，大概是因为那个时候更迷恋领域驱动设计(DDD)的理念。直到最近这段时间，博主需要在一个项目中用到Envoy，终于决定花点时间来学习一下相关内容。所以，接下来这几篇博客，大体上会以记录我学习Envoy的历程为主。考虑到Envoy的配置项特别多，在写作过程中难免会出现纰漏，希望大家谅解。如对具体的配置项存在疑问，请以官方最新的 文档 为准，本文所用的示例代码已经上传至 Github，大家作为参考即可。对于今天这篇博客，我们来聊聊 ASP.NET Core 搭载 Envoy 实现微服务的反向代理 这个话题，或许你曾经接触过 Nginx 或者 Ocelot，这次我们不妨来尝试一点新的东西，譬如，通过Docker-Compose来实现服务编排，如果对我说的这些东西感兴趣的话，请跟随我的脚步，一起来探索这广阔无垠的技术世界吧！&#xA;走近 Envoy Envoy 官网对Envoy的定义是：&#xA;Envoy 是一个开源边缘和服务代理，专为原生云应用设计。&#xA;而更进一步的定义是：&#xA;Envoy 是专为大型现代服务导向架构设计的 L7 代理和通讯总线。&#xA;这两个定义依然让你感到云里雾里？没关系，请看下面这张图：&#xA;Envoy架构图&#xD;注：图片来源&#xA;相信从这张图中，大家多少能看到反向代理的身影，即下游客户端发起请求，Envoy对请求进行侦听(Listeners)，并按照路由转发请求到指定的集群(Clusters)。接下来，每一个集群可以配置多个终结点，Envoy按照指定的负载均衡算法来筛选终结点，而这些终结点则指向了具体的上游服务。例如，我们熟悉的 Nginx ，使用listen关键字来指定侦听的端口，使用location关键字来指定路由，使用proxy_pass关键字来指定上游服务的地址。同样地，Ocelot 使用了类似的上下游(Upstream/Downstream)的概念，唯一的不同是，它的上下游的概念与这里是完全相反的。&#xA;你可能会说，这个Envoy看起来“平平无奇”嘛，简直就像是“平平无奇”的古天乐一般。事实上，Envoy强大的地方在于：&#xA;非侵入式的架构： 独立进程、对应用透明的Sidecar模式 Envoy 的 Sidecar 模式&#xD;L3/L4/L7 架构：Envoy同时支持 OSI 七层模型中的第三层(网络层, IP 协议)、第四层(传输层，TCP / UDP 协议)、第七层(应用层，HTTP 协议) 顶级 HTTP/2 支持： 视 HTTP/2 为一等公民，且可以在 HTTP/2 和 HTTP/1.1间相互转换 gRPC 支持：Envoy 支持 HTTP/2，自然支持使用 HTTP/2 作为底层多路复用协议的 gRPC 服务发现和动态配置：与 Nginx 等代理的热加载不同，Envoy 可以通过 API 接口动态更新配置，无需重启代理。 特殊协议支持：Envoy 支持对特殊协议在 L7 进行嗅探和统计，包括：MongoDB、DynamoDB 等。 可观测性：Envoy 内置 stats 模块，可以集成诸如 prometheus/statsd 等监控方案。还可以集成分布式追踪系统，对请求进行追踪。 Envoy配置文件 Envoy通过配置文件来实现各种各样的功能，其完整的配置结构如下：</description>
    </item>
    <item>
      <title>使用 HttpMessageHandler 实现 HttpClient 请求管道自定义</title>
      <link>http://localhost:1313/posts/2070070822/</link>
      <pubDate>Wed, 28 Apr 2021 20:25:47 +0000</pubDate>
      <guid>http://localhost:1313/posts/2070070822/</guid>
      <description>最近，博主偶然间在 博客园 看到一篇文章：ASP.NET Core 扩展库之 Http 请求模拟，它里面介绍了一种利用 HttpMessageHandler 来实现 Http 请求模拟的方案。在日常工作中，我们总是不可避免地要和第三方的服务或者接口打交道，尤其是当我们需要面对“联调”这样一件事情的时候。通常，我们可以通过类似 YAPI 这样的工具来对尚在开发中的接口进行模拟。可是，因为这种方式会让我们的测试代码依赖于一个外部工具，所以，从严格意义上讲，它其实应该属于“集成测试”的范畴。在接触前端开发的过程中，对于其中的 Mock.js 印象深刻。故而，当看到 .NET 中有类似实现的时候，好奇心驱使我对其中的核心，即 HttpMessageHandler 产生了浓厚的兴趣。平时，我们更多的是使用 Moq 这样的库来模拟某一个对象的行为，而对一个 Http 请求进行模拟，可以说是开天辟地头一遭。带着这些问题出发，就有了今天这篇博客，通过 HttpMessageHandler 实现 HttpClient 请求管道的自定义。&#xA;什么是 HttpMessageHandler？ 相信大家读过我提到的文章以后，都能找到这里面最核心的一个点：HttpMessageHandler。于是，我们今天要面对的第一个问题就是，什么是 HttpMessageHandler？此时，我们需要一张历久弥新的示意图，来自 微软官方。这里，我们重点关注的是 DelegatingHandler，它继承自 HttpMessageHandler。通过这张图，我们能够获得哪些信息呢？&#xA;我认为，主要有以下几点：第一，HttpMessageHandler 处于整个 Http 请求管道的第一梯队，每一个路由匹配的请求都会从这里“进入”和“离开”；第二，HttpMessageHandler 可以是全局配置或者针对某个特定的路由，只要这个路由被匹配到就会执行；第三，HttpMessageHandler 可以直接构造 Http 响应并且返回，跳过剩余的管道流程。不知道大家看到这里会想到什么？坦白讲，我联想到了.NET Core 中的中间件，而唯一不同的地方或许是，中间件是 ASP.NET Core 里的概念，这里则是 ASP.NET Web API 里的概念。尤其是第三点，它对于我们的意义非常重大，因为它，我们才可以做到对一个 Http 请求进行模拟。&#xA;HttpMessageHandler 与 ASP.NET Web API&#xD;而事实上，在 ASP.NET Web API 的设计中，它是由一组 HttpMessageHandler 经过“首尾相连”而成，这种管道式的设计使得框架本身具有很高的扩展性。虽然，作为一个服务端框架，ASP.NET Web API 最主要的作用是就是“处理请求、响应回复”，可具体采用的处理策略会因具体场景的不同而不同。所以，管道式设计的本质，就是让某一个 Handler 只负责某个单一的消息处理功能，在根据具体场景的不同，选择需要的 Handler 并将其串联成一个完整的消息处理通道。而在这里，这个负责单一的消息处理功能的 Handler 其实就是 HttpMessageHandler，因为它不单单可以对请求消息(HttpRequestMessage)进行处理，同时还可以对响应消息(HttpResponseMessage)进行处理。此时，我们就不难理解 HttpMessageHandler 的定义：</description>
    </item>
    <item>
      <title>ABP vNext 对接 Ant Design Vue 实现分页查询</title>
      <link>http://localhost:1313/posts/3670340170/</link>
      <pubDate>Wed, 07 Apr 2021 21:07:47 +0000</pubDate>
      <guid>http://localhost:1313/posts/3670340170/</guid>
      <description>在 上一篇 博客中，博主和大家分享了如何在 EF Core 中实现多租户架构。在这一过程中，博主主要参考了 ABP vNext 这个框架。从上个月开始，我个人发起了一个项目，基于 ABP vNext 和 Ant Design Vue 来实现一个通用的后台管理系统，希望以此来推进 DDD 和 Vue 的学习，努力打通前端与后端的“任督二脉”。因此，接下来的这段时间内，我写作的主题将会围绕 ABP vNext 和 Ant Design Vue。而在今天的这篇博客中，我们来说说 ABP vNext 对接 Ant Design Vue 实现分页查询的问题，希望能让大家在面对类似问题时有所帮助。我不打算写一个系列教程，更多的是从我个人的关注点出发，如果大家有更多想要交流的话题，欢迎大家通过评论或者邮件来留言，谢谢大家！&#xA;ABP vNext中的分页查询 OK，当大家接触过 ABP vNext 以后，就会了解到这样一件事情，即，ABP vNext 中默认提供的分页查询接口，在大多数情况下，通常都会是下面这样的风格。这里以角色查询的接口为例，它对应的请求地址是：/api/identity/roles?SkipCount=0&amp;amp;MaxResultCount=10。此时，我们可以注意到，返回的数据结构中含有totalCount和items两个属性。其中，totalCount表示记录的总数目，items表示当前页对应的记录。&#xA;{ &amp;#34;totalCount&amp;#34;: 2, &amp;#34;items&amp;#34;: [ { &amp;#34;name&amp;#34;: &amp;#34;Admin&amp;#34;, &amp;#34;isDefault&amp;#34;: false, &amp;#34;isStatic&amp;#34;: true, &amp;#34;isPublic&amp;#34;: true, &amp;#34;concurrencyStamp&amp;#34;: &amp;#34;cb53f2d7-159e-452d-9d9c-021629b500e0&amp;#34;, &amp;#34;id&amp;#34;: &amp;#34;39fb19e8-fb34-dfbd-3c70-181f604fd5ff&amp;#34;, &amp;#34;extraProperties&amp;#34;: {} }, { &amp;#34;name&amp;#34;: &amp;#34;Manager&amp;#34;, &amp;#34;isDefault&amp;#34;: false, &amp;#34;isStatic&amp;#34;: false, &amp;#34;isPublic&amp;#34;: false, &amp;#34;concurrencyStamp&amp;#34;: &amp;#34;145ec550-7fe7-4c80-85e3-f317a168e6b6&amp;#34;, &amp;#34;id&amp;#34;: &amp;#34;39fb6216-2803-20c6-7211-76f8fe38b90e&amp;#34;, &amp;#34;extraProperties&amp;#34;: {} } ] } 事实上，ABP vNext 中自带的分页查询，主要是通过SkipCount和MaxResultCount两个参数来实现。假设MaxResultCount，即分页大小为m，则第n页对应的SkipCount应该为(n-1) * m。如果大家对于LINQ非常熟悉的话，应该可以自然而然地联想到Skip()和Take()两个方法，这是一个非常自然的联想，因为 ABP vNext 就是这样实现分页查询的。这里以博主的“数据字典”分页查询接口为例：</description>
    </item>
    <item>
      <title>低代码，想说爱你不容易</title>
      <link>http://localhost:1313/posts/2637069146/</link>
      <pubDate>Mon, 15 Feb 2021 12:37:47 +0000</pubDate>
      <guid>http://localhost:1313/posts/2637069146/</guid>
      <description>一直想写篇文章，聊一聊“低代码”这个话题。一方面，“低代码”这个概念确实非常火，其热度丝毫不亚于曾经的“中台”。有人说，2021 年是属于“云原生”的时代，看起来我们每一年都在被技术的“娱乐圈”抛弃，明明连 Kubernetes 都还没有入门呢？人们已然在欢呼雀跃般地声称要抛弃 Docker 。这个世界有时就是如此地魔幻，明明我们生活在一个拥有大量基础设施的时代，我们不必再像前辈们“刀耕火种”一般地去开发软件，可我们的生存空间为什么就越来越狭窄了呢？拼多多事件过去没有多久，腾讯的阳光普照奖再次让“打工魂”觉醒，也许果真像大鱼海棠里设定的一样，人的记忆只有 7 秒。而另一方面，我想结合我最近开发“工作流”的感受，来吐槽下这个看起来美好的“低代码”。也许，对企业而言，引入“低代码”的确能减少研发成本，可博主并不认为，它会降低业务本身的复杂性，如果所有声称“低代码”或者“无代码”的项目，最终依然需要研发人员来作为收场。对此，我想说，对不起，这不是我想要的“低代码”。&#xA;低代码发展现状 或许，一个人成熟的标志就是，在面对一个未知的事物的时候，决不会不由分说地一通吐槽，就像一个人在职场上，你不能永远都只是学会抱怨，相对于抱怨，人们更希望听到的是解决方案。所以，一个人的成长，本质上就是不断学会为自己、为别人找解决方案的过程，前者是为了认识自我，而后者是为了交换资源。所以，在听我吐槽“低代码”前，不妨先一起来看看低代码的发展现状。&#xA;低代码产品发展现状&#xD;国外趋势 有人认为，“低代码”的兴起源于钉钉的低代码应用 易搭 的落地。诚然，巨头企业的每一个动向都引领着整个行业的风潮，可低代码这个概念最早要追溯到 1980 年。彼时，IBM 的快速应用程序开发工具(RAD)被冠以新的名字——低代码，这是低代码这个概念首次面向大众，此后的 40 年里，国外诞生了诸如 Outsystem 、Mendix 、 Zoho Creator 等等的产品，整体发展相对缓慢。直到 2015 年以后，AWS、Google、Microsoft 和 Oracle 等巨头开始入局低代码领域。2018 年，西门子更是宣布以 6 亿欧元收购低代码应用开发领域的领导者 Mendix 、快速应用开发的低代码平台 Outsystem 获得 3.6 亿美金的投资，低代码平台市场开始火爆起来，我们所熟悉的 Power Platform，其实就是微软的低代码开发平台，低代码领域通常都需要大量的积累和研发，需要有 10 到 20 年左右的技术沉淀。&#xA;国内风云 国内的低代码领域，相比国外发展起步较晚，可依然涌现出像牛刀、APICloud、iVX、搭搭云、氚云、简道云、云表、宜搭云等等产品。从整体上而言，这类这类产品基本上都提供了可视化搭建环境，都声称无需编码即可完成业务系统的搭建。其实，从一名程序员的初心出发，我们所做的一切努力都是为了以后不写代码。经常有人问，怎么样可以做到零缺陷、零 Bug ，其实不写代码就好啦！我们并不担心低代码让我们失业，相反地，如果低代码可以消化掉 30% 的垃圾项目，那么，我们将会有更多的时间去做些有意义的事情，而不是在一个“劣币驱逐良币”的市场里，靠着 996 来争个你死我活。而从低代码的商业价值角度来看，Salesforce、Appian、Joget 这三家公司均已上市，Mendix 和 Outsystem 更是估值 10 亿美元以上的独角兽公司，这正是巨头们入局低代码的原因所在。&#xA;低代码领域，目前关注的重点主要集中在：表单生成和处理、工作流生成和管理、办公协作、人力资源、客户关系、ERP 等企业应用上，就如同 SAP 、金蝶、 SCM 等企业软件一样，每一个软件都曾声称能帮助企业解决某一类问题，低代码领域同样遵循“二八原则”，即 80% 的场景，通过定义的方法论、方式、工具集能够实现；而剩下的 20% 的场景或许实现不了，需要使用者通过扩展的方式来自行解决。譬如，针对大多数企业都存在的 CRUD 的需求，通过在线的 Excel 表格来实现基于表的业务驱动。例如 SeaTable 就是这类主打协同工作的产品；针对大多数企业都存在的审批类的需求，则可以通过可视化的工作流设计系统来完成。例如 葡萄城 的 SpreadJS 和 活字格 ，同样可以视为低代码平台，甚至早期的 .</description>
    </item>
    <item>
      <title>一道 HashSet 面试题引发的蝴蝶效应</title>
      <link>http://localhost:1313/posts/3411909634/</link>
      <pubDate>Tue, 20 Oct 2020 12:19:02 +0000</pubDate>
      <guid>http://localhost:1313/posts/3411909634/</guid>
      <description>没错，我又借着“面试题”的名头来搞事情了，今天要说的是 HashSet ，而这确实是一个实际面试中遇到的问题。当时的场景大概是这样的，面试官在了解了你的知识广度以后，决心来考察一番你的基本功底，抛出了一个看起来平平无奇的问题：说一说你平时工作中都用到了哪些数据结构。你心想，这还不简单，Array、ArrayList、List、Dictionary、HashSet、Stack、Queue&amp;hellip;等等各种集合类简直如数家珍，甚至你还能说出这些数据结构间的优劣以及各自使用的场景。可没想到，面试官话锋一转，直接来一句，“你能说说 HashSet 去重的原理吗”，好家伙，你这简直不按套路出牌啊&amp;hellip;本着每次面试都有一点收获的初心，于是就有了今天这篇博客，不同的是，顺着这个思路继续深挖下去，博主又发现了几个平时关注不到的技术盲点，所以，博主称之为：一道 HashSet 面试题引发的蝴蝶效应。&#xA;HashSet 源代码解读 OK，首先，我们来回答第一个问题，即：HashSet 去重的原理是什么？。为此，博主翻阅了 HashSet 的 源代码。首先，我们会注意到 HashSet 的构造函数，它需要一个类型为IEqualityComparer&amp;lt;T&amp;gt;的参数。从这个命名上我们就可以知道，这是一个用于相等性比较的接口，我们初步推测，HashSet 去重应该和这个接口有关：&#xA;public HashSet() : this(EqualityComparer&amp;lt;T&amp;gt;.Default) { } public HashSet(int capacity) : this(capacity, EqualityComparer&amp;lt;T&amp;gt;.Default) { } public HashSet(IEqualityComparer&amp;lt;T&amp;gt; comparer) { } public HashSet(IEnumerable&amp;lt;T&amp;gt; collection) : this(collection, EqualityComparer&amp;lt;T&amp;gt;.Default) { } public HashSet(IEnumerable&amp;lt;T&amp;gt; collection, IEqualityComparer&amp;lt;T&amp;gt; comparer) : this(comparer) { } 我们都知道 HashSet 可以去重，比如，我们向 HashSet 添加多个相同的元素，实际上 HashSet 中最终只会有一个元素。所以，我们自然而然地想到，看看 HashSet 中的 Add() 方法呗，或许能从这里看出一点端倪。HashSet 中一共有两个 Add() 方法，它们内部都调用了 AddIfNotPresent() 方法：&#xA;void ICollection&amp;lt;T&amp;gt;.</description>
    </item>
    <item>
      <title>使用 Dynamic Linq 构建动态 Lambda 表达式</title>
      <link>http://localhost:1313/posts/118272597/</link>
      <pubDate>Fri, 08 May 2020 12:27:11 +0000</pubDate>
      <guid>http://localhost:1313/posts/118272597/</guid>
      <description>相信大家都有这样一种感觉，Linq和Lambda是.NET 中一以贯之的存在，从最早的 Linq to Object 到 Linq to SQL，再到 EF/EF Core 甚至如今的.NET Core，我们可以看到Lambda表达式的身影出现地越来越频繁。虽然 Linq to Object 和 Linq to SQL，分别是以IEnumerable&amp;lt;T&amp;gt;和IQueryable &amp;lt;T&amp;gt;为基础来实现的。我个人以为，Lambda呢，其实就是匿名委托的“变种”，而Linq则是对Lambda的进一步封装。在System.Linq.Expressions命名空间下，提供大量关于表达式树的 API，而我们都知道，这些表达式树最终都会被编译为委托。所以，动态创建 Lambda 表达式，实际上就是指从一个字符串生成对应委托的过程，而一旦这个委托被生成，可以直接传递给 Where()方法作为参数，显然，它可以对源数据进行过滤，这正是我们想要的结果。&#xA;事出有因 在今天这篇博客中，我们主要介绍System.Linq.Dynamic.Core这个库，即我所说的 Dynamic Linq。本着“艺术源于生活的态度”，在介绍它的用法之前，不妨随博主一起看看，一个“简单“的查询是如何随着业务演进而变得越来越复杂。从某种意义上来说，正是它让博主想起了 Dynamic Linq。我们为客户编写了一个生成订单的接口，它从一张数据表中“消费”订单数据。最开始，它只需要过滤状态为“未处理”的记录，对应的 CRUD 可以表示为这样：&#xA;var orderInfos = repository.GetByQuery&amp;lt;tt_wg_order&amp;gt;(x =&amp;gt; x.STATUS == 10); 后来，因为业务方存在重复/错误下单的情况，业务数据有了“软删除”的状态，相应地查询条件再次发生变化，这看起来还行对吧：&#xA;var orderInfos = repository.GetByQuery&amp;lt;tt_wg_order&amp;gt;(x =&amp;gt; x.STATUS == 10 &amp;amp;&amp;amp; x.Isdelete == 0); 再后来，因为接口处理速度不理想，无法满足客户的使用场景，公司大佬们建议“加机器”，而为了让每台服务器上消费的订单数据不同(据说是为了避免发生并发)，大佬们要求博主开放所有字段作为查询条件，这样，每台服务器上可以配置不同查询条件。自此，又双叒叕改：&#xA;var repository = container.Resolve&amp;lt;CrudRepositoryBase&amp;gt;(); var searchParameters = new SearchParameters() { PageInfo = new PageInfo() { PageSize = parameters.</description>
    </item>
    <item>
      <title>从 .NET Core 2.2 升级到 3.1 的踩坑之旅</title>
      <link>http://localhost:1313/posts/3099575458/</link>
      <pubDate>Wed, 22 Jan 2020 10:23:08 +0000</pubDate>
      <guid>http://localhost:1313/posts/3099575458/</guid>
      <description>有时候，版本更新太快并不是一件好事。虽然，两周一个迭代的“敏捷”开发依然被客户嫌弃交付缓慢，可一边是前端领域“求不要再更新了，学不动了”的声音，一边则是.NET Core从1.x到2.x再到3.x的高歌猛进。版本更新太快，带来的是API的频繁变动，无法形成有效的知识沉淀，就像转眼到了2020年，Python 2.x和Windows 7都引来了“寿终正寝”，可能你都还没有认真地学习过这些知识，突然就被告知这些知识要过期了，想想还是觉得挺疯狂啊。最近一直在捣鼓，如何让.NET Core应用跑在Heroku平台上，因为Docker镜像里使用最新的.NET Core 3.1运行时，所以，痛定思痛之余，决定把手头项目升级到3.1。上一次痛苦还是在2.1升级2.2，这还真没过多长时间。所以呢，这篇博客主要梳理下从2.2升级到3.1过程中遇到的问题。&#xA;更新项目文件 调整目标框架为netcoreapp3.1 删除引用项：Microsoft.AspNetCore.App、Microsoft.AspNetCore.Razor.Design 删除AspNetCoreHostingModel，如果项目文件中的值为InProcess(因为ASP.NET Core 3.0 或更高版本项目默认为进程内承载模型） 更新程序入口 CreateWebHostBuilder()方法的返回值类型由IWebHostBuilder调整为IHostBuilder 增加引用项：Microsoft.Extensions.Hosting Kestrel配置变更至ConfigureWebHostDefaults()方法 public static IHostBuilder CreateWebHostBuilder(string[] args) =&amp;gt;&#xD;Host.CreateDefaultBuilder(args)&#xD;.ConfigureWebHostDefaults(webBuilder =&amp;gt;&#xD;{&#xD;webBuilder.ConfigureKestrel(serverOptions =&amp;gt;&#xD;{&#xD;// Set properties and call methods on options&#xD;})&#xD;.UseStartup&amp;lt;Startup&amp;gt;();&#xD;}); 如果通过 HostBuilder手动创建宿主，则需要在 ConfigureWebHostDefaults()方法中显式调用·UseKestrel()：&#xA;public static void Main (string[] args) {&#xD;var host = new HostBuilder ()&#xD;.UseContentRoot (Directory.GetCurrentDirectory ())&#xD;.ConfigureWebHostDefaults (webBuilder =&amp;gt; {&#xD;webBuilder.UseKestrel (serverOptions =&amp;gt; {&#xD;// Set properties and call methods on options&#xD;})&#xD;.</description>
    </item>
    <item>
      <title>Vue 快速实现通用表单验证</title>
      <link>http://localhost:1313/posts/169430744/</link>
      <pubDate>Fri, 06 Sep 2019 14:53:46 +0000</pubDate>
      <guid>http://localhost:1313/posts/169430744/</guid>
      <description>本文开篇第一句话，想引用鲁迅先生《祝福》里的一句话，那便是：“我真傻，真的，我单单知道后端整天都是 CRUD，我没想到前端整天都是 Form 表单”。这句话要从哪里说起呢？大概要从最近半个月的“全栈工程师”说起。项目上需要做一个城市配载的功能，顾名思义，就是通过框选和拖拽的方式在地图上完成配载。博主选择了前后端分离的方式，在这个过程中发现：首先，只要有依赖 jQuery 的组件，譬如 Kendoui，即使使用了 Vue，依然需要通过 jQuery 去操作 DOM。其次，只有有通过 Rozar 生成的 DOM，譬如 HtmlHelper，Vue 的双向绑定就突然变得尴尬起来，更不用说，Rozar 中的@语法和 Vue 中的@指令相互冲突的问题，原本可以直接用 v-for 生成列表，因为使用了 HtmlHelper，突然一下子变得厌恶起来，虽然 Rozar 语法非常强大，可我依然没有在 JavaScript 里写 C#的热情，因为实在太痛苦啦 Orz……&#xA;所以，想做好前后端分离，首先需要分离出一套前端组件库，做不到这一点，前后端分离就无从谈起，就像我们公司的项目，即使框架切换到.NET Core，可是在很长的一段时间里，我们其实还是再写 MVC，因为所有的组件都是后端提供的 HtmlHelper/TagHelper 这种形式。我这次做项目的过程中，其实是通过 jQuery 实现了一部分组件，正因为如此，一个在前后端不分离时非常容易实现的功能，在前后端分离以后发现缺好多东西，就比如最简单的表单验证功能，即便你是在做一个新项目，为了保证产品在外观上的一致性，你还是得依赖老项目的东西，所以，这篇博客主要想说说前后端分离以后，Vue 的时代怎么去做表单的验证。因为我不想测试同事再给我提 Bug，问我为什么只有来自后端接口的验证，而没有来自前端页面的验证。我希望，在写下这篇博客之前，我可以实现和老项目一模一样的表单验证。如同 CRUD 之于后端，80%的前端都是在写 Form 表单，所以，这个事情还是挺有意思的。&#xA;最简单的表单验证 OK，作为国内最接“地气”的前端框架，Vue 的文档可以说是相当地“亲民”啦！为什么这样说呢，因为其实在官方文档中，尤大已经提供了一个表单验证的示例，这个示例让我想起给某银行做自动化工具时的情景，因为这两者都是采用 MVVM 的思想，所以，理解起来是非常容易的，即：通过一个列表来存储错误信息，而这个错误信息会绑定到视图层，所以，验证的过程其实就是向这个列表里添加错误信息的过程。我们一起来看这个例子：&#xA;&amp;lt;div&amp;gt; &amp;lt;h2&amp;gt;你好，请登录&amp;lt;/h2&amp;gt; &amp;lt;div&amp;gt; &amp;lt;form id=&amp;#34;loginFrom&amp;#34;&amp;gt; &amp;lt;div&amp;gt; &amp;lt;label&amp;gt;邮箱&amp;lt;/label&amp;gt; &amp;lt;input type=&amp;#34;text&amp;#34; class=&amp;#34;form-control&amp;#34; id=&amp;#34;inputEmail3&amp;#34; placeholder=&amp;#34;Email&amp;#34; v-model=&amp;#34;email&amp;#34;&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div&amp;gt; &amp;lt;label&amp;gt;密码&amp;lt;/label&amp;gt; &amp;lt;input type=&amp;#34;password&amp;#34; class=&amp;#34;form-control&amp;#34; id=&amp;#34;inputPassword3&amp;#34; placeholder=&amp;#34;Password&amp;#34; v-model=&amp;#34;password&amp;#34;&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div&amp;gt; &amp;lt;button type=&amp;#34;button&amp;#34; class=&amp;#34;btn btn-default login&amp;#34; v-on:click=&amp;#34;login()&amp;#34;&amp;gt;登录&amp;lt;/button&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div v-if=&amp;#34;errorList.</description>
    </item>
    <item>
      <title>使用 ASP.NET Core 和 Hangfire 实现 HTTP 异步化方案</title>
      <link>http://localhost:1313/posts/1071063696/</link>
      <pubDate>Thu, 04 Jul 2019 08:56:28 +0000</pubDate>
      <guid>http://localhost:1313/posts/1071063696/</guid>
      <description>Hi，大家好，我是 Payne，欢迎大家一如既往地关注我的博客。今天这篇博客里的故事背景，来自我工作中的一次业务对接，因为客户方提供的是长达上百行的 XML，所以一度让更喜欢使用 JSON 的博主感到沮丧，我这里不是想讨论 XML 和 JSON 彼此的优缺点，而是我不明白 AJAX 里的 X 现在基本都被 JSON 替代了，为什么还有这么多的人坚持使用并友好的 XML 作为数据的交换协议呢？也许你会说，因为有这样或者那样等等的理由，就像 SOA、ESB、SAP 等等类似的技术在企业级用户依然大量流行一样，而这些正是“消费”XML 的主力军。我真正想说的是，在对接这类接口时，我们会遇到一个异步化的 HTTP 协议场景，这里的异步和多线程、async/await 没有直接关系，因为它描述的实际上是业务流程上的一种“异步”。&#xA;引子-想对 XML 说不 我们知道，HTTP 协议是一个典型的请求-响应模型，由调用方(Client)调用服务提供者(Server)提供的接口，在理想状态下，后者在处理完请求后会直接返回结果。可是当后者面对的是一个“耗时”任务时，这种方式的问题就立马凸显出来，此时调用者有两个选择：一直等对方返回直至超时(同步)、隔一会儿就看看对方是否处理完了(轮询)。这两种方式，相信大家都非常熟悉了，如果继续延伸下去，我们会联想到长/短轮询、SignalR、WebSocket。其实，更好的方式是，我们接收到一个“耗时”任务时，立即返回表明我们接收了任务，等任务执行完以后再通知调用者，这就是我们今天要说的 HTTP 异步化方案。因为对接过程中，客户采用的就是这种方案，ESB 这类消息总线本身就提供了这种功能，可作为调用方的博主就非常难受啦，因为明明能“同步”地处理完的事情，现在全部要变成“异步”处理，就像一个习惯了 async/await 语法糖的人，突然间就要重新开始写 APM 风格的代码，宝宝心里苦啊，“异步”处理就异步处理嘛，可要按人家要求去返回上百行的 XML，博主表示想死的心都有了好嘛……&#xA;好了，吐槽归吐槽，吐槽完我们继续梳理下 HTTP 异步化的方案，这种方式在现实生活中还是相当普遍的，毕竟人类都是“异步”做事，譬如“等你哪天有空一起吃个饭”，测试同事对我说得最多的话就是，“等你这个 Bug 改完了同我说一声”，更不用说，JavaScript 里典型的异步单线程的应用等等……实现“异步”的思路其实是非常多的，比如同样在 JavaScript 里流行的回调函数，比如通过一张中间表存起来，比如推送消息到消息队列里。在面向数据库编程的时候，我听到最多的话就是，没有什么问题是不能用一张中间表来解决的，如果一张不行那就用两张。项目上我是用 Quartz+中间表的方式实现的，因为这是最为普通的方式。这里，我想和大家分享下，关于使用 Hangfire 来实现类似 Quartz 定时任务的相关内容，果然，我这次又做了一次标题党呢，希望大家会对今天的内容感兴趣。简单来说，我们会提供一个接口，调用方提供参数和回调地址，调用后通过 Hangfire 创建后台任务，等任务处理结束后，再通过回调地址返回结果给调用方，这就是所谓的 HTTP 异步化。&#xA;开箱即用的 Hangfire 我们项目上是使用 Quartz 来实现后台任务的，因为它采用了反射的方式来调用具体的 Job，因此，它的任务调度和任务实现是耦合在同一个项目里的，常常出现单个 Job 引发整个系统卡顿的情况，尤其是是它的触发器，常常导致一个 Job 停都停不下来，直到后来才渐渐开始通过 Web API 来分离这两个部分。Quartz 几乎没有一个自己的可视化界面，我们为此专门为它开发了一套 UI。我这里要介绍的 Hangfire，可以说它刚好可以作为 Quartz 的替代品，它是一个开箱即用的、轻量级的、开源后台任务系统，想想以前为 Windows 开发定时任务，只能通过定时器(Timer)来实现，尚不知道 CRON 为何物，而且只能用命令行那种拙劣的方式来安装/卸载，我至今都记得，测试同事问我，能不能不要每次都弹个黑窗口出来，这一起想起来还真是让人感慨啊。好了，下面我们开始今天的实践吧！首先，第一步自然是安装 Hangfire 啦，这里我们新建一个 ASP.</description>
    </item>
    <item>
      <title>使用 .NET Core 和 Vue 搭建 WebSocket 聊天室</title>
      <link>http://localhost:1313/posts/1989654282/</link>
      <pubDate>Wed, 01 Aug 2018 15:42:23 +0000</pubDate>
      <guid>http://localhost:1313/posts/1989654282/</guid>
      <description>Hi，大家好，我是Payne，欢迎大家关注我的博客，我的博客地址是：https://qinyuanpei.github.io。今天这篇博客，我们来说说WebSocket。各位可能会疑惑，为什么我会突然间对WebSocket感兴趣，这是因为最近接触到了部分“实时”的业务场景，譬如：用户希望在远程视频通话过程中，实时地监控接入方的通话状态，实时地将接入方的响应时间、通话时长以及接通率等信息推送到后台。与此同时，用户可以通过监控平台看到实时变化着的图表。坦白地讲，这种业务场景陌生吗？不，每一年的双11，都能见到小伙伴们实时地“剁手”。所以，在今天这篇文章中，我们会以WebSocket聊天室为例，来讲解如何基于WebSocket构建实时应用。&#xA;WebSocket概述 WebSocket是HTML5标准中的一部分，从Socket这个字眼我们就可以知道，这是一种网络通信协议。WebSocket是为了弥补HTTP协议的不足而产生的，我们知道，HTTP协议有一个重要的缺陷，即：请求只能由客户端发起。这是因为HTTP协议采用了经典的请求-响应模型，这就限制了服务端主动向客户端推送消息的可能。与此同时，HTTP协议是无状态的，这意味着连接在请求得到响应以后就关闭了，所以，每次请求都是独立的、上下文无关的请求。这种单向请求的特点，注定了客户端无法实时地获取服务端的状态变化，如果服务端的状态发生连续地变化，客户端就不得不通过“轮询”的方式来获知这种变化。毫无疑问，轮询的方式不仅效率低下，而且浪费网络资源，在这种背景下，WebSocket应运而生。&#xA;WebSocket协议最早于2008年被提出，并于2011年成为国际标准。目前，主流的浏览器都已经提供了对WebSocket的支持。在WebSocket协议中，客户端和服务器之间只需要做一次握手操作，就可以在客户端和服务器之间实现双向通信，所以，WebSocket可以作为**服务器推送**的实现技术之一。因为它本身以HTTP协议为基础，所以对HTTP协议有着更好的兼容性，无论是通信效率还是传输的安全性都能得到保证。WebSocket没有同源限制，客户端可以和任意服务器端进行通信，因此具备通过一个单一连接来支持上下游通信的能力。从本质上来讲，WebSocket是一个在握手阶段使用HTTP协议的TCP/IP协议，换句话说，一旦握手成功，WebSocket就和HTTP协议再无瓜葛，下图展示了它与HTTP协议的区别：&#xA;HTTP与WebSocket的区别&#xD;构建一个聊天室 OK，在对WebSocket有了一个基本的认识以后，接下来，我们以一个最简单的场景来体验下WebSocket。这个场景是什么呢？你已经知道了，答案就是网络聊天室。这是一个非常典型的实时场景。这里我们分为服务端实现和客户端实现，其中：服务端实现自豪地采用.NET Core，而客户端实现采用Vue的双向绑定特性。现在是公元2018年了，当jQuery已成往事，操作DOM这种事情交给框架去做就好，而且我本人很喜欢MVVM这种模式，Vue的渐进式框架，非常适合我这种不会写ES6的伪前端。&#xA;.NET Core与中间件 关于.NET Core中对WebSocket的支持，这里主要参考了官方文档，在这篇文档中，演示了一个最基本的Echo示例，即服务端如何接收客户端消息并返回消息给客户端。这里，我们首先需要安装Microsoft.AspNetCore.WebSockets这个库，直接通过Visual Studio Code内置的终端安装即可。接下来，我们需要在Startup类的Configure方法中添加WebSocket中间件：&#xA;app.UseWebSockets() 更一般地，我们可以配置以下两个配置，其中，KeepAliveInterval表示向客户端发送Ping帧的时间间隔；ReceiveBufferSize表示接收数据的缓冲区大小：&#xA;var webSocketOptions = new WebSocketOptions() { KeepAliveInterval = TimeSpan.FromSeconds(120), ReceiveBufferSize = 4 * 1024 }; app.UseWebSockets(webSocketOptions); 好了，那么怎么接收一个来自客户端的请求呢？这里以官方文档中的示例代码为例来说明。首先，我们需要判断下请求的地址，这是客户端和服务端约定好的地址，默认为**/，这里我们以/ws为例；接下来，我们需要判断当前的请求上下文是否为WebSocket请求，通过context.WebSockets.IsWebSocketRequest来判断。当这两个条件同时满足时，我们就可以通过context.WebSockets.AcceptWebSocketAsync()**方法来得到WebSocket对象，这样就表示“握手”完成，这样我们就可以开始接收或者发送消息啦。&#xA;if (context.Request.Path == &amp;#34;/ws&amp;#34;) { if (context.WebSockets.IsWebSocketRequest) { WebSocket webSocket = await context.WebSockets.AcceptWebSocketAsync(); //TODO } }); 一旦建立了Socket连接，客户端和服务端之间就可以开始通信，这是我们从Socket中收获的经验，这个经验同样适用于WebSocket。这里分别给出WebSocket发送和接收消息的实现，并针对代码做简单的分析。&#xA;private async Task SendMessage&amp;lt;TEntity&amp;gt;(WebSocket webSocket, TEntity entity) { var Json = JsonConvert.SerializeObject(entity); var bytes = Encoding.UTF8.GetBytes(Json); await webSocket.SendAsync( new ArraySegment&amp;lt;byte&amp;gt;(bytes), WebSocketMessageType.</description>
    </item>
    <item>
      <title>使用 Mono 让.NET 程序跨平台运行</title>
      <link>http://localhost:1313/posts/1836680899/</link>
      <pubDate>Sun, 06 Mar 2016 12:20:09 +0000</pubDate>
      <guid>http://localhost:1313/posts/1836680899/</guid>
      <description>&lt;p&gt;众所周知，Unity3D 引擎凭借着强大的跨平台能力而备受开发者的青睐，在跨平台应用开发渐渐成为主流的今天，具备跨平台开发能力对程序员来说就显得特别重要。传统的针对不同平台进行开发的方式常常让开发者顾此失彼，难以保证应用程序在不同的平台都有着相同的、出色的体验，这种情况下寻找到一种跨平台开发的方式将会为解决这个问题找到一种思路。从目前的开发环境来看，Web 应该是最有可能成为跨平台开发的神兵利器，可是长期以来 Web 开发中前端和后端都有各自不同的工作流，虽然现在出现了前端和后端逐渐融合的趋势，可在博主看来想让 Web 开发变得像传统开发这样简单还需要一定的过渡期。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>

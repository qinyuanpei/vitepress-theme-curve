{
    "posts": [
        {
            "title": "Semantic Kernel × MCP：智能体的上下文增强探索",
            "date": "2025-03-09",
            "permalink": "http://localhost:1313/posts/semantic-kernel-mcp-agent-context-enhanced-exploration/",
            "summary": "时光飞逝，转眼间已步入阳春三月，可我却迟迟未曾动笔写下 2025 年的第一篇 AI 博客。不知大家心中作何感想，从年初 DeepSeek 的爆火出圈，到近期 Manus 的刷屏热议，AI 领域的发展可谓是日新月异。例如，DeepSeek R1 的出现，让人们开始接受慢思考，可我们同样注意到，OpenAI 的 Deep Research 选择了一条和 R1 截然不同的路线，模型与智能体之间的界限开始变得模糊。对于这一点，使用过 Cursor Composer 或者 Deep Research 的朋友，相信你们会有更深刻的感悟。有人说，Agent 会成为 2025 年的 AI 主旋律。我不知道大家是否清楚 AutoGPT 与 Manus 的差别，对我个人而言，最重要的事情是在喧嚣过后找到 “值得亲手去做的事情”。所以，今天这篇博客，我想分享一个 “熟悉而陌生” 的东西：MCP，即：模型上下文协议，并尝试将这个协议和 Semantic Kernel 连接起来。\nMCP 介绍 [TL;DR] MCP 是由 Anthropic 设计的开放协议，其定位类似于 AI 领域的 USB 接口，旨在通过统一接口解决大模型连接不同数据源和工具的问题。该协议通过 JSON-RPC 规范定义了 Prompt 管理、资源访问和工具调用三大核心能力，使得任何支持 Function Calling 的模型都能无缝对接外部系统，从而帮助大语言模型实现 “万物互联”。\n什么是 MCP? MCP（Model Context Protocol）是由 Anthropic 设计的一种开放协议，旨在标准化应用程序向大语言模型（LLMs）提供上下文的方式，使大模型能够以统一的方法连接各种数据源和工具。你可以将其理解为 AI 应用的 USB 接口，为 AI 模型连接到不同的数据源和工具提供了标准化的方法。架构设计上，MCP 采用了经典的 C/S 架构，客户端可以使用该协议灵活地连接多个 MCP Server，从而获取丰富的数据和功能支持，如下图所示：",
            "content": "时光飞逝，转眼间已步入阳春三月，可我却迟迟未曾动笔写下 2025 年的第一篇 AI 博客。不知大家心中作何感想，从年初 DeepSeek 的爆火出圈，到近期 Manus 的刷屏热议，AI 领域的发展可谓是日新月异。例如，DeepSeek R1 的出现，让人们开始接受慢思考，可我们同样注意到，OpenAI 的 Deep Research 选择了一条和 R1 截然不同的路线，模型与智能体之间的界限开始变得模糊。对于这一点，使用过 Cursor Composer 或者 Deep Research 的朋友，相信你们会有更深刻的感悟。有人说，Agent 会成为 2025 年的 AI 主旋律。我不知道大家是否清楚 AutoGPT 与 Manus 的差别，对我个人而言，最重要的事情是在喧嚣过后找到 “值得亲手去做的事情”。所以，今天这篇博客，我想分享一个 “熟悉而陌生” 的东西：MCP，即：模型上下文协议，并尝试将这个协议和 Semantic Kernel 连接起来。\nMCP 介绍 [TL;DR] MCP 是由 Anthropic 设计的开放协议，其定位类似于 AI 领域的 USB 接口，旨在通过统一接口解决大模型连接不同数据源和工具的问题。该协议通过 JSON-RPC 规范定义了 Prompt 管理、资源访问和工具调用三大核心能力，使得任何支持 Function Calling 的模型都能无缝对接外部系统，从而帮助大语言模型实现 “万物互联”。\n什么是 MCP? MCP（Model Context Protocol）是由 Anthropic 设计的一种开放协议，旨在标准化应用程序向大语言模型（LLMs）提供上下文的方式，使大模型能够以统一的方法连接各种数据源和工具。你可以将其理解为 AI 应用的 USB 接口，为 AI 模型连接到不同的数据源和工具提供了标准化的方法。架构设计上，MCP 采用了经典的 C/S 架构，客户端可以使用该协议灵活地连接多个 MCP Server，从而获取丰富的数据和功能支持，如下图所示：\nMCP 基本架构\r具体而言，MCP 架构中包括四个核心角色：\nMCP Host：承载用户交互的终端，如 Claude Desktop、Cusror、VSCode 等，负责发起请求 MCP Client：协议客户端，负责建立、维护与服务器端的一对一连接，通常需要集成 SDK 到 MCP Host MCP Server: 协议服务器端，对外暴露三种核心能力：Prompts、Resources 和 Tools Data Source：数据源，是本地资源（如 SQLite、文件系统）与远程服务（如 Github API）的集合 为什么选择 MCP? 在过去的这一年里，AI 智能体的技术生态逐渐呈现出两种典型的演进方向。首先，是以 LangChain、Semantic Kernel 等为代表的 AI 框架；其次，是以 Dify、Coze 等为代表的智能体编排平台。这实际上揭示了当前智能体技术发展的双重路径，即：人们正试图从框架层和平台层两个维度去攻克 Agent 技术的高峰。\n为什么选择 MCP?\r然而，当你真正地深入实践这一切的时候，你会在这些框架和平台中发现许多痛点。例如：\n语言框架的割裂性：不同技术栈中对 Agent 基础元素的定义存在着根本性差异。例如，LangChain 中采用 Python 的 @tool 装饰器来标注工具方法，而 C# 系列的 Semantic Kernel 则通过 [KernelFunction] 特性来实现功能注册。这种语法层面的分歧，无形中增加了跨平台协作的成本。 平台生态的封闭性：以 Coze 和 Dify 的插件系统为例，虽然二者均支持集成 Jina AI 插件，但是其工作流编排和配置的规范完全不同。这种生态壁垒加剧了不同技术体系间的 “数字鸿沟”，造成应用迁移成本过高，最终导致智能体平台沦为信息孤岛。 开发资源的重复消耗：目前，无论是服务供应商还是开发者，均需要参与智能体平台的适配工作，容易造成重复性工作，这对于 AI 时代而言是一种注意力的浪费。更重要是，这不利于 AI 技术的进一步发展，真正具有突破性的技术创新难以获得足够关注。 如你所见，有了 MCP 以后，开发人员只需要和 MCP 打交道，这是真正意义上的 “Attention Is All You Need”。\nMCP 如何工作? 现在，当我们将目光聚焦在 MCP 上面时，我们会发现情况开始有所好转，因为 Anthropic 使用 JSON-RPC 规范定义了一套与语言、平台无关的协议。在该协议中，定义了 Requests、Responses 和 Notifications 三种消息类型：\nType Description Requirements Requests Messages sent to initiate an operation Must include unique ID and method name Responses Messages sent in reply to requests Must include same ID as request Notifications One-way messages with no reply Must not include an ID 在上文中我们提到，MCP 支持 Prompts、Resources 和 Tools 三大核心能力。以 Tools 这个最常见的能力为例，MCP 支持工具的发现、调用和更新，其交互过程通常如下图所示：\nMCP-Tools 能力示意图\r此时，我们会注意到，MCP 针对工具调用主要提供了三个 API：tools/list、tools/call 以及 notifications/tools/list_changed。其中，notifications/tools/list_changed 是可选的，属于 Notification 的一部分。顾名思义，当服务器端提供的工具列表发生变化时，它能够以通知的形式告知客户端这一变化。如果你熟悉 JSON-RPC 规范，相信你已经在脑海中推测出具体的消息结构。首先，客户端通过 tools/list 方法向服务器端发起请求：\n{ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;id\u0026#34;: 1, \u0026#34;method\u0026#34;: \u0026#34;tools/list\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;cursor\u0026#34;: \u0026#34;optional-cursor-value\u0026#34; } } 接下来，服务器端会返回它目前支持的工具列表。这里，我们以经典的 get_weather 方法为例：\n{ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;id\u0026#34;: 1, \u0026#34;result\u0026#34;: { \u0026#34;tools\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;get_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;get current weather information for a location\u0026#34;, \u0026#34;inputSchema\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;city name or zip code\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;] } }], \u0026#34;nextCursor\u0026#34;: \u0026#34;next-page-cursor\u0026#34; } } 如果你接触过 ReAct、Tool Use、Function Calling 这些概念，你会发现这一切是如此地熟悉和亲切。当我们将这些工具提供给 LLM 以后，由 LLM 决定是否要调用指定的工具。此时，我们可以通过 tools/call 方法来调用指定的工具。这里，同样以经典的 get_weather 方法为例：\n{ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;id\u0026#34;: 2, \u0026#34;method\u0026#34;: \u0026#34;tools/call\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_weather\u0026#34;, \u0026#34;arguments\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;New York\u0026#34; } } } 此时，我们会收到服务器端的响应消息，如下所示：\n{ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;id\u0026#34;: 2, \u0026#34;result\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Current weather in New York:\\nTemperature: 72°F\\nConditions: Partly cloudy\u0026#34; } ], \u0026#34;isError\u0026#34;: false } } 读到这里，诸位看官心里一定在吐槽：有没有搞错，就这？这好像和 Function Calling 没什么区别嘛！我的理解是，MCP、Function Calling 和 Agent 本质上是一个层层递进的关系，MCP 提供一种与模型、语言、框架无关的工具抽象，任何支持 Function Calling 的模型都可以调用这些工具，而 Agent 框架则在此基础上对工具进行规划与编排。\nMCP、Function Calling 和 Agent 三者间的联系\r例如，去年年底的时候，Anthropic 和智谱相继发布了 Cumputer Use 功能，可这些功能大多都仅限于在厂商自家的产品中使用。如果你想在国内的 DeekSeek 或者 Kimi 上面尝试，基本上是痴心妄想。可有了 MCP 以后，情况就大不相同。你只需要使用 Playwright MCP Server 或者 Browser-Use MCP Server 便可以轻松 “尝鲜”。这次 Manus 爆火后，社区在几个小时内迅速复刻出了OpenManus，这与该团队直接使用第三方库 browser-use 息息相关。由此可见，一个健康、开放的 AI 生态会极大地促进 AI 应用的繁荣。事实上，自去年 MCP 发布以来，社区里涌现出了大量的第三方 MCP 服务器，这些服务器极大地扩展了 AI 的能力边界。现在，AI 可以连接到 Notion、Slack、Github、Elasticsearch 等众多平台，如下图所示，mcpservers.org、mcp.so 等网站收录了许多 MCP Server：\nAwesome MCP Servers\r所以，我们为什么要了解 MCP 呢？因为只要接入了 MCP， 便可以拥抱 MCP 背后的整个生态，这意味着 AI 领域的 “万物互联” 时刻已悄然到来。唯一的问题在于，国内外的 AI 厂商是否有意愿一起将 MCP 发展为行业标准。我想，届时无论是服务供应商还是个人开发者，都能从 MCP 这个协议中受益。除了 Tools，MCP 还支持 Resources 和 Prompts 相关的功能，它们负责对提示词、文件等进行管理。当然，这些并不是本文关注的重点，这里不再赘述。我们只需要知道一件事情，对一个 MCP Server 而言，最重要的是实现 tools/list 和 tools/call 这两个方法。目前，官方 SDK 支持 Python、TypeScript、Java 和 Kotlin 这四种语言，我们可以使用这些 SDK 来集成或者开发一个 MCP Server。下面是一个 Python 版本的 SQLite Explorer 示例：\nfrom mcp.server.fastmcp import FastMCP import sqlite3 mcp = FastMCP(\u0026#34;SQLite Explorer\u0026#34;) @mcp.resource(\u0026#34;schema://main\u0026#34;) def get_schema() -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Provide the database schema as a resource\u0026#34;\u0026#34;\u0026#34; conn = sqlite3.connect(\u0026#34;database.db\u0026#34;) schema = conn.execute( \u0026#34;SELECT sql FROM sqlite_master WHERE type=\u0026#39;table\u0026#39;\u0026#34; ).fetchall() return \u0026#34;\\n\u0026#34;.join(sql[0] for sql in schema if sql[0]) @mcp.tool() def query_data(sql: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Execute SQL queries safely\u0026#34;\u0026#34;\u0026#34; conn = sqlite3.connect(\u0026#34;database.db\u0026#34;) try: result = conn.execute(sql).fetchall() return \u0026#34;\\n\u0026#34;.join(str(row) for row in result) except Exception as e: return f\u0026#34;Error: {str(e)}\u0026#34; 如你所见，在该示例中，MCP Server 提供了一个 resource、一个 tool，前者负责返回当前数据库中的 DDL，后者提供一个查询数据的方法。恭喜你，现在你可以开始着手设计一个针对 Text2SQL 的 Agent 了。\nMCP 的局限性 当然，我们需要学会辩证地看待事物，MCP 并非完美无瑕。首先，我们不清楚国内外厂商适配这一协议的热情到底有多少；其次，类似于大多数 AI 框架，MCP 正处在迅速发展阶段，该协议的最新版本是 2024-11-05。截止目前，官方在 2025 年上半年的 Roadmap 主要集中在：认证/授权、服务发现、无状态操作。所以，未来走向到底如何，着实充满了变数。例如，按照官方的设计，MCP 在传输层（Transports）支持 stdio 和 HTTP with Server-Sent Events (SSE)，可目前大多数的 MCP Server 都是运行在本地的 stdio。对于终端用户而言，使用 MCP 依然需要了解 Python、Node.js 甚至 Docker，不得不说，这其实是一种隐形的成本。\nSemantic Kernel x MCP Semantic Kernel 集成 MCP 流程示意图\r在 Semantic Kernel 中，我们使用插件（Plugin）这个概念来描述一组工具，而每个工具则是一个 KernelFunction。因此，如果希望在 Semantic Kernel 中集成 MCP，本质上就是将 MCP 中的 Tools 转换为 Semantic Kernel 中的 KernelFunction。如上图所示，我们将在 Semantic Kernel 中集成一个 MCP 客户端，然后利用 tools/list 和 tools/call 这两个 API 分别实现工具获取、工具调用这两个流程。\n工具获取 截止目前，MCP 官方还没有提供对 .NET 的支持，不过社区里还是出现了第三方实现。例如：\nMCPSharp: https://github.com/afrise/MCPSharp mcpdotnet: https://github.com/PederHP/mcpdotnet 博主这里选择的是 mcpdotnet，假设我们希望在 Semantic Kernel 中集成 Playwright MCP Server。此时，我们可以编写下面的代码来连接到对应的 MCP Server：\nvar clientOptions = new McpClientOptions() { ClientInfo = new McpDotNet.Protocol.Types.Implementation() { Name = name, Version = \u0026#34;1.0.0\u0026#34; }, }; var serverConfig = new McpServerConfig() { Id = \u0026#34;playwright\u0026#34;, Name = \u0026#34;playwright\u0026#34;, TransportType = \u0026#34;stdio\u0026#34;, TransportOptions = new Dictionary\u0026lt;string, string\u0026gt; { [\u0026#34;command\u0026#34;] = \u0026#34;npx\u0026#34;, [\u0026#34;arguments\u0026#34;] = \u0026#34;-y @executeautomation/playwright-mcp-server\u0026#34;, } }; var loggerFactory = kernel.Services.GetRequiredService\u0026lt;ILoggerFactory\u0026gt;(); var clientFactory = new McpClientFactory( [serverConfig], clientOptions, NullLoggerFactory.Instance ); var client = await clientFactory.GetClientAsync(serverConfig.Id).ConfigureAwait(false); 从 clientFactory 获取 IMcpClient 实例时，客户端会先调用 initialize() 方法。服务器端初始化完成后，会给客户端发送 notifications/initialized 通知，表明服务器端已完成初始化。此时，可调用 ListToolsAsync() 方法，获取 MCP 服务器端提供的工具列表:\nvar listToolsResult = await client.ListToolsAsync().ConfigureAwait(false); var tools = listToolsResult.Tools; 协议转换 从前文中可知，MCP 使用 JSONSchema 来描述工具的输入参数，返回值则被定义为一个数组，如下图所示：\nMCP 工具调用输入 \u0026amp;amp; 输出\r因此，我们需要写一个中间层，将 MCP 的工具转换为 KernelFunction，这部分内容非常简单，不再赘述：\n// 将 MCP 中的 Tool 转换为 KernelFunction private static KernelFunction ToKernelFunction(this Tool tool, IMcpClient client) { async Task\u0026lt;string\u0026gt; InvokeToolAsync( Kernel kernel, KernelFunction function, KernelArguments arguments, CancellationToken cancellationToken ) { try { var mcpArguments = new Dictionary\u0026lt;string, object\u0026gt;(); foreach (var arg in arguments) { if (arg.Value is not null) mcpArguments[arg.Key] = function.ToArgumentValue(arg.Key, arg.Value); } var result = await client.CallToolAsync( tool.Name, mcpArguments, cancellationToken: cancellationToken ).ConfigureAwait(false); return string.Join(\u0026#34;\\n\u0026#34;, result.Content .Where(c =\u0026gt; c.Type == \u0026#34;text\u0026#34;) .Select(c =\u0026gt; c.Text)); } catch { throw; } } return KernelFunctionFactory.CreateFromMethod( method: InvokeToolAsync, functionName: tool.Name, description: tool.Description, parameters: ToKernelParameters(tool), returnParameter: ToKernelReturnParameter() ); } // 将 MCP 中工具的输入转换为 KernelFunction 输入 private static List\u0026lt;KernelParameterMetadata\u0026gt; ToKernelParameters(Tool tool) { var inputSchema = tool.InputSchema; var properties = inputSchema?.Properties; if (properties == null) return []; HashSet\u0026lt;string\u0026gt; requiredProperties = new(inputSchema!.Required ?? []); return properties.Select(kvp =\u0026gt; new KernelParameterMetadata(kvp.Key) { Description = kvp.Value.Description, ParameterType = ConvertParameterDataType(kvp.Value, requiredProperties.Contains(kvp.Key)), IsRequired = requiredProperties.Contains(kvp.Key) }) .ToList(); } // 将 JSONSchema 中的数据类型转换为 C# 的数据类型 private static Type ConvertParameterDataType(JsonSchemaProperty property, bool required) { var type = property.Type switch { \u0026#34;string\u0026#34; =\u0026gt; typeof(string), \u0026#34;integer\u0026#34; =\u0026gt; typeof(int), \u0026#34;number\u0026#34; =\u0026gt; typeof(double), \u0026#34;boolean\u0026#34; =\u0026gt; typeof(bool), \u0026#34;array\u0026#34; =\u0026gt; typeof(List\u0026lt;string\u0026gt;), \u0026#34;object\u0026#34; =\u0026gt; typeof(Dictionary\u0026lt;string, object\u0026gt;), _ =\u0026gt; typeof(object) }; return !required \u0026amp;\u0026amp; type.IsValueType ? typeof(Nullable\u0026lt;\u0026gt;).MakeGenericType(type) : type; } // 转换返回值，简化处理，直接返回字符串类型 private static KernelReturnParameterMetadata? ToKernelReturnParameter() { return new KernelReturnParameterMetadata() { ParameterType = typeof(string), }; } // 将 KernelFunction 参数转换为 object private static object ToArgumentValue(this KernelFunction function, string name, object value) { var parameter = function.Metadata.Parameters.FirstOrDefault(p =\u0026gt; p.Name == name); return parameter?.ParameterType switch { Type t when Nullable.GetUnderlyingType(t) == typeof(int) =\u0026gt; Convert.ToInt32(value), Type t when Nullable.GetUnderlyingType(t) == typeof(double) =\u0026gt; Convert.ToDouble(value), Type t when Nullable.GetUnderlyingType(t) == typeof(bool) =\u0026gt; Convert.ToBoolean(value), Type t when t == typeof(List\u0026lt;string\u0026gt;) =\u0026gt; (value as IEnumerable\u0026lt;object\u0026gt;)?.ToList(), Type t when t == typeof(Dictionary\u0026lt;string, object\u0026gt;) =\u0026gt; (value as Dictionary\u0026lt;string, object\u0026gt;)?.ToDictionary(kvp =\u0026gt; kvp.Key, kvp =\u0026gt; kvp.Value), _ =\u0026gt; value, } ?? value; } 现在，一切就变得简单了，我们可以封装一个如下的扩展方法：\npublic static async Task\u0026lt;IEnumerable\u0026lt;KernelFunction\u0026gt;\u0026gt; GetKernelFunctionsAsync(this IMcpClient client) { var listToolsResult = await client.ListToolsAsync().ConfigureAwait(false); return listToolsResult.Tools.Select(tool =\u0026gt; ToKernelFunction(tool, client)).ToList(); } 工具调用 在 MCP 中，客户端调用服务器端提供的工具，可以直接使用 CallToolAsync() 方法：\nvar result = await client.CallToolAsync( tool.Name, mcpArguments, cancellationToken: cancellationToken ).ConfigureAwait(false); 当我们转换为 KernelFunction 以后，只需要调用 InvokeAsync() 方法即可调用对应的插件函数。考虑到，在 Agent 中，插件函数通常是由 LLM 来调用的，我们将编写下面的扩展方法来实现工具的注册：\n// 注册 MCP Server public static async Task AddMCPServer( this Kernel kernel, string name, string command, string version = \u0026#34;1.0.0\u0026#34;, string[] args = null, Dictionary\u0026lt;string, string\u0026gt; env = null ) { var clientOptions = new McpClientOptions() { ClientInfo = new McpDotNet.Protocol.Types.Implementation() { Name = name, Version = \u0026#34;1.0.0\u0026#34; }, }; var serverConfig = new McpServerConfig() { Id = name, Name = name, TransportType = \u0026#34;stdio\u0026#34;, TransportOptions = new Dictionary\u0026lt;string, string\u0026gt; { [\u0026#34;command\u0026#34;] = command, [\u0026#34;arguments\u0026#34;] = string.Join(\u0026#39; \u0026#39;, args ?? []), } }; var loggerFactory = kernel.Services.GetRequiredService\u0026lt;ILoggerFactory\u0026gt;(); var clientFactory = new McpClientFactory([serverConfig], clientOptions, loggerFactory); var client = await clientFactory.GetClientAsync(serverConfig.Id).ConfigureAwait(false); var kernelFunctions = await client.GetKernelFunctionsAsync(); kernel.Plugins.AddFromFunctions(name, kernelFunctions); } 至此，我们便完成了 MCP 在 Semantic Kernel 中的集成。现在，你只需要使用下面的代码片段即可：\n// 添加 playwright-mcp-server await kernel.AddMCPServer( name: \u0026#34;playwright\u0026#34;, command: \u0026#34;npx\u0026#34;, version: \u0026#34;1.0.0\u0026#34;, args: [\u0026#34;-y\u0026#34;, \u0026#34;@executeautomation/playwright-mcp-server\u0026#34;], env: null ); 场景化效果展示 好的，当我们给 Semantic Kernel 集成 MCP 以后，现在我们来一起看看它具体能帮我们做什么事情？\n操作浏览器 如图所示，用户请求：打开 Bing 主页，搜索 \u0026ldquo;Model Context Protocol\u0026rdquo;\nMCP-操作浏览器-A\rMCP-操作浏览器-B\r访问文件系统 如图所示，用户请求：我在 D:\\Projects\\2024 这个 Git 仓库中都提交过那些代码，最近的一次的更新是什么？\nMCP-访问文件、读取 Git 提交记录\r读取 Github 仓库 如图所示，用户请求：阅读 OpenManus 仓库的代码，帮我分析其架构、设计相关的细节\nMCP-读取 Github 仓库\r博主先后体验了四个 MCP Server。其中，Knowledge Graph Memory Server 可以在本地构建知识图谱，从对话中提取并持久化三元组，实现长期记忆。当然，这里的关键在于，确定三元组的读写时机。今年，我准备将现有 RAG 和 Agent 融合，升级为 Agentic RAG。目前，考虑的是将 ReAct 模式应用于 RAG，显然，这里同样会遇到一个问题，即： LLM 如何判定上下文足以生成最终答案。因篇幅限制，此处不再展示更多截图，一切都需要大家去亲自体验。\n本文小结 MCP 是 Anthropic 设计的开放协议，其定位类似于 AI 领域的 USB 接口，希望通过统一接口解决大模型连接不同数据源和工具的问题。Semantic Kernel 是微软开源的 Agent 框架，两者的结合可以让 .NET 开发者快速、高效地接入社区中的 MCP 服务器，减少重复性的平台对接工作。除工具调用外，还可以考虑将项目中的提示词模板统一放置到 MCP Server 上管理。博主撰写此文时，网络正在传播着被破解的 Manus 源代码。虽然经常有人说提示工程已不存在了，可在实际的项目中提示词依旧不可或缺。从这个角度来看，尽管提示词技术含量不高，但若能得到妥善管理，至少会比项目被破解、提示词被泄露更显体面。关于更多 MCP 的细节，请参考官方文档：Introduction - Model Context Protocol ，无论你是开发者还是普通用户，相信都能在那里找到答案。\n参考链接 Model Context Protocol Model Context Protocol Specification LLM Function-Calling vs. Model Context Protocol (MCP) Integrating Model Context Protocol Tools with Semantic Kernel: A Step-by-Step Guide 什么是模型上下文协议（MCP）？它如何比传统API更简单地集成AI？ ",
            "tags": ["MCP","Agent","Semantic Kernel","Function Calling"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "命运、偏见与自由：《魔童之哪吒闹海》的终极抗争",
            "date": "2025-02-06",
            "permalink": "http://localhost:1313/posts/ne-zha-2/",
            "summary": "在金庸先生的《射雕英雄传》中，有一个情节令人难以忘怀。一向被视为离经叛道的“东邪”黄药师，在得知女儿黄蓉对郭靖情根深种时，亦不免喟然长叹：“且夫天地为炉兮，造化为工；阴阳为炭兮，万物为铜”。贾谊的郁郁而终，靖蓉的长岭遇雨，在遥远的历史的长河中千回百转，可谓“不谋而遐迩自同”。或许，无论是事业与爱情，其底色都难免带着些许苦涩，否则，天地何以成为熔炉，让万物在其中备受煎熬。当然，今天我想聊的不是金庸，而是春节档票房冠军《哪吒之魔童闹海》。这部改编自中国古代神话的动画电影，融合传统神话与现代叙事，在冰与火交织的冲突下，在大银幕上演绎了一部普罗米修斯式的抗争史，带给观众一场视觉与心灵的双重震撼。\n当哪吒不再困扰于魔丸身份\r从身份认同到体系反思 如果说，《哪吒》系列第一部《魔童降世》的核心冲突源于身份认同，那么，这一次《魔童闹海》开始将视野拓展至对体系与秩序的思考。帕拉图曾提出“人生三问”——我是谁？我从哪里来？我要到哪里去？哪吒曾为“魔丸”和“灵珠”的身份困扰，然而，在其父母的影响下，他坚定地喊出：“我命由我不由天”，是魔是仙要由自己说了算，表达对命运的不屈抗争。他完全不在意世俗的眼光，毅然决然地等待天雷落下。如果用一句话概括：“若命运不公，就和它斗到底”，我个人认为，这句话是《哪吒》系列最好的注脚。剧情上，这部作品与前作一脉相承，哪吒和敖丙在天劫中大难不死，虽然肉身毁灭，可魂魄得以在七色宝莲中保全。当然，这一切的代价是太乙真人被夺去顶上三花。\n闭嘴！你这只愚蠢的土拨鼠\r因此，第二部的故事便围绕着“劫后余生”展开。因为哪吒和敖丙的肉身损坏，所以，太乙真人借助七色宝莲和藕粉为二人重塑身体。然而，此时申公豹借四海龙王之力，引海底妖族攻入陈塘关。敖丙为护陈塘关擅自移动身体，导致塑造的肉身再次损坏。适逢七色宝莲灵力枯竭，为了得到能让七色宝莲再次盛开的玉液琼浆，哪吒和敖丙被迫共用一个身体，前往昆仑山玉虚宫升仙考核。这种剧情上的转折、递进，节奏紧凑，扣人心弦，可以说是全程无尿点。重塑身体与升仙考核两大情节，贡献了整部电影的大部分的笑点，特别是哪吒塑型这一步，简直就是现实中甲方乙方的真实写照。影片中中真正深刻的东西，我认为，直到陈塘关被屠城这一刻才开始逐渐呈现出来。\n命运熔炉中的权力游戏 教员有首词叫做《贺新郎·读史》，其中有一句“铜铁炉中翻火焰，为问何时猜得”。众所周知，铜器时代和铁器时代是人类历史上的两个重要阶段，它们标志着人类在金属加工技术、社会结构和文明进步方面的巨大飞跃。结合第一部的剧情，哪吒的宿命，原本就开始于一场精心设计的“冶金游戏”，元始天尊将混元珠投入天元鼎，试图在一个相生相克的世界中提炼出永恒秩序。自此，被标记为“魔丸”的哪吒一生下来，便承受着世俗偏见带来“先天原罪”。如申公豹所言，“人心中的成见是一座大山”。龙族接受天庭“招安”，负责为其镇压海底妖兽，可这处名为“龙宫”的所在，又何尝不是关押龙族的“地牢”呢？因此，敖丙从一出生就被赋予了振兴龙族的使命，那件万龙甲便是最好的证明。\n无量仙翁开始露出狐狸尾巴\r电影中的偏见不止于此，当两只结界兽赶到玉虚宫报信时，有多少人想当然地认为，是申公豹屠戮了陈塘关的百姓呢？这同样是一种偏见。如果说，土坡鼠、申正道和石矶娘娘这三道试题，只是让你隐隐对阐教的正义性产生了一丝怀疑。那么，当巨大的天元鼎垂直插入东海海底的时候，你是否从无量仙翁那张道貌岸然的脸上，读出了某种献祭的意味呢？因为妖族修行比人类更勤勉，哪怕它们没做过坏事；因为龙族是实力强大的妖族，哪怕它们早已归顺；因为要嫁祸龙族而大举屠城，哪怕陈塘关的百姓无辜冤死。《贺新郎》里写道：“人世难逢开口笑，上疆场彼此弯弓月。流遍了，郊原血”。回顾历史，宏大叙事下的“电车难题”持续上演，十字军东征时的“上帝旨意”、殖民主义者的“文明开化”……，无一不是在用别人的血肉浇筑“封神”的圣坛。《封神演义》中阐截二教，到底孰是孰非呢？\n自由与秩序的永恒斗争 诚然，电影依旧没能突破束缚，即：“高层隐身、中层搞事、底层遭殃”的叙事套路，甚至我们无从得知，无量仙翁的行为，到底是私心作祟下的自作主张，还是元始天尊的暗中授意。可这种“个体失范代替制度失范”的切入点，还是能给观众带来某种思考：从来如此，便对吗？善与恶，是魔是仙，从来都不应该由种族和身份决定，申公豹有自己的坚守与珍视，阐教有在封神大战前壮大实力的私心，敖光有保护龙族而委曲求全的无奈，在体系框架内循规蹈矩固然重要，可当个体困境变成群体困境、退无可退的时候，或许，跳出陈规、尝试打破规则，一切自然会柳暗花明、迎来转机。当哪吒的肉身在三昧真火中完成重塑、当人族和妖族齐心协力打破天元鼎的桎梏、当定海神针在落日下缓缓浮出海面……我想，哪吒撕裂的或许是人类文明进程中的永恒困境：在命运熔炉的烈焰中，我们到底是等待煅烧的铜铁，还是执掌锤柄的工匠？“怀璧其罪”、“欲加之罪”，本质上都是一种群体对个体道德上的绑架和霸凌。\n哪吒浴火重生\r古希腊哲人赫拉克利特曾说过：“世界是一团永恒的活火”。而哪吒的选择则昭示着：与其在烈焰中化为灰烬，不如让火焰成为涅槃的锤砧。我们无需过渡解读，说昆仑山的玉虚宫像白宫、天元鼎上的符文像美元、成仙的玉牌像绿卡、压住丹炉的像棵摇钱树，甚至无量仙翁、鹿童、鹤童比妖怪更像妖怪……当哪吒与敖丙联手击败无量仙翁，一切的建筑都灰飞烟灭，那个定义神仙妖魔的符号体系，更是被彻底解构、直至崩塌。在这部电影中，所有的海底妖兽都被用锁链束缚在定海神针上面。或许，只需阐教的一道咒语，便能解开这些枷锁。可要解开人心的锁链，显然要艰难百倍。对哪吒而言，是不再纠结于魔丸的身份；对敖丙而言，是不再背负振兴家族的使命。加缪在《反抗者》中写道：“我反抗，故我们存在”。影片最后，妖族与阐教的决战，难道不比《复仇者联盟》更燃？我相信，如果贾谊看过这部电影，一定不会如此草率地写下“天地为炉”。因为在自由面前，秩序终将被反抗者打破。",
            "content": "在金庸先生的《射雕英雄传》中，有一个情节令人难以忘怀。一向被视为离经叛道的“东邪”黄药师，在得知女儿黄蓉对郭靖情根深种时，亦不免喟然长叹：“且夫天地为炉兮，造化为工；阴阳为炭兮，万物为铜”。贾谊的郁郁而终，靖蓉的长岭遇雨，在遥远的历史的长河中千回百转，可谓“不谋而遐迩自同”。或许，无论是事业与爱情，其底色都难免带着些许苦涩，否则，天地何以成为熔炉，让万物在其中备受煎熬。当然，今天我想聊的不是金庸，而是春节档票房冠军《哪吒之魔童闹海》。这部改编自中国古代神话的动画电影，融合传统神话与现代叙事，在冰与火交织的冲突下，在大银幕上演绎了一部普罗米修斯式的抗争史，带给观众一场视觉与心灵的双重震撼。\n当哪吒不再困扰于魔丸身份\r从身份认同到体系反思 如果说，《哪吒》系列第一部《魔童降世》的核心冲突源于身份认同，那么，这一次《魔童闹海》开始将视野拓展至对体系与秩序的思考。帕拉图曾提出“人生三问”——我是谁？我从哪里来？我要到哪里去？哪吒曾为“魔丸”和“灵珠”的身份困扰，然而，在其父母的影响下，他坚定地喊出：“我命由我不由天”，是魔是仙要由自己说了算，表达对命运的不屈抗争。他完全不在意世俗的眼光，毅然决然地等待天雷落下。如果用一句话概括：“若命运不公，就和它斗到底”，我个人认为，这句话是《哪吒》系列最好的注脚。剧情上，这部作品与前作一脉相承，哪吒和敖丙在天劫中大难不死，虽然肉身毁灭，可魂魄得以在七色宝莲中保全。当然，这一切的代价是太乙真人被夺去顶上三花。\n闭嘴！你这只愚蠢的土拨鼠\r因此，第二部的故事便围绕着“劫后余生”展开。因为哪吒和敖丙的肉身损坏，所以，太乙真人借助七色宝莲和藕粉为二人重塑身体。然而，此时申公豹借四海龙王之力，引海底妖族攻入陈塘关。敖丙为护陈塘关擅自移动身体，导致塑造的肉身再次损坏。适逢七色宝莲灵力枯竭，为了得到能让七色宝莲再次盛开的玉液琼浆，哪吒和敖丙被迫共用一个身体，前往昆仑山玉虚宫升仙考核。这种剧情上的转折、递进，节奏紧凑，扣人心弦，可以说是全程无尿点。重塑身体与升仙考核两大情节，贡献了整部电影的大部分的笑点，特别是哪吒塑型这一步，简直就是现实中甲方乙方的真实写照。影片中中真正深刻的东西，我认为，直到陈塘关被屠城这一刻才开始逐渐呈现出来。\n命运熔炉中的权力游戏 教员有首词叫做《贺新郎·读史》，其中有一句“铜铁炉中翻火焰，为问何时猜得”。众所周知，铜器时代和铁器时代是人类历史上的两个重要阶段，它们标志着人类在金属加工技术、社会结构和文明进步方面的巨大飞跃。结合第一部的剧情，哪吒的宿命，原本就开始于一场精心设计的“冶金游戏”，元始天尊将混元珠投入天元鼎，试图在一个相生相克的世界中提炼出永恒秩序。自此，被标记为“魔丸”的哪吒一生下来，便承受着世俗偏见带来“先天原罪”。如申公豹所言，“人心中的成见是一座大山”。龙族接受天庭“招安”，负责为其镇压海底妖兽，可这处名为“龙宫”的所在，又何尝不是关押龙族的“地牢”呢？因此，敖丙从一出生就被赋予了振兴龙族的使命，那件万龙甲便是最好的证明。\n无量仙翁开始露出狐狸尾巴\r电影中的偏见不止于此，当两只结界兽赶到玉虚宫报信时，有多少人想当然地认为，是申公豹屠戮了陈塘关的百姓呢？这同样是一种偏见。如果说，土坡鼠、申正道和石矶娘娘这三道试题，只是让你隐隐对阐教的正义性产生了一丝怀疑。那么，当巨大的天元鼎垂直插入东海海底的时候，你是否从无量仙翁那张道貌岸然的脸上，读出了某种献祭的意味呢？因为妖族修行比人类更勤勉，哪怕它们没做过坏事；因为龙族是实力强大的妖族，哪怕它们早已归顺；因为要嫁祸龙族而大举屠城，哪怕陈塘关的百姓无辜冤死。《贺新郎》里写道：“人世难逢开口笑，上疆场彼此弯弓月。流遍了，郊原血”。回顾历史，宏大叙事下的“电车难题”持续上演，十字军东征时的“上帝旨意”、殖民主义者的“文明开化”……，无一不是在用别人的血肉浇筑“封神”的圣坛。《封神演义》中阐截二教，到底孰是孰非呢？\n自由与秩序的永恒斗争 诚然，电影依旧没能突破束缚，即：“高层隐身、中层搞事、底层遭殃”的叙事套路，甚至我们无从得知，无量仙翁的行为，到底是私心作祟下的自作主张，还是元始天尊的暗中授意。可这种“个体失范代替制度失范”的切入点，还是能给观众带来某种思考：从来如此，便对吗？善与恶，是魔是仙，从来都不应该由种族和身份决定，申公豹有自己的坚守与珍视，阐教有在封神大战前壮大实力的私心，敖光有保护龙族而委曲求全的无奈，在体系框架内循规蹈矩固然重要，可当个体困境变成群体困境、退无可退的时候，或许，跳出陈规、尝试打破规则，一切自然会柳暗花明、迎来转机。当哪吒的肉身在三昧真火中完成重塑、当人族和妖族齐心协力打破天元鼎的桎梏、当定海神针在落日下缓缓浮出海面……我想，哪吒撕裂的或许是人类文明进程中的永恒困境：在命运熔炉的烈焰中，我们到底是等待煅烧的铜铁，还是执掌锤柄的工匠？“怀璧其罪”、“欲加之罪”，本质上都是一种群体对个体道德上的绑架和霸凌。\n哪吒浴火重生\r古希腊哲人赫拉克利特曾说过：“世界是一团永恒的活火”。而哪吒的选择则昭示着：与其在烈焰中化为灰烬，不如让火焰成为涅槃的锤砧。我们无需过渡解读，说昆仑山的玉虚宫像白宫、天元鼎上的符文像美元、成仙的玉牌像绿卡、压住丹炉的像棵摇钱树，甚至无量仙翁、鹿童、鹤童比妖怪更像妖怪……当哪吒与敖丙联手击败无量仙翁，一切的建筑都灰飞烟灭，那个定义神仙妖魔的符号体系，更是被彻底解构、直至崩塌。在这部电影中，所有的海底妖兽都被用锁链束缚在定海神针上面。或许，只需阐教的一道咒语，便能解开这些枷锁。可要解开人心的锁链，显然要艰难百倍。对哪吒而言，是不再纠结于魔丸的身份；对敖丙而言，是不再背负振兴家族的使命。加缪在《反抗者》中写道：“我反抗，故我们存在”。影片最后，妖族与阐教的决战，难道不比《复仇者联盟》更燃？我相信，如果贾谊看过这部电影，一定不会如此草率地写下“天地为炉”。因为在自由面前，秩序终将被反抗者打破。\n",
            "tags": ["影评","哪吒","封神","动漫"],
            "categories": ["生活感悟"],
            "thumbnail": ""
        },
        {
            "title": "基于 K-Means 聚类分析实现人脸照片的快速分类",
            "date": "2025-01-14",
            "permalink": "http://localhost:1313/posts/face-photo-fast-classification-using-k-means-clustering/",
            "summary": "注：本文在创作过程中得到了 ChatGPT、DeepSeek、Kimi 的智能辅助支持，由作者本人完成最终审阅。\n在 “视频是不能 P 的” 系列文章中，博主曾先后分享过人脸检测、人脸识别等相关主题的内容。今天，博主想和大家讨论的是人脸分类问题。你是否曾在人群中认错人，或是盯着熟人的照片却一时想不出对方的名字？这种 “脸盲症” 的困扰，不仅在生活中令人感到尴尬，在整理照片时更是让人头疼不已。想象一下，某次聚会结束后，你的手机里存了上百张照片——有你的笑脸、朋友的自拍，甚至还有一部分陌生面孔混杂其中。手动将这些照片按人物分类，不仅费时费力，还可能会因为 “脸盲” 而频繁出错。此时，你是否期待有一种技术，可以像魔法一样，自动将这些照片按人物分类？事实上，这种 “魔法” 已经存在，它的名字叫做 K-Means 聚类分析。作为一种经典的无监督学习算法，K-Means 能够通过分析人脸特征，自动将相似的面孔归类到一起，完全无需人工干预。接下来，为了彻底根治 “脸盲症”，我们将详细介绍如何使用 K-Means 聚类分析来实现这一目标，哈利·波特拥有魔法，而我们则拥有科技。\n实现过程 如图所示，我们将按照下面的流程来达成 “自动分类人脸” 这一目标。其中，Dlib 负责提取人脸特征向量、Scikit-Learn 中的 K-Means 负责聚类分析、Matplotlib 负责结果的可视化：\n基于 K-Means 聚类分析实现人脸照片的快速分类示意图\rK-Means 简介 K-Means 是一种广泛应用的聚类算法，其基本原理是将数据集分成 K 个簇，目标是让每个簇内的数据点尽可能相似，而不同簇之间的数据点尽可能差异明显。K-Means 的执行过程如下：\n随机选取 K 个初始中心点。\n将每个数据点分配到距离最近的中心点所对应的簇。\n更新每个簇的中心点，通常取簇内所有数据点的均值。\n重复步骤 2 和 3，直到中心点不再发生变化或达到预设的最大迭代次数。\n如下图所示，图中展示了四种不同的聚类数据分布情况，按照从左到右、自上而下的顺序：\n图一：簇划分不正确或者簇数量假设错误 图二：数据分布具有各向异性，簇的形状是一个拉长的椭圆形，而不是对称的圆形 图三：各个簇之间的方差不同，绿色簇分布更紧密，而黄色簇分布更稀疏 图四：簇的大小不均匀，黄色簇数据点较少，而紫色簇数据点较多 四种不同的聚类数据分布情况\r因此，适用于 K-Means 的数据通常满足：\n簇是球状且分布均匀 簇的大小相近 簇无明显噪声点或者离群点 数据是各向同性分布 簇的数量已知 数据维度适中 如何确定 K 值 在使用 K-Means 之前，我们需要确定 K 值，即簇的数量。下面是三种常用的确定 K 值的方法：",
            "content": " 注：本文在创作过程中得到了 ChatGPT、DeepSeek、Kimi 的智能辅助支持，由作者本人完成最终审阅。\n在 “视频是不能 P 的” 系列文章中，博主曾先后分享过人脸检测、人脸识别等相关主题的内容。今天，博主想和大家讨论的是人脸分类问题。你是否曾在人群中认错人，或是盯着熟人的照片却一时想不出对方的名字？这种 “脸盲症” 的困扰，不仅在生活中令人感到尴尬，在整理照片时更是让人头疼不已。想象一下，某次聚会结束后，你的手机里存了上百张照片——有你的笑脸、朋友的自拍，甚至还有一部分陌生面孔混杂其中。手动将这些照片按人物分类，不仅费时费力，还可能会因为 “脸盲” 而频繁出错。此时，你是否期待有一种技术，可以像魔法一样，自动将这些照片按人物分类？事实上，这种 “魔法” 已经存在，它的名字叫做 K-Means 聚类分析。作为一种经典的无监督学习算法，K-Means 能够通过分析人脸特征，自动将相似的面孔归类到一起，完全无需人工干预。接下来，为了彻底根治 “脸盲症”，我们将详细介绍如何使用 K-Means 聚类分析来实现这一目标，哈利·波特拥有魔法，而我们则拥有科技。\n实现过程 如图所示，我们将按照下面的流程来达成 “自动分类人脸” 这一目标。其中，Dlib 负责提取人脸特征向量、Scikit-Learn 中的 K-Means 负责聚类分析、Matplotlib 负责结果的可视化：\n基于 K-Means 聚类分析实现人脸照片的快速分类示意图\rK-Means 简介 K-Means 是一种广泛应用的聚类算法，其基本原理是将数据集分成 K 个簇，目标是让每个簇内的数据点尽可能相似，而不同簇之间的数据点尽可能差异明显。K-Means 的执行过程如下：\n随机选取 K 个初始中心点。\n将每个数据点分配到距离最近的中心点所对应的簇。\n更新每个簇的中心点，通常取簇内所有数据点的均值。\n重复步骤 2 和 3，直到中心点不再发生变化或达到预设的最大迭代次数。\n如下图所示，图中展示了四种不同的聚类数据分布情况，按照从左到右、自上而下的顺序：\n图一：簇划分不正确或者簇数量假设错误 图二：数据分布具有各向异性，簇的形状是一个拉长的椭圆形，而不是对称的圆形 图三：各个簇之间的方差不同，绿色簇分布更紧密，而黄色簇分布更稀疏 图四：簇的大小不均匀，黄色簇数据点较少，而紫色簇数据点较多 四种不同的聚类数据分布情况\r因此，适用于 K-Means 的数据通常满足：\n簇是球状且分布均匀 簇的大小相近 簇无明显噪声点或者离群点 数据是各向同性分布 簇的数量已知 数据维度适中 如何确定 K 值 在使用 K-Means 之前，我们需要确定 K 值，即簇的数量。下面是三种常用的确定 K 值的方法：\n肘部法则/手肘图法（Elbow Method）：通过计算不同 K 值下的聚类误差平方和（SSE, Sum of Squared Errors），找到误差下降速度明显减缓的 “拐点”，这个拐点对应的 K 值即为最佳聚类数。 轮廓系数法（Silhouette Coefficient）：通过计算每个数据点的轮廓系数，评估聚类效果，选择轮廓系数最大的 K 值。轮廓系数结合了聚类的紧密度(同一簇内样本的相似度)和分离度(不同簇之间的差异度)。 戴维斯-博尔丁评分（Davies-Bouldin Index）：通过计算每个簇的簇内距离（样本到簇中心的平均距离）与簇间距离（不同簇中心之间的距离）的比值，评估聚类效果，选择评分最低的 K 值。 如图所示，分别展示了不同方法下的最佳 K 值，综合考虑三种评估方案，此处取 K=6：\n肘部法则/手肘图法\r轮廓系数法\r戴维斯-博尔丁评分\r提取人脸特征 为了对人脸照片进行聚类，我们首先需要提取人脸特征。为此，我们使用 Dlib 库，它提供了一个基于深度学习的预训练模型，该模型能够高效地将人脸图像转换为 128 维的特征向量。\nimport dlib import cv2 import numpy as np # 加载人脸检测器和特征提取器 detector = dlib.get_frontal_face_detector() predictor = dlib.shape_predictor(\u0026#34;./models/shape_predictor_68_face_landmarks.dat\u0026#34;) face_model = dlib.face_recognition_model_v1(\u0026#34;./models/dlib_face_recognition_resnet_model_v1.dat\u0026#34;) # 提取人脸特征 def extract_face_features(image_path): img = cv2.imread(image_path) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) faces = detector(gray) if not faces: return None max_face = max(faces, key=lambda rect: rect.width() * rect.height()) shape = predictor(gray, max_face) feature = face_model.compute_face_descriptor(img, shape) return np.array(feature) K-Means 聚类 接下来，在提取了人脸特征值后，我们可以使用 Scikit-Learn 中的 K-Means 方法进行聚类:\nfrom sklearn.cluster import KMeans X, image_paths = load_dataset(dataset_path) scaler = StandardScaler() X = scaler.fit_transform(X) # 使用 K-means 进行聚类 num_clusters = 6 kmeans = KMeans(n_clusters=num_clusters, n_init=50, random_state=42, init=\u0026#39;k-means++\u0026#39;) kmeans.fit(X) # 获取聚类标签 labels = kmeans.labels_ 其中，load_dataset() 方法用于返回特征值和图片路径，因为最终我们需要将这些图片移动到不同的文件夹中，以实现图片分类。下面是该方法的具体实现：\ndef load_dataset(dataset_path): results = [ (features, image_path) for image_name in os.listdir(dataset_path) if (image_path := os.path.join(dataset_path, image_name)) and (features := extract_face_features(image_path)) is not None ] X, image_paths = zip(*results) if results else ([], []) return np.array(X), list(image_paths) 这里，简单说一下各个参数对于 K-Means 的影响：\n使用 StandardScaler 对矩阵 X 进行标准化处理，将每一列的均值调整为 0，标准差调整为 1。标准化后的数据更适合 K-Means 聚类，因为 K-Means 对特征的尺度比较敏感。 参数 n_clusters 取值为 6，表示将数据分为 6 个簇，该值由最佳的 K 值确定。 参数 n_init 取值为 50，表示运行 K-Means 的次数，每次将使用不同的初始质心，确保可以得到最优结果。 参数 random_state 取值为 42，设置随机种子，确保结果可以复现。 参数 init 取值为 k-means++，表示使用 K-Means++ 算法初始化质心，避免陷入局部最优。 更多的参数及细节，请参考官方文档：https://scikit-learn.org.cn/view/108.html#2.3.2.%20K-means\nPCA 降维与可视化 理论上，这一步可以省略。不过，为了更加直观地展示聚类结果，我们可以使用 PCA（Principal Component Analysis，主成分分析）对人脸特征进行降维，将高维特征映射到低维空间。在本文中，我们会将 128 维的特征向量映射到 2 维平面，并通过 Matplotlib 库进行可视化展示，其中的关键代码展示如下：\nfrom sklearn.decomposition import PCA import matplotlib.pyplot as plt from matplotlib.offsetbox import OffsetImage, AnnotationBbox # 使用 PCA 降维到 2 维 pca = PCA(n_components=2) X_reduced = pca.fit_transform(X) # 获取聚类质心 centers = kmeans.cluster_centers_ centers_reduced = pca.transform(centers) # 绘制每个聚类的散点 plt.figure(figsize=(12, 10)) # 对于每个聚类，绘制聚类的点和代表性图片 for cluster in range(num_clusters): cluster_mask = labels == cluster cluster_points = X_reduced[cluster_mask] # 获取降维后的聚类中心坐标 x_center, y_center = centers_reduced[cluster] # 每个聚类的二维坐标 # 绘制聚类中的数据点 plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\u0026#39;Cluster {cluster}\u0026#39;) # 选择每个聚类中最接近聚类中心的图像 cluster_indices = np.where(cluster_mask)[0] distances = np.linalg.norm(X_reduced[cluster_mask] - centers_reduced[cluster], axis=1) closest_image_idx = cluster_indices[np.argmin(distances)] closest_image_path = image_paths[closest_image_idx] # 读取并处理该图片 img = extract_face_rect(closest_image_path) h, w = img.shape[:2] # 设定目标图像大小 target_size = 100 aspect_ratio = w / h # 根据长宽比计算新宽高 if aspect_ratio \u0026gt; 1: new_w = target_size new_h = int(target_size / aspect_ratio) else: new_h = target_size new_w = int(target_size * aspect_ratio) # 调整图片大小 img_resized = cv2.resize(img, (new_w, new_h)) # 在聚类中心绘制代表性图片 imagebox = OffsetImage(img_resized, zoom=0.5) imagebox.image.axes = plt.gca() ab = AnnotationBbox(imagebox, (x_center, y_center),frameon=True,pad=0.5) plt.gca().add_artist(ab) # 绘制聚类中心 plt.scatter(centers_reduced[:, 0], centers_reduced[:, 1], c=\u0026#39;red\u0026#39;, marker=\u0026#39;X\u0026#39;, s=200, label=\u0026#39;CentroIds\u0026#39;) # 标题、标签和图例 plt.title(\u0026#39;K-means Clustering Results (PCA Reduced)\u0026#39;) plt.xlabel(\u0026#39;PCA Dimension 1\u0026#39;) plt.ylabel(\u0026#39;PCA Dimension 2\u0026#39;) plt.legend() plt.grid(True) plt.show() 这段代码看起来非常复杂，它具体做了什么事情呢？实际上，在通过 K-Means 聚类获得分类标签以后，我们开始尝试在每个簇的中心绘制一张人脸图片，这张图片是如何找到的呢？这里我们选取的的是离簇中心最近的那张图片。考虑到，通过 Dlib 提取的人脸特征值都是 128 维的，如果希望将其绘制到二维平面上，就需要通过 PCA 来完成降维。如图所示，通过 PCA 降维后，我们可以将聚类结果可视化。不同颜色代表不同的簇，同一簇内的点表示相似的人脸照片。通过这张图片，相信你会更加直观地理解核心点、边界点、聚类中心、重叠点等概念：\nK-Means 聚类结果可视化\r本文小结 本文尝试探索运用 K-Means 聚类技术对人脸照片进行快速分类的方案。首先，我们介绍了 K-Means 的基本原理及其适用范围，并探讨了如何合理确定簇数 K。随后，博主详细阐述了人脸特征的提取方法、K-Means 聚类的具体实施步骤，以及借助 PCA 降维技术实现数据可视化的流程。这一系列步骤帮助我们高效地对大量人脸照片进行分类。然而，K-Means 并非完美无瑕。它需要预先设定簇数 K，对初始中心点的选择较为敏感，且默认簇为球形或凸形。这导致其在处理非凸形簇或含噪声数据时表现不佳。针对这些局限性，实际应用中可以考虑使用 DBSCAN 等聚类算法，这类方法通常能够自动确定簇的数量，并对噪声数据具有更强的适应性。作为一种经典的聚类算法，K-Means 具有计算高效、实现简单的优点。然而，在普适性、精确度以及对深度特征的利用上，相较于基于深度学习的聚类方法，K-Means 存在一定不足。本文旨在为读者提供一种基础思路，激发其对于聚类分析这一领域的兴趣。本文代码已上传至 Github，更多细节请参考：https://github.com/Regularly-Archive/2024/blob/main/face-classify/kmeans_cluster_classify.py。\n",
            "tags": ["机器学习","人脸分类","K-Means","Scikit-Learn","聚类"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "基于 LLaMA 和 LangChain 实践本地 AI 知识库",
            "date": "2024-02-29",
            "permalink": "http://localhost:1313/posts/practice-local-ai-knowledg-base-based-on-llama-and-langchain/",
            "summary": "有时候，我难免不由地感慨，真实的人类世界，本就是一个巨大的娱乐圈，即使是在英雄辈出的 IT 行业。数日前，Google 正式对外发布了 Gemini 1.5 Pro，一个建立在 Transformer 和 MoE 架构上的多模态模型。可惜，这个被 Google 寄予厚望的产品并未激起多少水花，因为就在同一天 OpenAI 发布了 Sora，一个支持从文字生成视频的模型，可谓是一时风光无二。有人说，OpenAI 站在 Google 的肩膀上，用 Google 的技术疯狂刷屏。此中曲直，远非我等外人所能预也。我们唯一能确定的事情是，通用人工智能，即：AGI（Artificial General Intelligence）的实现，正在以肉眼可见的速度被缩短，以前在科幻电影中看到的种种场景，或许会比我们想象中来得更快一些。不过，等待 AGI 来临前的黑夜注定是漫长而孤寂的。在此期间，我们继续来探索 AI 应用落地的最佳实践，即：在成功部署本地 AI 大模型后，如何通过外挂知识库的方式为其 “注入” 新的知识。\n从 RAG \u0026amp; GPTs 开始 在上一期博客中，博主曾经有一个困惑，那就是当前阶段 AI 应用的最佳实践到底是什么？站在 2023 年的时间节点上，博主曾经以为未来属于提示词工程(Prompt Engineering)，而站在 2024 年的时间节点上，博主认为 RAG \u0026amp; GPTs 在实践方面或许要略胜一筹。在过去的一年里，我们陆陆续续看到像 Prompt Heroes、PromptBase、AI Short\u0026hellip;等等这样的提示词网站出现，甚至提示词可以像商品一样进行交易。与此同时，随着 OpenAI GPT Store 的发布，我们仿佛可以看到一种 AI 应用商店的雏形。什么是 GPTs 呢？通常是指可以让使用者量身定做 AI 助理的工具。譬如，它允许用户上传资料来丰富 ChatGPT 的知识库，允许用户使用个性化的提示词来指导 ChatGPT 的行为，允许用户整合各项技能(搜索引擎、Web API、Function Calling)\u0026hellip;等等。我们在上一期博客中提到人工智能的 “安卓时刻”，一个重要的契机是目前产生了类似应用商店的 GPT Store，如下图所示：",
            "content": "有时候，我难免不由地感慨，真实的人类世界，本就是一个巨大的娱乐圈，即使是在英雄辈出的 IT 行业。数日前，Google 正式对外发布了 Gemini 1.5 Pro，一个建立在 Transformer 和 MoE 架构上的多模态模型。可惜，这个被 Google 寄予厚望的产品并未激起多少水花，因为就在同一天 OpenAI 发布了 Sora，一个支持从文字生成视频的模型，可谓是一时风光无二。有人说，OpenAI 站在 Google 的肩膀上，用 Google 的技术疯狂刷屏。此中曲直，远非我等外人所能预也。我们唯一能确定的事情是，通用人工智能，即：AGI（Artificial General Intelligence）的实现，正在以肉眼可见的速度被缩短，以前在科幻电影中看到的种种场景，或许会比我们想象中来得更快一些。不过，等待 AGI 来临前的黑夜注定是漫长而孤寂的。在此期间，我们继续来探索 AI 应用落地的最佳实践，即：在成功部署本地 AI 大模型后，如何通过外挂知识库的方式为其 “注入” 新的知识。\n从 RAG \u0026amp; GPTs 开始 在上一期博客中，博主曾经有一个困惑，那就是当前阶段 AI 应用的最佳实践到底是什么？站在 2023 年的时间节点上，博主曾经以为未来属于提示词工程(Prompt Engineering)，而站在 2024 年的时间节点上，博主认为 RAG \u0026amp; GPTs 在实践方面或许要略胜一筹。在过去的一年里，我们陆陆续续看到像 Prompt Heroes、PromptBase、AI Short\u0026hellip;等等这样的提示词网站出现，甚至提示词可以像商品一样进行交易。与此同时，随着 OpenAI GPT Store 的发布，我们仿佛可以看到一种 AI 应用商店的雏形。什么是 GPTs 呢？通常是指可以让使用者量身定做 AI 助理的工具。譬如，它允许用户上传资料来丰富 ChatGPT 的知识库，允许用户使用个性化的提示词来指导 ChatGPT 的行为，允许用户整合各项技能(搜索引擎、Web API、Function Calling)\u0026hellip;等等。我们在上一期博客中提到人工智能的 “安卓时刻”，一个重要的契机是目前产生了类似应用商店的 GPT Store，如下图所示：\nOpenAI 推出 GPT Store\r如果你觉得 OpenAI 的 GPT Store 离我们还稍微有点距离的话，不妨了解一下 FastGPT 这个项目，它以更加直观的方式展示了一个 GPTs 是如何被创造出来的。如图所示，博主利用我的博客作为知识库创建了一个博客助手，而这一切只需要选模型、编写提示词、上传资料三个步骤即可。感兴趣的朋友可以从 这里 进行体验：\n通过 FastGPT 创建 AI 应用\r由此，我们就可以得出一个结论，目前 AI 应用落地主要还是围绕大模型微调(Fine Tuning)、提示词工程(Prompt Engineering) 以及知识增强展开，并且 GPTs 里依然有提示词参与，两者并不冲突。考虑到，大模型微调这条线存在一定的门槛，我们暂且将其放在一旁。此时，提示词工程和知识增强就成为了 AI 应用落地的关键。知识增强，专业术语为检索增强生成，即：Retrieval-Augmented Generation，RAG，其基本思路就是将大语言模型和知识库结合起来，通过外挂知识库的方式来增强大模型的生成能力。比如微软的 New Bing 是 GPT-4 + 搜索引擎的方案，而更一般的方案则是 LLM + 向量数据库的思路，下图展示了 RAG 运作的基本原理：\nRAG 运作的基本原理\r从这个角度来看，LangChain 及其衍生项目 AutoChain、Embedchain，甚至 FastGPT 等项目解决的本质都是 RAG 和 Agent 的问题。其中，Agent 不在本文的讨论范围内，这里博主不打算详细展开。接下来的内容，博主会按照这个思路进行阐述，并且以 LangChain 为例来对其中的细节进行说明。\n知识库构建 如你所见，RAG 由 LLM 和 知识库两部分组成。首先，我们来构建知识库，通常，这个过程可以划分为下面四个步骤，即：载入文档(Loader)、拆分文本(Splitter)、文本向量化(Embeddings)、向量存储(VectorStore)。\n构建知识库的步骤\rLoader 你会注意到，博主在文章中加粗显示了这四个步骤的英文描述，事实上，这代表了 LangChain 中的一部分概念，以 Loader 为例，它负责从各种文档中载入内容，下面展示了从文本文件、PDF 文件以及网页中载入内容：\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, WebBaseLoader # TextLoader # 指定编码 loader = TextLoader(\u0026#34;./input/金庸武侠小说全集/射雕英雄传.txt\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) loader.load() # 自动推断 # python -m pip install chardet loader = TextLoader(\u0026#34;./input/金庸武侠小说全集/射雕英雄传.txt\u0026#34;, autodetect_encoding=True) loader.load() # PyPDFLoader # python -m pip install pypdf loader = PyPDFLoader(\u0026#34;./input/文学作品/追风筝的人.pdf\u0026#34;) loader.load() # WebBaseLoader # python -m pip install beautifulsoup4 loader = WebBaseLoader(web_paths=(\u0026#39;https://blog.yuanpei.me\u0026#39;,), bs_kwargs={}) loader.load() 当然，现实中通常会有很多文档，此时，我们可以使用 DirectoryLoader 来一次性载入多个文档：\nfrom langchain_community.document_loaders import DirectoryLoader loader = DirectoryLoader(\u0026#34;./posts/\u0026#34;, glob=\u0026#34;*.md\u0026#34;, loader_kwargs={}, show_progress=True, silent_errors=True) 默认情况下，DirectoryLoader 使用 UnstructuredFileLoader 这个通用的 Loader 来兼容各种格式的文件，不过，你依然可以使用 loader_cls 参数来指定 Loader 类型：\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader loader = DirectoryLoader(\u0026#34;./posts/\u0026#34;, glob=\u0026#34;*.md\u0026#34;, loader_cls=TextLoader, loader_kwargs={}, show_progress=True, silent_errors=True) Splitter 调用 Loader 的 load() 方法，返回的是一组 Document 的集合。此时，我们可以将这些 Document 交给 TextSplitter 来对分本内容的分割，因为我们最终需要对文本块做向量化处理：\nfrom langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter text_splitter = CharacterTextSplitter(separator = \u0026#34;\\n\\n\u0026#34;, chunk_size = 600, chunk_overlap = 100, length_function = len) text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) 这里，博主先后使用了 CharacterTextSplitter 和 RecursiveCharacterTextSplitter 两种 Splitter，两者的区别是：CharacterTextSplitter 会将文本按照单个字符进行分割，而 RecursiveCharacterTextSplitter 则会将文本按照连续的多个字符进行分割。不过，经过博主的测试，两种分割方式最终对 LLM 的影响微乎其微，大家可以参照官方文档中的说明，选择符合个人预期的分割方法：\nLangChain 中不同的 TextSplitter\r按照一般的流程，我们只需要按下面的方式，即可完成文档的分割：\ndocuments = loader.load()。 text_splitter.split_documents(documents) 不过，实际操作中更推荐使用 load_and_Split() 方法，两步合并为一步，更简洁一点：\ndocuments = loader.load_and_split(text_splitter) Embeddings 经过 Splitter 处理以后，我们将得到一系列文本块，它依然是一组 Document 集合。此时，便轮到 Embeddings 出场，它将负责将文本块向量化。这里，博主使用的是 HuggingFaceEmbeddings，首次运行它会自动下载模型文件。当然，我天朝上国自有国情在此，从 Hugging Face 下载模型的问题在 AI 的道路上可谓是阴魂不散。因此，下面演示的是，通过镜像站来下载模型的方法。考虑到，默认的 sentence-transformers/all-mpnet-base-v2 模型是英文模型，我们引入一个对中文更友好的 GanymedeNil/text2vec-large-chinese 模型:\nimport os from langchain_community.embeddings import HuggingFaceEmbeddings os.environ[\u0026#34;HF_ENDPOINT\u0026#34;] = \u0026#34;https://hf-mirror.com\u0026#34; embeddings = HuggingFaceEmbeddings(model_name=\u0026#34;GanymedeNil/text2vec-large-chinese\u0026#34;) # 单个句子 print(embeddings.embed_query(\u0026#34;人生若只如初见，何事秋风悲画扇\u0026#34;)) # 多个句子 print(embeddings.embed_documents([\u0026#34;人生若只如初见，何事秋风悲画扇\u0026#34;,\u0026#34;等闲变却故人心，却道故人心易变\u0026#34;])) 当然，从最终向量化以后的结果来看，前者的向量维数只有 768，而后者则可以达到 1024。至此，我们就完成文本内容的向量化。一旦所有的文本信息都被转换为向量数据，此时，信息检索就完完全全地变成了一道数学题，由向量的余弦公式，我们可以非常容易地计算出两个向量间的夹角，这个夹角越小，则表明两个向量越相近。这就是向量相似性检索的基本原理，在此前的文章《视频是不能 P 的系列：使用 Milvus 实现海量人脸快速检索》一文中，博主曾经尝试利用这种思路来优化人脸识别效率，不知道大家是否还留下印象🙃\u0026hellip;\nVectorStore OK，当我们将文本信息转化为向量以后，就需要考虑如何储存这些向量信息，如上文中给出的图片所示，LangChain 中支持诸如 FAISS、Milvus、Pinecone、Chroma\u0026hellip;等多种向量数据库，以及像 Elasticsearch、PostgreSQL 这样的具备向量存储能力的传统数据库，我们可以根据自己的需求选择合适的存储方案。为了方便演示，这里我们使用 Facebook 出品的 FAISS，它不需要像 Milvus 那样准备额外的环境，使用体验上更接近 SQLite 这种嵌入式的数据库。当然，如果你比较中意于 Chroma，一切行止由心，LangChain 在自由度上拉满：\nfrom langchain_community.vectorstores.faiss import FAISS import pickle # python -m pip install faiss-cpu vector_store = FAISS.from_documents(documents, embeddings) with open(\u0026#39;./output/\u0026lt;Your-Persistence-Path\u0026gt;\u0026#39;, \u0026#34;wb\u0026#34;) as f: pickle.dump(vector_store, f) FastGPT 中创建知识库的过程，原理完全相同，它支持从文档或者网址构建知识库，最终将向量化数据存储在 MonggoDB 中，目前，这个项目使用的 FastAI 4K 模型来源不明，而 Embedding-2 模型则是来自于 OpenAI。所以，FastGPT 这个网站最大的资金消耗主要来自 FastAI，目测是一个独立训练出来的私有模型。下图展示了博主在本地构建知识库的过程，如你所见，博主使用了 263 篇博客的内容来构建这个知识库：\n本地知识库构建过程\rLLaMA 的再度整合 坦白讲，纯 CPU 环境下的知识库非常花时间，譬如，博主使用 1024 个维度的向量来储存金庸先生的 15 部小说，经序列化后的文件体积高达 1.2G。果然，大模型相关的一切事物都非常庞大，这或许能帮大家理解类似 7B、13B、60B\u0026hellip;等等的大模型的本质，因为它们同样是由大量的高精度的、浮点型的向量数据构成。可偏偏就是这些对人类来说毫无意义、宛如天书一般的数字，能够从你的只言片语中“理解”你的意图，回应你的各种输入，难道你还能笃定，这一切不是某种魔法，而提示词不过是施展魔法时吟唱的咒语？从这个角度来看，RAG 是什么呢？RAG 更像是你在吟唱咒语时随手拿一本魔法书在手里，你随时可以从这本魔法书里查询资料完善咒语。\nRAG 如何工作\rRAG 与 Prompt 现在，我们顺着这个思路，将 LLaMA 和 LangChain 结合起来，这里的 LLaMA 指的是 llama.cpp + Qwen-1_8B-Chat，这个方案我们在上一期博客已经介绍过了，这里不再赘述。我们先来看看 Retrieve 的过程，即：如何从向量数据库中找到问题相关的内容，显然，这是 RAG 里的第一步：\nwith open(\u0026#34;./output/个人博客.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: vectorstore = pickle.load(f) query = \u0026#39;Envoy 在微服务中的应用场景有哪些\u0026#39; topK = 3 # 普通的相似性检索 result = vectorstore.similarity_search(query=query, k=topK) # 带有分数的相似性检索 result = vectorstore.similarity_search_with_relevance_scores(query=query, k=topK) # 最大边界相关性检索 result = vectorstore.max_marginal_relevance_search(query=query, k=topK) 这里，博主例举出了常用的向量相似性检索的方法，大家可以参考 官方文档 选择合适的方法，通常情况下，使用 similarity_search() 方法即可；可有时候向量数据库会检索出与问题相关性不强的内容，此时，就需要使用 similarity_search_with_relevance_scores() 配合某个阈值进行处理；第三种方法，我个人感觉，其返回结果的相关性不如前两种方法。还是那句话，目前博主的 AI 道路还在摸索阶段，大家酌情使用即可。\nfrom langchain.prompts.prompt import PromptTemplate # 定义提示词模板 PROMPT_TEMPLATE = \u0026#34;\u0026#34;\u0026#34; You are a helpful AI bot. Your name is {name}. Please answer the question only based on the following context: {context} If the question is about your identity or role or name, answer \u0026#39;{name}\u0026#39; directly, no need to refer to the context. If the context is not enough to support the generation of an answer, Please return \u0026#34;I\u0026#39;m sorry, I can\u0026#39;t answer your question.\u0026#34; immediately. You have an opportunity to refine the existing answer (only if needed) with current context. You must always answer the question in Chinese. Queation: {question} \u0026#34;\u0026#34;\u0026#34; # 根据问题做相似性检索 question = \u0026#34;Envoy在微服务中都有哪些应用场景\u0026#34; documents = vector_store.similarity_search(question, k=3) context = \u0026#39;\\n\\n\u0026#39;.join([document.page_content for document in documents]) # 从模板创建提示词并填充模板 prompt = PromptTemplate.from_template(PROMPT_TEMPLATE) query = prompt.format(question=question, name=\u0026#34;ChatGPT\u0026#34;, context=context) RAG 中的 A 是指 Augmented，表示增强的，这个增强体现在哪里呢？答案是提示词。如图所示，在这段提示词里面，博主试图让大语言模型假扮 ChatGPT，同时告诉它按照给定的上下文来回答问题，并且要求它必须使用中文进行回答。通过这段代码，我们就可以产生符合我们预期的输入，而接下来，我们只需要将其传递给大语言模型即可。除了这种方式以外，我们还可以按 OpenAI 的规范，使用 ChatPromptTemplate 来组织提示词：\nfrom langchain_core.prompts import ChatPromptTemplate # 定义提示词模板 PROMPT_TEMPLATE = \u0026#34;\u0026#34;\u0026#34; You are a helpful AI bot. Your name is {name}. Please answer the question only based on the following context: {context} If the question is about your identity or role or name, answer \u0026#39;{name}\u0026#39; directly, no need to refer to the context. If the context is not enough to support the generation of an answer, Please return \u0026#34;I\u0026#39;m sorry, I can\u0026#39;t answer your question.\u0026#34; immediately. You have an opportunity to refine the existing answer (only if needed) with current context. You must always answer the question in Chinese. \u0026#34;\u0026#34;\u0026#34; # 根据问题做相似性检索 question = \u0026#34;Envoy在微服务中都有哪些应用场景\u0026#34; documents = vector_store.similarity_search(question, k=3) context = \u0026#39;\\n\\n\u0026#39;.join([document.page_content for document in documents]) # 从模板创建提示词并填充模板 prompt = ChatPromptTemplate.from_messages([ (\u0026#34;system\u0026#34;, PROMPT_TEMPLATE), (\u0026#34;user\u0026#34;, \u0026#34;{question}\u0026#34;), ]) query = prompt.format(question=question, name=\u0026#34;ChatGPT\u0026#34;, context=context) 显而易见，第二种更好一点，因为它可以将问题和上下文显著地区分开来。当然，为了让 LLM 确信它就是 ChatGPT，博主在提示词工程上可没少下功夫。这其实佐证了博主一开始的观点：2023 年的提示词工程，2024 年的 GPTs，这两者并不冲突，而是一种相辅相成的关系。RAG 中的 Retrieve 和 Augmented 现在都已先后登场， 最后这个 G 自然是指 Generation，理论上剩下的工作交给 LLM 即可。可是如何你仔细想想，就会发现这里隐含着两个问题，其一是从向量数据库中检索到的信息，有一定的可能性超过 LLM 的上下文长度；其二是对上下文的约束越严格，LLM 就越显得 “弱智”，你必须要在发散和收敛中做出选择。以一言蔽之，LLM 有幻觉固然不好，可如果缺乏想象力，一切只怕是会变得更糟糕！不知大家是如何考虑这个问题的呢？\nLangChain 中的 Chain 在 LangChain 的诸多概念中，Chain 或许是最抽象、最重要的哪一个，因为它就像一个管道一样，可以讲我们这篇文章中提到的各种组件串联起来。譬如， LLMChain 可以将一个LLM 和 Prompt 串联起来，RetrievalQA 可以配合 LLM 和知识库实现简单的 Q\u0026amp;A，ConversationalRetrievalChain 可以配合 LLM、知识库和聊天历史实现对话式检索。起初，我对于 RetrievalQA 这个类是极其排斥和反感的，因为它不论是看起来还是用起来，都像极了一个 Chain，可它就偏偏不是一个 Chain，真是奇哉怪也！\nfrom langchain.chains import LLMChain from langchain_community.llms import OpenAI from langchain_core.prompts import PromptTemplate prompt_template = \u0026#34;Tell me a {adjective} joke\u0026#34; prompt = PromptTemplate( input_variables=[\u0026#34;adjective\u0026#34;], template=prompt_template ) llm = LLMChain(llm=OpenAI(), prompt=prompt) 刚刚提到，我们大体上有两种方式来将 LLM 和 LangChain 结合起来，一种是基本的 Q\u0026amp;A，一种是对话式检索。其中，streaming 参数用于支持流式返回，return_source_documents 参与用于控制是否返回引用的文档信息，ConversationBufferMemory 组件用于处理对话历史。下面给出具体的实现代码：\n# 基本的 Q\u0026amp;A def get_basic_qa_chain(baseUrl=\u0026#39;\u0026#39;, apiKey=\u0026#39;\u0026#39;, storeFilePath=\u0026#39;\u0026#39;): llm = ChatOpenAI( model_name=\u0026#34;gpt-3.5-turbo\u0026#34;, temperature=0.75, openai_api_base=baseUrl, openai_api_key=apiKey, streaming=True ) retriever = load_retriever(storeFilePath) chain = RetrievalQA.from_chain_type( llm=llm, chain_type=\u0026#34;stuff\u0026#34;, retriever=retriever, return_source_documents=True, ) return chain # 对话式检索 def get_conversational_retrieval_chain(baseUrl=\u0026#39;\u0026#39;, apiKey=\u0026#39;\u0026#39;, storeFilePath=\u0026#39;\u0026#39;): llm = ChatOpenAI( model_name=\u0026#34;gpt-3.5-turbo\u0026#34;, temperature=0.75, openai_api_base=baseUrl, openai_api_key=apiKey, streaming=True ) retriever = load_retriever(storeFilePath) memory = ConversationBufferMemory( memory_key=\u0026#34;chat_history\u0026#34;, return_messages=True, input_key=\u0026#34;question\u0026#34;, output_key=\u0026#34;source_documents\u0026#34; ) chain = ConversationalRetrievalChain.from_llm( llm=llm, retriever=retriever, memory=memory, return_source_documents=True, ) return chain 现在，我们就可以调用 Chain 来和大语言模型及知识库进行交互：\n# 构造 Chain，使用本地模型 chain = get_conversational_retrieval_chain_(\u0026#34;http://localhost:8080/v1/\u0026#34;, \u0026#39;sk-1234567890\u0026#39;, \u0026#39;./output/个人博客.pkl\u0026#39;) # 根据问题做相似性检索 question = \u0026#34;Envoy在微服务中都有哪些应用场景\u0026#34; documents = vector_store.similarity_search(question, k=3) context = \u0026#39;\\n\\n\u0026#39;.join([document.page_content for document in documents]) # 从模板创建提示词并填充模板 prompt = ChatPromptTemplate.from_messages([ (\u0026#34;system\u0026#34;, PROMPT_TEMPLATE), (\u0026#34;user\u0026#34;, \u0026#34;{question}\u0026#34;), ]) query = prompt.format(question=question, name=\u0026#34;ChatGPT\u0026#34;, context=context) # 方式一： 同步调用 result = chain.invoke(query) # 方式二： 流式调用 result = chain.stream(query) 如下图所示，在经过反复地调试和优化以后，我们可以针对博客中的内容进行提问，并且程序会返回与问题相关的文档信息，这个结果整体而言还是挺不错的。当然，距离 FastGPT 这种相对完善的产品还是挺遥远的，使用 CPU 进行推理的 llama.cpp 除了生成速度慢以外，还会出现缓存 kv cache 不足的问题。庆幸的是，在折腾 LangChain 的过程中，逐渐理解了 RAG 以及 LangChain 的整体思路。在 LangChain 的 官方文档 中，官方提供了使用 LangChain 落地 AI 应用的示例，这篇文章主要参考了 RAG 这一篇，大家可以特别关注一下。\n本地 AI 知识库运行结果展示\rLangChain 除了概念多以外，API 变动非常频繁，经常出现破坏性的变更，而且官方现在主要在推 LangChain Expression Language(LCEL)，所以，网络上的资料经常都是过期的，建议大家有时间还是去看纯英文的官方文档。当然，大家有兴趣的话，可以考虑下 LangChain 的替代品，譬如 AutoChain 和 Embedchain，它们的 API 毫无疑问都比 LangChain 简单，实际使用情况，只有靠各位自己去体验啦，哈哈！\n本文小结 在探索 AI 的过程中，一个令人难过的事实是：为了创作这篇大约1.5万字的文章，博主先后花了一周左右的时间来做实验，然后花了两天的时间来形成文字，而在将这些博客内容 “投喂” 给 AI 以后，它可以迅速 “描摹” 出我的轮廓。你不得不承认，这个世界的信息实在多到爆炸，以致于我们绝无可能构建出一个无穷大的知识库。AI 更像是这个复杂时代地观察者，可以帮助我们更快、更好地去理解这个世界，虽然这个代价是 AI 可能会产生更多噪音，这有一点像什么呢？就像你在看视频时开启倍速播放，或许你节省了时间、提升了节奏，可终究失去了某种身体力行的体验感，正如在这篇文章里，我使用 LLaMA 和 LangChain 实现 RAG 的过程，于屏幕前的你而言，不过是互联网海洋里鼠标轻轻掠起的浪花。对我而言，写作的整个过程，与 RAG 更是何其的相似，我在输出这篇文章的同时，查阅大量的资料，最终，它们成为了我认知、感悟以及价值观的一部分，而这，便是自我的增强。\n",
            "tags": ["LLaMA","LangChain","RAG","GPTs"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "为你的服务器集成 LDAP 认证",
            "date": "2022-11-15",
            "permalink": "http://localhost:1313/posts/integrate-ldap-authentication-for-your-server/",
            "summary": "回顾我这些年的工作经历，面向企业(2B)和面向用户(2C)的项目都曾接触过。我个人觉得，面向企业的项目更注重业务，参与决策的人数多、周期长，目的是为企业提供生产经营价值，如缩减成本、提升效率等等，而面向用户的项目更注重体验，参与决策的人数少、周期短，目的是为消费者提供更多的使用价值，本质上是为了圈揽用户和抢夺流量。我在参与这些项目的过程中发现，企业级应用的研发更注重与第三方软件如 SAP、金蝶、用友、ERP 等等的整合，因此，类似单点登录、数据同步这样的需求非常普遍。每当这个时候，我就不由地想起一位前辈。\n时间就像沙漏里的沙一样流逝\r当我还在 Automation 打杂的时候，前辈总是一脸得意地问我：“听说过 AD Domain 吗？”。那时，初出茅庐的我年少轻狂，不好意思说我不会，立马敷衍道：“当然听说过，只是一直没用过”。前辈目光如炬，大抵是看出我心虚，立马不屑一顾地回应道：“那就是不会”。过了几秒钟，前辈不紧不慢地接着说道：“只有学会了 AD Domain，你才算是一只脚踏进了企业级应用开发这个领域，知道吗？”，我点了点头，心道：“这不就和茴香豆的茴字有五种写法一样无聊吗？”。多年后，当 LDAP 这个字眼再次映入眼帘的时候，我内心终于清楚地知道：我错了。\n为什么需要 LDAP 认证 我错在哪里了呢？我想，要回答这个问题，还是需要从企业管理的角度来着手。一个面向用户(2C)的产品，其用户基本上是不受地域因素限制的，而对于一个面向企业(2B)的产品，其用户基本上是在一个层次分明、有着明显边界的范围内。运营一个企业，除了业务系统以外，可能还需要 OA、财务、ERP 等等外围软件的支持，如果是一家互联网公司，可能还需要 DevOps、监控、协作等等方面的支撑。此时，从企业的角度自然是希望可以统一账号体系，这样就衍生出了各种各样的单点登陆和认证方案，单单是博主接触过的就有：OAuth2、CAS、Keycloak、IdentityServer4，这些方案可以说是各有千秋，此中曲折我们按下不表。\n运行在 Windows Server 上的 AD\r这里博主想说的是，一旦企业通过 AD Domain 或者说 Active Directory 来管理用户，就自然而然地牵扯出域登录或者域账号登录的问题。这类围绕 AD Domain 或者说域的问题，我们都可以考虑使用 LDAP 认证或者 Kerberos 认证，特别是后者，主流的软件如 Kafka、Zookeeper、MySQL 等等均支持这一协议，它可以实现在登录本地账户后，免登录打开一个网站的效果。可想而知，这是一个对企业而言极具诱惑力的特性，一个账号打通所有基础设施。当然，我承认 Kerberos 这个协议是非常复杂的，绝非三言两语可以厘清其中的千丝万缕，所以，我们今天只是聊聊 LDAP 认证这个话题。\n通过 LDAP Browser 访问 AD\r可能大家会纠结，LDAP 和 Active Directory 这两者间的关系，事实上， LDAP 是指轻量目录访问协议(Lightweight Directory Access Protocol)，而 Active Directory 则是微软针对该协议的一种实现。当然，微软为了解决域控的问题，利用 LDAP 存储了一部分私有的数据。所以，两者的关系就像是接口和实现类，我们这里只需要 Active Directory 当成一台 LDAP 服务器即可。关于 Active Directory 的基础知识，这里不再做更多的科普。总而言之，通过 LDAP 我们可以对某个网站实现认证，从而达到保护资源的目的。譬如博主目前参与的前端项目，它是没有常规的登录、注册页面的，它采用的就是域账号登录的形式。下面，我们来看看如何集成 LDAP 认证。",
            "content": "回顾我这些年的工作经历，面向企业(2B)和面向用户(2C)的项目都曾接触过。我个人觉得，面向企业的项目更注重业务，参与决策的人数多、周期长，目的是为企业提供生产经营价值，如缩减成本、提升效率等等，而面向用户的项目更注重体验，参与决策的人数少、周期短，目的是为消费者提供更多的使用价值，本质上是为了圈揽用户和抢夺流量。我在参与这些项目的过程中发现，企业级应用的研发更注重与第三方软件如 SAP、金蝶、用友、ERP 等等的整合，因此，类似单点登录、数据同步这样的需求非常普遍。每当这个时候，我就不由地想起一位前辈。\n时间就像沙漏里的沙一样流逝\r当我还在 Automation 打杂的时候，前辈总是一脸得意地问我：“听说过 AD Domain 吗？”。那时，初出茅庐的我年少轻狂，不好意思说我不会，立马敷衍道：“当然听说过，只是一直没用过”。前辈目光如炬，大抵是看出我心虚，立马不屑一顾地回应道：“那就是不会”。过了几秒钟，前辈不紧不慢地接着说道：“只有学会了 AD Domain，你才算是一只脚踏进了企业级应用开发这个领域，知道吗？”，我点了点头，心道：“这不就和茴香豆的茴字有五种写法一样无聊吗？”。多年后，当 LDAP 这个字眼再次映入眼帘的时候，我内心终于清楚地知道：我错了。\n为什么需要 LDAP 认证 我错在哪里了呢？我想，要回答这个问题，还是需要从企业管理的角度来着手。一个面向用户(2C)的产品，其用户基本上是不受地域因素限制的，而对于一个面向企业(2B)的产品，其用户基本上是在一个层次分明、有着明显边界的范围内。运营一个企业，除了业务系统以外，可能还需要 OA、财务、ERP 等等外围软件的支持，如果是一家互联网公司，可能还需要 DevOps、监控、协作等等方面的支撑。此时，从企业的角度自然是希望可以统一账号体系，这样就衍生出了各种各样的单点登陆和认证方案，单单是博主接触过的就有：OAuth2、CAS、Keycloak、IdentityServer4，这些方案可以说是各有千秋，此中曲折我们按下不表。\n运行在 Windows Server 上的 AD\r这里博主想说的是，一旦企业通过 AD Domain 或者说 Active Directory 来管理用户，就自然而然地牵扯出域登录或者域账号登录的问题。这类围绕 AD Domain 或者说域的问题，我们都可以考虑使用 LDAP 认证或者 Kerberos 认证，特别是后者，主流的软件如 Kafka、Zookeeper、MySQL 等等均支持这一协议，它可以实现在登录本地账户后，免登录打开一个网站的效果。可想而知，这是一个对企业而言极具诱惑力的特性，一个账号打通所有基础设施。当然，我承认 Kerberos 这个协议是非常复杂的，绝非三言两语可以厘清其中的千丝万缕，所以，我们今天只是聊聊 LDAP 认证这个话题。\n通过 LDAP Browser 访问 AD\r可能大家会纠结，LDAP 和 Active Directory 这两者间的关系，事实上， LDAP 是指轻量目录访问协议(Lightweight Directory Access Protocol)，而 Active Directory 则是微软针对该协议的一种实现。当然，微软为了解决域控的问题，利用 LDAP 存储了一部分私有的数据。所以，两者的关系就像是接口和实现类，我们这里只需要 Active Directory 当成一台 LDAP 服务器即可。关于 Active Directory 的基础知识，这里不再做更多的科普。总而言之，通过 LDAP 我们可以对某个网站实现认证，从而达到保护资源的目的。譬如博主目前参与的前端项目，它是没有常规的登录、注册页面的，它采用的就是域账号登录的形式。下面，我们来看看如何集成 LDAP 认证。\n如何集成 LDAP 认证 结合博主在上文中描述的场景，假设我们有一个前端项目通过 Nginx 或者 Apache 进行托管。通常情况下，我们可以直接访问这些前端页面，这意味这些资源是不受任何保护的。对企业来说，它更希望将这些资源保护起来，以确保只有它的员工或者说加入域的用户才可以访问，此时，我们该如何解决这个问题呢？\nNginx 篇 以 Nginx 为例，我们可以通过 nginx-auth-ldap 这个模块来解决这个问题。下面是一个简单的示意图，其基本思路是：在输入用户名和密码后，该模块会连接 LDAP 服务器对用户身份进行校验，如果校验通过，则会以 Basic 认证的的方式生成 Authorization 请求头；反之，将会返回 401 状态码，表示认证失败。\nNginx 集成 LDAP 示意图\r注意到，这是一个第三方的模块，不管你是通过 Docker 或者主机来部署 Nginx，它都不会包含这个模块，此时，我们就需要为 Nginx 安装这个模块。因为 Nginx 采用的是静态编译的策略，所以，安装模块本质上就是同时拉取 Nginx 和模块的源码，然后重新编译生成二进制文件的过程。首先，我们来下载该模块的源码：\ngit clone https://github.com/kvspb/nginx-auth-ldap.git /usr/src/nginx-auth-ldap/; 接下来，我们需要下载 Nginx 源码，无论你是在全新安装的 Nginx 上安装模块，还是在一个安装好的 Nginx 上安装模块，这一步都是必须的，千言万语汇成一句话：Nginx 采用的是静态编译的策略。博主这里是在 nginx:stable-alpine 这个镜像的基础上安装模块：\nwget http://nginx.org/download/nginx-1.22.1.tar.gz tar -zxvf nginx-1.22.1.tar.gz cd nginx-1.22.1 接下来，如果是全新安装 Nginx，那么，你可以像下面这样列出常用的模块，然后用 --add-module 参数指向当前模块的路径：/usr/src/nginx-auth-ldap/ 。请注意，下面的参数经过简化，并不代表实际使用的参数：\n./configure \\ --prefix=/etc/nginx \\ --sbin-path=/usr/sbin/nginx \\ --with-http_addition_module \\ --with-http_auth_request_module \\ --add-module=/usr/src/nginx-auth-ldap/ make \u0026amp;\u0026amp; make install 如果是安装好的 Nginx，那么，这个参数就必须从当前的 Nginx 上继承过来，否则，已经安装好的模块将不会参与编译。原本博主非常喜欢 Nginx ，可这一条博主实在不能容忍，这完全是在用户面前摆烂，难道使用了哪些了模块你心里没点数吗？非要逼着用户去帮你维护这些配置信息？这些又臭又长的参数真的不考虑做持久化吗？吐槽完 Nginx，我们继续来填坑，为了获取 Nginx 当前的配置参数，你可以使用 nginx -V 这个命令(注意：这个 V 必须要大写)，此时，我们就可以得到下面的答案：\n获取 Nginx 配置参数\r这意味着什么呢？这意味着你需要把红框里的这一堆参数放到 ./configure 后面，然后再像上面一样添加 --add-module 参数，这简直是造孽啊！为了简化这个过程，我改进了一下脚本：\n# 匹配 configure arguments ，然后再从第 21 个字符开始截取，即冒号后面的所有内容 NGINX_CONFIG=$(nginx -V 2\u0026gt;\u0026amp;1 | grep \u0026#39;configure arguments\u0026#39; | cut -c21-) ./configure $NGINX_CONFIG --add-module=/usr/src/nginx-auth-ldap/ make \u0026amp;\u0026amp; make install 当然，如果你亲自折腾过这一切，就知道这是痴心妄想，因为提取出来的这组参数会提示各种各样的错误，总之，你需要按照实际的情况来对这组参数就行调整，看着 Dockerfile 里比裹脚布还要长的参数，Nginx 官方难道你们不会心痛吗？当然，当你熬过这一切以后，我们就可以着手 Nginx 的配置啦：\nhttp { # 定义 LDAP 服务器 ldap_server ldap { # LDAP 服务器地址，使用 sAMAccountName 还是 uid 以实际的 AD 配置为准 url ldap://\u0026lt;IP\u0026gt;:389/OU=OPS,DC=company,DC=com?sAMAccountName?one; # 管理员账号, CN/OU/DC 以实际的 AD 配置为准 binddn \u0026#34;CN=\u0026lt;Administrator\u0026gt;,OU=OPS,DC=company,DC=com\u0026#34;; # 管理员密码 binddn_passwd \u0026#34;\u0026lt;Password\u0026gt;\u0026#34;; group_attribute uniquemember; group_attribute_is_dn on; require valid_user; } server { listen 80; server_name localhost; location / { # 启用 LDAP 认证 auth_ldap \u0026#34;Forbidden\u0026#34;; auth_ldap_servers ldap; root /usr/nginx/wwwroot; index index.html; } } } 可以注意到，我们只需要按实际域控来配置 ldap_server 节点，然后为受保护的资源启用 LDAP 认证即可。此时，如果访问前端页面，浏览器将会提示输入用户名和密码：\nNginx 集成 LDAP 效果演示-1\r如果我们输入的用户名或者密码不正确会怎么样呢？浏览器将会孜孜不倦地提示你输入用户名和密码。如果我们点击取消会怎么样呢？此时，如下图所示，它将会返回 401 状态码，表示认证失败：\nNginx 集成 LDAP 效果演示-2\r此外，我们可以注意到，一旦认证通过，我们就可以正常访问前端页面。与此同时，所有的请求都会自动带上 Authorization 请求头，显然，这是一个 Basic 认证：\nNginx 集成 LDAP 效果演示-3\rNginx 集成 LDAP 效果演示-5\r至此，我们就实现了 Nginx 下的 LDAP 认证集成。\nApache 篇 虽然 Nginx 比 Apache 更轻量、社区更活跃，可是在模块管理这方面，Apache 是完全吊打 Nginx 的，类似地，Apache 通过 mod-authnz-ldap 这个模块来实现 LDAP 认证的集成，并且这个模块有二进制包，可以直接通过包管理器来安装，从这方面来看，Nginx 完全不如 Apache ：\n# 安装模块 apt-get update; apt-get install -y libldap2 mod_ldap mod-authnz-ldap; # 启用模块 a2enmod ldap; a2enmod authnz_ldap; 接下来，我们只需要修改一下 Apache 的配置文件即可：\n\u0026lt;VirtualHost *:80\u0026gt; ServerName localhost ServerAdmin webmaster@localhost DocumentRoot /var/www/html LogLevel debug ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined \u0026lt;Location \u0026#34;/\u0026#34;\u0026gt; AuthType Basic AuthName \u0026#34;LDAP SSO\u0026#34; AuthBasicProvider ldap AuthLDAPBindDN \u0026#34;CN=\u0026lt;Administrator\u0026gt;,OU=OPS,DC=company,DC=com\u0026#34; AuthLDAPBindPassword \u0026#34;\u0026lt;Password\u0026gt;\u0026#34; AuthLDAPURL ldap://\u0026lt;IP\u0026gt;:389/OU=OPS,DC=company,DC=com?sAMAccountName?one Require valid-user \u0026lt;/Location\u0026gt; \u0026lt;/VirtualHost\u0026gt; 可以注意到，两者的配置是非常相似，可以实现相同的效果，这里就不再详细展示啦！\nBackend 篇 在前面的示意图里，我们看到还有 Backend 这样一个环节。其实，很多时候，我觉得企业级应用的开发非常别扭，因为它总是在现代中透着些古板，说它现代是因为它在跟进微服务、前后端分离、容器化这些流行趋势，说它古板是因为它总在尝试用新技术做旧时代的东西。譬如，前后端分离以后，JWT 是最常见的认证方式，在这种模式下，服务就应该是无状态的，可实际开发中它还是会搞出来一个 Session 的概念，难道现在还是汤姆猫的时代吗？这个域账号登录的想法确实不错，可它和现在主流的 JWT 是不兼容的，除非在服务器端实现了 Basic 认证。下面是 ASP.NET Core 中实现 Basic 认证的一个简单流程：\nasync Task Invoke(HttpContext context) { try { // 1.提取用户信息 var header = context.Request.Headers[\u0026#34;Authorization\u0026#34;] var authHeader = AuthenticationHeaderValue.Parse(header); var authParams = Convert.FromBase64String(authHeader.Parameter); var credentials = Encoding.UTF8.GetString(authParams).Split(\u0026#39;:\u0026#39;, 2); var username = credentials[0]; var password = credentials[1]; // 2.验证用户信息 var user = await _userService.Authenticate(username, password); if (user != null) { // 3.指定当前用户 var claims = new[] { new Claim(ClaimTypes.NameIdentifier, user.Id.ToString()), new Claim(ClaimTypes.Name, user.UserName), }; var identity = new ClaimsIdentity(claims, \u0026#34;Basic\u0026#34;); var principal = new ClaimsPrincipal(identity); context.User = principal; } } catch {} await _next(context); } 可以看到，Basic 认证的过程其实就是从 Authorization 请求头里提取用户信息并进行验证的过程，博主这里是以中间件的形式来进行说明。按照一般的做法，你可能需要继承 AuthenticationHandler 并重写 HandleAuthenticateAsync() 和 HandleChallengeAsync() 这两个方法，思路其实是完全一样的，这里就不再详细展开说啦！总而言之，如果前端想结合 LDAP 做身份认证，需要后端提供相应的支持。当然，如果你可以在请求传入到后端前自动产生一个 JWT 令牌，那再好不过啦！\n本文小结 本文分享了 Nginx 和 Apache 结合 LDAP 实现身份认证的过程，背景则是企业级应用开发过程中对单点登陆、域账号登录的诉求。在这个过程中，博主对 Nginx 和 Apache 这两款服务器的差异有了更深刻的认识。考虑到 Nginx 采用的静态编译的策略，因此，给 Nginx 安装模块，本质上就是重新编译 Nginx 的过程，这个过程可谓是痛并快乐着，就像你在文章里看到的那样，博主已经不止一次吐槽过 Nginx 这个奇葩的模块管理方式。相比之下，Apache 的舒适感简直爆棚，因为它的模块都可以直接通过包管理器来安装。当我们为网站集成了 LDAP 认证以后，它会在打开站点时提示输入用户名和密码，一旦身份验证通过，Nginx 或者 Apache 会自动地为每个请求生成 Authorization 请求头，考虑到这个认证方式和目前主流的 JWT 认证不兼容，因此，这个方案需要后端实现 Basic 认证。在此基础上，博主提供了一个 ASP.NET Core 实现 Basic 认证的示例。有趣的是，虽然 Kerberos 协议被指出存在安全漏洞，可它还是在用 Basic 认证来传递用户信息，那么，你告诉我，企业应用需要的这种认证方案，到底是好是坏呢？\n",
            "tags": ["Apache","Nginx","LDAP","认证"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "支持外部链接跳转的 Vue Router 扩展实现",
            "date": "2022-07-12",
            "permalink": "http://localhost:1313/posts/implementation-of-vue-router-extension-that-supports-external-link/",
            "summary": "众所周知，Vue Router 是 Vue 中重要的插件之一，特别是在当下流行的 单页面应用/SPA 中，这种感觉会越来越明显。此时，路由的作用就是根据 URL 来决定要显示什么内容。诚然，页面这个概念在工程/模块中依然存在，可当你开始关注最终发布的产物时，你会发现本质上它只有一个页面。无论你选择 hash 或者是 history 模式的路由，它都像是在同一张纸上反复写写画画，让你看起来觉得它有很多个不同的页面。回顾早期的前端项目，它往往会有多个不同的页面组成，我们是通过一个个的超链接来实现不同页面间的跳转。如今，这一切都已一去不复返，我们只能在单页面应用的世界里继续披荆斩棘。当然，绝大多数的普通用户无法感知到这种程度的变化，在他们的眼中，那依然不过是普通的一个超链接。那么，当一个项目中充斥着各种各样的超链接的时候，这个问题就值得我们单独拿出来讲一讲。所以，今天这篇博客的主题是路由和外部链接。请注意，这是一组相对通用的概念，不受限于任何一个前端框架，我们只是选择了使用 Vue 来进行说明。\n问题现状 我们的项目存在着大量的超链接以及导航菜单，在 UI 设计阶段，通常不会有人关心，一个链接到底是内部链接还是外部链接。与此同时，由于 HTML 这门标记语言的极大灵活性，实现一个导航链接的方式有 N 多种，可以是一个 a 标签，可以是一个 div 标签，甚至可以是一个 span 标签。虽然 Vue Router 里提供了 router-link 组件，可在实际的项目中，需要综合考虑团队风格和第三方 UI 库的因素，甚至有时候，再没有设计规范的情况下，可能大家连 router-link 组件都不愿意用或者说压根就没机会用。\n这样就造成一个非常尴尬的局面，当你需要为页面编写业务代码的时候，你不得不在各种各样的超链接上浪费时间，只要不是通过 a 标签实现的，你都必须处理它点击的事件，更不必说，你还要区分这个链接是一个内部链接还是一个外部链接，原因是 Vue Router 不支持外部链接，你不得不通过 window.location 或者 window.open() 的这样的方式来实现“曲线救国”，试想，如果每一个都这么折腾一遍，你还会觉得有趣吗？\n而在我们的项目里，实际上它还需要从网页端唤起应用，这样便又涉及到了 URL Schemes 这个话题。除了 Android 和 iOS 这个平台上的差异，单单就 Windows 而言，其基于注册表的方案对协议提供者的约束并不强，如果团队内对此没有任何规范的话，你将面对各种千奇百怪的参数传递方式。听到这里，你是不是感觉头都大了一圈？如果因为某种原因，它还需要你每次都传递一个令牌过去，你告诉我，你准备如何让这一切的混乱与不堪重新归于宁静呢？\n学如逆水行舟，不进则退\r改进思路 OK，现在假设，我们制止这场混乱的方式，是强迫大家都去使用 router-link 这个组件，虽然它最终渲染出来就是一个 a 标签。相信参加工作以后，大家都会有这样一种感觉，那就是工作中 99.9% 的事情，都是在最好和最坏中间选一个过渡状态，然后不断地为之投入精力或者叫做填坑，甚至有很多东西，从来都不是为了让一件事情变得更好而存在。作为这个地球上脆弱而渺小的个体，时间、生命、爱，每一样东西都像缓缓从指尖滑落的沙子，我们实在是太喜欢这种可以掌控点什么的感觉了。所以，如果一件事情没法从道理或者科学上讲通的话，那就用制度或者规范来作为武器，在一个连国家都可以宣布破产的年代，大概，话语权比是非对错更重要。因此，在博主的博客里，在这小小的一方天地里，不妨假设我有这种话语权，可以强迫大家都使用 router-link 这个组件。我们讲，Vue Router 不支持外部链接，一个非常直观的理由是，当我们写出下面的代码时，它会完全辜负我们的期望：",
            "content": "众所周知，Vue Router 是 Vue 中重要的插件之一，特别是在当下流行的 单页面应用/SPA 中，这种感觉会越来越明显。此时，路由的作用就是根据 URL 来决定要显示什么内容。诚然，页面这个概念在工程/模块中依然存在，可当你开始关注最终发布的产物时，你会发现本质上它只有一个页面。无论你选择 hash 或者是 history 模式的路由，它都像是在同一张纸上反复写写画画，让你看起来觉得它有很多个不同的页面。回顾早期的前端项目，它往往会有多个不同的页面组成，我们是通过一个个的超链接来实现不同页面间的跳转。如今，这一切都已一去不复返，我们只能在单页面应用的世界里继续披荆斩棘。当然，绝大多数的普通用户无法感知到这种程度的变化，在他们的眼中，那依然不过是普通的一个超链接。那么，当一个项目中充斥着各种各样的超链接的时候，这个问题就值得我们单独拿出来讲一讲。所以，今天这篇博客的主题是路由和外部链接。请注意，这是一组相对通用的概念，不受限于任何一个前端框架，我们只是选择了使用 Vue 来进行说明。\n问题现状 我们的项目存在着大量的超链接以及导航菜单，在 UI 设计阶段，通常不会有人关心，一个链接到底是内部链接还是外部链接。与此同时，由于 HTML 这门标记语言的极大灵活性，实现一个导航链接的方式有 N 多种，可以是一个 a 标签，可以是一个 div 标签，甚至可以是一个 span 标签。虽然 Vue Router 里提供了 router-link 组件，可在实际的项目中，需要综合考虑团队风格和第三方 UI 库的因素，甚至有时候，再没有设计规范的情况下，可能大家连 router-link 组件都不愿意用或者说压根就没机会用。\n这样就造成一个非常尴尬的局面，当你需要为页面编写业务代码的时候，你不得不在各种各样的超链接上浪费时间，只要不是通过 a 标签实现的，你都必须处理它点击的事件，更不必说，你还要区分这个链接是一个内部链接还是一个外部链接，原因是 Vue Router 不支持外部链接，你不得不通过 window.location 或者 window.open() 的这样的方式来实现“曲线救国”，试想，如果每一个都这么折腾一遍，你还会觉得有趣吗？\n而在我们的项目里，实际上它还需要从网页端唤起应用，这样便又涉及到了 URL Schemes 这个话题。除了 Android 和 iOS 这个平台上的差异，单单就 Windows 而言，其基于注册表的方案对协议提供者的约束并不强，如果团队内对此没有任何规范的话，你将面对各种千奇百怪的参数传递方式。听到这里，你是不是感觉头都大了一圈？如果因为某种原因，它还需要你每次都传递一个令牌过去，你告诉我，你准备如何让这一切的混乱与不堪重新归于宁静呢？\n学如逆水行舟，不进则退\r改进思路 OK，现在假设，我们制止这场混乱的方式，是强迫大家都去使用 router-link 这个组件，虽然它最终渲染出来就是一个 a 标签。相信参加工作以后，大家都会有这样一种感觉，那就是工作中 99.9% 的事情，都是在最好和最坏中间选一个过渡状态，然后不断地为之投入精力或者叫做填坑，甚至有很多东西，从来都不是为了让一件事情变得更好而存在。作为这个地球上脆弱而渺小的个体，时间、生命、爱，每一样东西都像缓缓从指尖滑落的沙子，我们实在是太喜欢这种可以掌控点什么的感觉了。所以，如果一件事情没法从道理或者科学上讲通的话，那就用制度或者规范来作为武器，在一个连国家都可以宣布破产的年代，大概，话语权比是非对错更重要。因此，在博主的博客里，在这小小的一方天地里，不妨假设我有这种话语权，可以强迫大家都使用 router-link 这个组件。我们讲，Vue Router 不支持外部链接，一个非常直观的理由是，当我们写出下面的代码时，它会完全辜负我们的期望：\n\u0026lt;router-link to=\u0026#34;https://blog.yuanpei.me\u0026#34;\u0026gt;Go\u0026lt;/router-link\u0026gt; 显然，我们期望它可以跳转到 https://blog.yuanpei.me 这个地址，可你只要亲自试一下，就会知道这是你的一厢情愿。因为，此时浏览器地址栏中的地址会显示为：\nhttp://localhost:8080/#/https://https://blog.yuanpei.me 当然，我们的用户不会操心这种事情，正如他们从来不会去刻意地分辨，这是一个内部链接还是一个外部链接。这里讲一下博主的思路，博主打算在 router-link 的基础上再做一层封装，内部链接通常是以/ 来开头的，基于这个特点，我们可以区分出这是一个内部链接还是一个外部链接。针对内部链接，我们继续使用 router-link 组件；针对外部链接，我们直接使用 a 标签即可。此时，对应的 Vue 模板定义如下：\n\u0026lt;template\u0026gt; \u0026lt;a v-if=\u0026#34;isExternal\u0026#34; :href=\u0026#34;formatedUrl\u0026#34; :target=\u0026#34;target\u0026#34;\u0026gt; \u0026lt;slot\u0026gt;\u0026lt;/slot\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;router-link v-else v-bind=\u0026#34;originProps\u0026#34;\u0026gt; \u0026lt;slot\u0026gt;\u0026lt;/slot\u0026gt; \u0026lt;/router-link\u0026gt; \u0026lt;/template\u0026gt; 在这里，我们对外暴露了 to 和 target 两个属性，前者允许我们传入一个字符串或者对象，后者可以控制这个链接的打开方式，是在当前窗口还是一个新窗口中打开：\nexport default { name: \u0026#34;MyRouterLink\u0026#34;, props: { to: { type: [Object, String], default: () =\u0026gt; { path: \u0026#39;/\u0026#39; }, required: true, }, target: { type: String, default: () =\u0026gt; \u0026#39;\u0026#39;, }, }, // ... } 还记得我们是怎么区分内部链接和外部链接的吗？只需要判断传入的 URL 是否以 / 开头。在这里，我们需要对 to 的类型进行判断：\ncomputed: { isExternal() { if (typeof(this.to) === \u0026#39;object\u0026#39;) { return this.to.path \u0026amp;\u0026amp; this.to.path[0] !== \u0026#39;/\u0026#39; } if (typeof(this.to) === \u0026#39;string\u0026#39;) { return this.to \u0026amp;\u0026amp; this.to[0] !== \u0026#39;/\u0026#39; } return false }, } 当然，在某些情况下，这个 URL 允许使用者传入查询参数(QueryString)。这里，我们用 formatedUrl 这个计算属性来统一进行处理：\ncomputed: { formatedUrl() { let url = \u0026#34;\u0026#34;; if (typeof(this.to) === \u0026#39;object\u0026#39;) { url = this.to.path } else if (typeof(this.to) === \u0026#39;string\u0026#39;) { url = this.to } let queryArray = []; if (this.to.query) { for (let key in this.to.query) { const value = encodeURIComponent(this.to.query[key]); queryArray.push(`${key}=${value}`); } } if (queryArray.length == 0) { return url; } if (url.indexOf(\u0026#34;?\u0026#34;) != -1) { url = `${url}${queryArray.join(\u0026#34;\u0026amp;\u0026#34;)}`; } else { url = `${url}?${queryArray.join(\u0026#34;\u0026amp;\u0026#34;)}`; } return url; }, } 最后，需要特别说明的是 originProps 这个计算属性，虽然我们封装了 router-link 这个组件，但我们希望这个新组件是兼容 router-link 本身自带的属性的。此时，我们可以采用下面的方式来处理，具体可以参考官方文档：vm.$attrs：\ncomputed: { originProps() { return { ...this.$props, ...this.$attrs }; }, } 现在，万事具备，我们来试用一下这个新的组件，看看效果如何：\n\u0026lt;header\u0026gt; \u0026lt;my-router-link to=\u0026#34;/home\u0026#34;\u0026gt;首页\u0026lt;/my-router-link\u0026gt; \u0026lt;my-router-link to=\u0026#34;/message\u0026#34;\u0026gt;消息\u0026lt;/my-router-link\u0026gt; \u0026lt;my-router-link to=\u0026#34;https://blog.yuanpei.me\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;博客\u0026lt;/my-router-link\u0026gt; \u0026lt;my-router-link :to=\u0026#34;{ path: \u0026#39;https://www.baidu.com/s\u0026#39;, query: { wd: \u0026#39;天气\u0026#39; } }\u0026#34; target=\u0026#34;_blank\u0026#34; \u0026gt; 百度 \u0026lt;/my-router-link\u0026gt; \u0026lt;my-router-link :to=\u0026#34;{ path: \u0026#39;tencent://\u0026#39;, query: { uin: \u0026#39;875974254\u0026#39;, site: \u0026#39;Vue\u0026#39;, menu: \u0026#39;yes\u0026#39; }, }\u0026#34; \u0026gt; QQ \u0026lt;/my-router-link\u0026gt; \u0026lt;/header\u0026gt; 我们可以注意到，现在它可以同时支持内部链接和外部链接，并且我们可以传递一个对象来更好地控制 URL 的细节，当然，它还可以从桌面唤起 QQ 应用，只要协议提供方采用类似的传参方式，那么，这个方案其实可以做到一劳永逸的。完整的代码我已上传到 Github，方便大家可以做进一步的探索。\n从网页端唤起应用\r话题延伸 坦白讲，在我写这篇文章的时候，我一直在思考一个问题，即：如何给所有出站的超链接携带令牌信息？这个想法其实是在解决别人产生的问题，譬如，从子系统 A 跳转到子系统 B 的过程中，为了实现所谓的“免登录”，大佬们提议直接把令牌信息附加到 URL上传递过去，先不说令牌信息刷新和过期的问题，就单单是令牌信息附加到 URL上这一项，看起来都是非常愚蠢的做法，众所周知，浏览器对针对 GET 请求时的 URL 长度存在限制，你这不是直愣愣地往人家枪口上撞吗？放着 CAS、Keycloak 这种成熟的方案不用，非要用这种掩耳盗铃式的半桶水方案？也许，人类还真就喜欢做这样的事情，毕竟这样可以制造出问题和麻烦，让别人有事可做。听我说，谢谢你，因为有你\u0026hellip;吐槽归吐槽，一开始我是写了一个自定义指令来做这个事情：\nVue.directive(\u0026#39;attach-query-string\u0026#39;,function (el, binding) { if (el.tagName === \u0026#39;A\u0026#39;) { const token = resolveToken() const userId = resolveUserId() const posting = resolvePostings()[0] || \u0026#39;\u0026#39; if (el.href.indexOf(\u0026#39;?\u0026#39;) != -1){ el.href = `${el.href}\u0026amp;token=${token}\u0026amp;userId=${userId}\u0026amp;deviceType=${DeviceType.RCT}\u0026amp;posting=${posting}` } else { el.href = `${el.href}?token=${token}\u0026amp;userId=${userId}\u0026amp;deviceType=${DeviceType.RCT}\u0026amp;posting=${posting}` } } }) 注意到，这个指令只对 a 标签有效，所以，那些花里胡哨、奇形怪状的超链接依然是个令人头疼的问题，我们先忽略它们就好：\n\u0026lt;a :href=\u0026#34;item.link\u0026#34; :target=\u0026#34;item.openNewTab ? \u0026#39;_blank\u0026#39; : \u0026#39;_self\u0026#39;\u0026#34; v-attach-query-string \u0026gt; {{ item.name }} \u0026lt;/a\u0026gt; 这个指令表示，它将会在 mounted 和 updated 的时候触发相应的逻辑，对于大多数的超链接而言，其 src 只会初始化一次，所以，这个方案基本上可行的，唯一的难点在于，并不是所有人都会如你期望的那样使用 a 标签。当然，我内心深处永远相信 jQuery 一把梭，所以，通常尝试过 querySelectorAll() ，但我始终觉得这样子显得有点丑陋，说好的不再操作 DOM 了呢？如果按照我们现在的思路，其实可以在组件内部统一处理，下面是一个简单的实现：\ncomputed: { formatedUrl() { let url = \u0026#34;\u0026#34;; if (typeof(this.to) === \u0026#39;object\u0026#39;) { url = this.to.path } else if (typeof(this.to) === \u0026#39;string\u0026#39;) { url = this.to } let queryArray = []; // 统一追加参数 const token = resolveToken() const userId = resolveUserId() const posting = resolvePostings()[0] || \u0026#39;\u0026#39; queryArray.push(`token=${token}`) queryArray.push(`userId=${userId}`) queryArray.push(`posting=${posting}`) // 处理组件传入的参数 if (this.to.query) { for (let key in this.to.query) { const value = encodeURIComponent(this.to.query[key]); queryArray.push(`${key}=${value}`); } } if (queryArray.length == 0) { return url; } if (url.indexOf(\u0026#34;?\u0026#34;) != -1) { url = `${url}${queryArray.join(\u0026#34;\u0026amp;\u0026#34;)}`; } else { url = `${url}?${queryArray.join(\u0026#34;\u0026amp;\u0026#34;)}`; } return url; }, }, 从本质上讲，这两种方案做得事情是完全相同的，无非是拥有了新知识或者技能以后，再去重新审视过去的种种选择，人虽然始终没有办法打破自身的历史局限性，可是能从新知识或者技能中不断丰富自我的认知，这又属实是种颇具幸福感的事情，因为，从这一刻起，你已经告别了昨天的自己，真正做到了“且将新火试新茶”。回过头来再次审视这个问题的时候，你会觉得哪一种更好呢？欢迎大家在评论区留下你的答案。\n本文小结 本文介绍了一种针对 Vue Router 进行扩展的思路，主要是为了解决 router-link 不支持外部链接跳转的问题。关注这个问题的契机，则是来源于项目中大量存在着的超链接和导航菜单。其中，除了指向站内的内部链接，还有指向站外的外部链接，而这些外部链接中，又牵扯到从网页端唤醒应用的问题，所以，我们需要一种相对统一的机制来处理这些内部细节，因此，就有了今天的这篇博客。除此以外，因为一部分人的愚蠢决定，我们必须要在所有出站的 URL 上附加令牌信息，针对这个问题，博主先是尝试了自定义指令的做法，然后又在现在的方案上做了一点处理，这使得我们能把精力放在真正重要的地方。从整体上而言，如果在设计 UI 前，就定好这样一种规范，所有人都使用这个统一的组件，这个问题处理起来会稍微简单一点，可惜，从人类让一群人一起编程的那一刻起，这种人与人间的磨合和牵制就会一直存在，正所谓“有人的地方就有江湖”，身处江湖的人，多少会有点身不由己的磕磕绊绊，本文完！\n",
            "tags": ["前端","Vue","路由","思考"],
            "categories": ["前端开发"],
            "thumbnail": ""
        },
        {
            "title": "视频是不能 P 的系列：OpenCV 和 Dlib 实现表情包",
            "date": "2022-07-01",
            "permalink": "http://localhost:1313/posts/make-memes-with-opencv-and-dlib/",
            "summary": "2020 年年底的时候，博主曾心血来潮地开启过一个系列：视频是不能 P 的，其灵感则是来源于互联网上的一个梗，即：视频不能 P 所以是真的。不过，在一个美颜盛行的时代，辨别真伪实在是一件奢侈的事情，在各种深度学习框架光环的加持下，在视频中实现“改头换面”已然不再是新鲜事儿，AI 换脸风靡一时的背后，带来是关乎隐私和伦理的一系列问题，你越来越难以确认，屏幕对面的那个到底是不是真实的人类。古典小说《红楼梦》里的太虚幻境，其牌坊上有幅对联写道，“假作真时真亦假，无为有处有还无”。果然，在这个亦真亦幻的世界里，哪里还有什么东西是不能 PS 的呢？在“鸽”了很久很久之后，博主决定要来更新这个系列啦，让我们继续以 OpenCV 作为起点，来探索那些好玩、有趣的视频/图像处理思路，这一次呢，我们来聊聊 OpenCV、Dlib 和 表情包，希望寓教于乐的方式能让大家感受到编程的快乐！\n环境准备 python -m pip install opencv-python python -m pip install opencv-contrib-python python -m pip install Pillow python -m pip install numpy python -m pip install imutils python -m pip install dlib 请注意，如果通过 pip 安装 dlib 不大顺利，你可以到 https://github.com/sachadee/Dlib 这个仓库中下载对应的 .whl 文件。例如，博主使用的是 64 位的 Windows 系统，而我的 Python 版本是 3.7，因此，我下载的是 dlib-19.22.99-cp37-cp37m-win_amd64.whl 这个文件。此时，我们可以用下面的方式来安装 dlib：\npython -m pip install dlib-19.22.99-cp37-cp37m-win_amd64.whl 除此以外，我们还需要下载 dlib 所需的模型文件，下载地址为：http://dlib.",
            "content": "2020 年年底的时候，博主曾心血来潮地开启过一个系列：视频是不能 P 的，其灵感则是来源于互联网上的一个梗，即：视频不能 P 所以是真的。不过，在一个美颜盛行的时代，辨别真伪实在是一件奢侈的事情，在各种深度学习框架光环的加持下，在视频中实现“改头换面”已然不再是新鲜事儿，AI 换脸风靡一时的背后，带来是关乎隐私和伦理的一系列问题，你越来越难以确认，屏幕对面的那个到底是不是真实的人类。古典小说《红楼梦》里的太虚幻境，其牌坊上有幅对联写道，“假作真时真亦假，无为有处有还无”。果然，在这个亦真亦幻的世界里，哪里还有什么东西是不能 PS 的呢？在“鸽”了很久很久之后，博主决定要来更新这个系列啦，让我们继续以 OpenCV 作为起点，来探索那些好玩、有趣的视频/图像处理思路，这一次呢，我们来聊聊 OpenCV、Dlib 和 表情包，希望寓教于乐的方式能让大家感受到编程的快乐！\n环境准备 python -m pip install opencv-python python -m pip install opencv-contrib-python python -m pip install Pillow python -m pip install numpy python -m pip install imutils python -m pip install dlib 请注意，如果通过 pip 安装 dlib 不大顺利，你可以到 https://github.com/sachadee/Dlib 这个仓库中下载对应的 .whl 文件。例如，博主使用的是 64 位的 Windows 系统，而我的 Python 版本是 3.7，因此，我下载的是 dlib-19.22.99-cp37-cp37m-win_amd64.whl 这个文件。此时，我们可以用下面的方式来安装 dlib：\npython -m pip install dlib-19.22.99-cp37-cp37m-win_amd64.whl 除此以外，我们还需要下载 dlib 所需的模型文件，下载地址为：http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2。下载该文件并解压后，可以得到一个 .dat 格式的文件，这就是我们用来做人脸识别的模型文件，即人脸的 68 个特征点检测，这个我们会在下面做更进一步的说明。\n初探 Dlib 其实，如果是简单的人脸检测，OpenCV 完全足矣。如果你读过我这个系列的第一篇文章，就会知道 OpenCV 人脸检测的实际效果如何。事实上，它会使用一个矩形来表示检测到的人脸范围，这里，我还是用堺雅人主演的电视剧《半泽直树》来作为说明，可以注意到每一个矩形对应着一张人脸：\nOpenCV人脸检测效果展示\r可是，如果我们希望对这些“人脸”进行比对以期望达到人脸识别的目的，这个精度对我们来说就显得捉襟见肘啦！为了解决这个问题， dlib 中使用的是一种被称之为 68 face landmarks 的方法，简单来说，就是用 68 个特性点来定位一个人的面目五官，为了方便大家理解，我们一起来看下面的例子：\n# 初始化 detector 和 predictor detector = dlib.get_frontal_face_detector() predictor = dlib.shape_predictor(\u0026#39;shape_predictor_68_face_landmarks.dat\u0026#39;) # 载入人脸图片 image = cv2.imread(\u0026#34;./faces/face1.jpg\u0026#34;) # 检测人脸 \u0026amp; 绘制特征点 face_rects = detector(image, 0) for index, face in enumerate(face_rects): shape = predictor(image, face_rects[index]) shape = face_utils.shape_to_np(shape) for idx, (x,y) in enumerate(): cv2.circle(img, (x, y), 1, (0, 0, 255), -1) cv2.putText(img, str(idx + 1), (x,y), cv2.FONT_HERSHEY_SIMPLEX, 0.2, (255, 255, 255), 1, cv2.LINE_AA) cv2.imshow(\u0026#34;image\u0026#34;,image) 此时，我们可以得到下面的结果：\n使用 dlib 进行人脸检测\r可以注意到， dlib 可以进一步识别出人脸中的特征点，譬如眼睛、眉毛、鼻子、嘴巴等等，这让我们有了更多可以探索的乐趣。举个例子，司机在道路上疲劳驾驶，有可能会引发交通事故，如果我们能实时分析司机的面目表情，就可以为这个世界做出一点小小的努力。一个常见的思路是计算“眼睛”部分的宽高比，因为当人眼睛闭合的时候，相当于“眼睛”部分纵向的特征点间距变小。再比如，我们可以通过嘴角和眉毛的弧度来“揣测”一个人的喜怒哀乐，让计算机不再是冷冰冰地一台机器。也许，你听说过“三庭五眼”这套理论，所以，从某种意义上来讲，这个思路还可以扩展到“看相”这个方向。果然，你可以永远相信神仙姐姐的颜值呢\u0026hellip;\n移花接木 OK，在对 dlib 有一个初步印象以后，我们来说说，在今天这篇博客里，博主到底想做一件什么样的事情。如下图所示，我们希望借助 dlib 逐步地“抠取”出人物表情，最终再和这个经典的“熊猫人”融合在一起，从而达到从某任意图片生成表情包的目的。当然啦，这个想法并不容易实现，因为你从这个图片中就可以看到它的最终效果。图中使用的素材出自半泽直树第二季，当半泽直树遇上对手黑崎骏一，瞬间碰撞出一种惺惺相惜的 CP 感，作为雅人叔的忠实粉丝，还有什么比做成表情包更直抒胸臆的表达方式呢？\n制作表情包的过程说明\r68 个特征点的绘制，在初探 dlib 的环节已经讲过，这里就不再赘述啦！这里，我们先来说说人脸的矩形范围如何获得，可能有读者朋友会问，这个矩形的作用是什么？其实，不管我们用怎样不规则的一个多边形来裁切图片，我们最终得到的一定是具备长和宽的矩形。因此，这个矩形就是帮我们定位整个脸的范围。下面是对应的代码片段：\ndef get_facemark_rect(shape): shape2np = face_utils.shape_to_np(shape) (x0, y0) = shape2np[0] minX = maxX = x0 minY = maxY = y0 for (x,y) in shape2np: if x \u0026lt; minX: minX = x if x \u0026gt;= maxX: maxX = x if y \u0026lt; minY: minY = y if y \u0026gt;= maxY: maxY = y return (minX, minY, maxX - minX, maxY - minY) 可以注意到，代码非常地朴实无华，只需要分别找到 x 和 y 的最小值/最大值，就可以确定人脸的矩形范围。如下图所示，红色线条呈现出的即为人脸的矩形范围。作为对比，我们同时绘制了 dlib 本身自带的一个矩形范围，这个矩形范围大致等同于 OpenCV 人脸检测的效果：\n获得人脸的矩形范围\r事实上，如果你回头去看我绘制特征点这一部分的代码，会发现这里有一个变量 face 一直没有用到，它实际上是 dlib里定义的一种类型 rectangle，从命名上我们就可以知道，这是一个表示矩形的数据结构。在 OpenCV 中，我们可以使用下面的代码片段来绘制一个矩形：\n# 使用自己计算出的矩形范围 (x, y, w, h) = get_facemark_rect(shape) cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 1) # 使用 dlib 自带的矩形范围 x, y, w, h = face.left(), face.top(), face.right() - face.left(), face.bottom() - face.top() cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 1) 考虑到，博主这里需要的是一个相对精确的面部的范围，大家可以结合自己的需要，选择其中一种即可。接下来，我们来考虑如何根据这 68 个特征点进行“抠脸”，为此，我们只需要找到那些表示面部轮廓的点即可，通过上面的图形，我们可以看出，脸的下半部分轮廓点为 1 到 17，上半部分轮廓点为 18 到 27。因此，我们只需要用这些点构造一个多边形即可：\ndef create_face_mask(image, facemark): shape, face = facemark shape2np = face_utils.shape_to_np(shape) mask = np.zeros(image.shape, dtype=np.uint8) points = np.concatenate([shape2np[0:16], shape2np[26:17:-1]]) cv2.fillPoly(img=mask, pts=[points], color=(255,) * image.shape[2]) return mask 这里，我们首先创建了一个和原图片同样大小的图片 mask , 然后调用 fillPoly() 对多边形进行填充。这样，我们就得到了一个和人脸轮廓完全一致的掩膜：\n利用人脸轮廓创建掩膜\r在 OpenCV 中，可以通过 bitwise_and() 函数来对两张图片进行“叠加”, 而在这个示例中，原图片为 image，掩膜图片为 mask，因此，对应的代码片段如下：\nimage = cv2.bitwise_and(image, mask) 此时，我们可以得到下面的结果，可以注意到，雅人叔的脸已经被我“抠取”出来了，哈哈！\n成功抠取出雅人叔的脸\r当然啦，这个图片对我们来说太大了！毕竟，我们需要的是雅人叔的这张脸，而不是这个黑乎乎的背景。此时此刻，前面我们计算出来的矩形范围就派上用场啦，因此，我们对图片做一次裁切，OpenCV中裁切图片是非常容易的，我们只需要像对待数组一样指定一个范围：\n(x, y, w, h) = get_facemark_rect(shape) image = image[y:y + h, x:x + w] 当然，这里你会发现一个问题，裁切出来的图片带着黑色背景，这显然不利于我们和“熊猫人”进行融合。这里，博主提供的解决方案是：创建一张同样大小的图片，再把除了黑色以外的颜色全部复制过来，因为黑色再在 NumPy 中使用一个元素全部为 0 的数组来表示。那么，这样就简单多啦：\ndef create_white_image(image): white = np.zeros(image.shape, dtype=np.uint8) for i in range(0, image.shape[0]): for j in range(0, image.shape[1]): white[ i, j ] = np.uint8(255) return white # 创建一张背景色为白色的图片，复制除黑色以外的每一个像素 white = create_white_image(image) for i in range(0, image.shape[0]): for j in range(0, image.shape[1]): if ((image[i, j] != 0).all()): white[i, j] = image[i, j] 接下来，为了让雅人叔的脸更贴近“熊猫人”的气质，我们对图片运用一次 threshold() 函数。事实上，到这一步为止，博主一直没有找到特别好的方法，因为人脸上可能会有阴影造成的深浅变化，这样就没有办法 100% 的转化为黑白图片。除了这种固定阈值的方案，博主同样尝试自适应阈值的方案，即 adaptiveThreshold() 函数。不过，从最终效果来看，自适应阈值的方案表现并不好，下面是这两种方案的效果对比：\n固定阈值和自适应阈值效果对比\r代码层面没什么悬念，转灰度图再调用相应的函数即可：\n# 转灰度图 image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # 固定阈值 + THRESH_BINARY _, image1 = cv2.threshold(image, 45, 255, cv2.THRESH_BINARY) # 自适应阈值 + ADAPTIVE_THRESH_GAUSSIAN_C image2 = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,9,7) # 自适应阈值 + ADAPTIVE_THRESH_MEAN_C image3 = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY,9,7) 如果抛开最终呈现出来的效果的好坏不谈，到这里我们勉强算是达到了预期目的，我们将这个结果以图片的形式保存下来。接下来，我们只需要把雅人叔的脸和“熊猫人”组合在一起即可：\n# 读取熊猫人和人脸图片 panda = Image.open(\u0026#34;PandaMan.jpg\u0026#34;) face = Image.open(\u0026#34;face.jpg\u0026#34;) w_g, h_g = bg.size w_f, h_f = face.size ratio = h_f / w_f # 根据熊猫人的大小对人脸进行缩放 w_new = int(w_g * 0.4) h_new = int(ratio * w_new) resized = face.resize((w_new, h_new),Image.ANTIALIAS) # 计算左上角坐标 left = int((w_g - w_new) / 2) top = int((h_g / 2 - h_new) / 2) # 把人脸贴到指定位置 panda.paste(resized, (left, top, left + w_new, top + h_new)) panda.save(\u0026#39;output.jpg\u0026#39;) 那么，对不起了，雅人叔，我实在是太喜欢你在不同的剧里呈现出来的形象，半泽直树、古美门、德川家定、山南敬助、真田信繁\u0026hellip;，每一个角色都能让人惊呼这居然是同一个人，更不用说，有那种根植于文学和话剧的清澈感，能在演绎角色的同时加入个人的理解。雅人叔，请变成我的表情包吧，哈哈！\n最终合成的表情包效果\r本文小结 其实，一开始在规划这篇文章的时候，我原本是打算写一写“换脸”这个话题的。可是后来发现，换脸涉及的那些知识，对现在的我而言，实在是一座难以逾越的高山，所以，最后不得不退而求其次，写一个相对简单的话题，可即使这样，写这篇文章还是花费了我挺长时间，因为一直没有找到更好的方法来处理人脸。所以，这篇文章写到这种程度，其实是因为我不想一直拖延下去。简单总结一下，这篇文章介绍了 dlib 的用法，相对于 OpenCV 而言，它可以拿到人脸的 68 个特征点，而基于特征点我们可以对人脸检测/识别做进一步的探索，比如驾驶过程中司机的疲劳检测、表情分析、活体检测等等，而这篇文章主要利用了其中的轮廓点来生成多边形掩膜，来达到抠取人脸的目的。在这个基础上，我们实现了通过照片制作表情包的想法，虽然目前实用性并不强，可我觉得处理图像的这个过程还是特别有意思。以前，有人讲过程序员的三大浪漫，即操作系统、编译原理 和 图像学，这种浪漫大概只有程序员能理解。好了，以上就是这篇博客的全部内容啦，如果大家对人脸的处理还有更好的方案，欢迎大家在评论区积极留言。\n",
            "tags": ["OpenCV","Python","Dlib","图像处理"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "Vue.js 前端项目容器化部署实践极简教程",
            "date": "2022-05-17",
            "permalink": "http://localhost:1313/posts/a-simplified-tutorial-on-containerized-deployment-of-front-end-projects-for-vue/",
            "summary": "大概一周前，在某个「微雨燕双飞」的下午，我正穿梭于熙熙攘攘的车流人海当中，而被雨水濯洗过的天空略显灰白，傍晚亮起的路灯恍惚中有种朝阳初升的错觉，内心更是涌现出一种「一蓑烟雨任平生」的豁达，我还没来得及给这场内心戏添油加醋，兴哥的电话突然打断了我的思绪。一番攀谈交心，我了解到，他想问的是前端容器化部署的相关问题。虽然，靠着兴哥的睿智、果敢，他第二天就想明白了整个事情的来龙去脉；但是，这完全不影响我水一篇博客出来。所以，今天这篇文章，我们来聊聊前端项目的容器化部署，并提供一个极简的实践教程，这里以 Vue.js 为例，希望对大家有所启发。\n你说，这像太阳吗？\r首先，我们来编写 Dockerfile，这里采用的是多阶段构建的做法，第一个阶段，即 build，主要是利用 node.js 基础镜像来实现前端项目的发布，所以，你可以看到 package.json、npm install 以及考虑到国情的 cnpm install 这些前端项目中喜闻乐见的东西，安装完依赖以后我们通过 npm run build 来完成打包，这取决于你项目中实际使用的脚本或者命令，如果你不喜欢 npm，你同样可以用 yarn 来编写这些指令，只要你喜欢就好。做人嘛，最重要的是开心！\n# build FROM node:lts-alpine as build WORKDIR /app COPY package*.json ./ RUN npm install -g cnpm --registry=https://registry.npm.taobao.org RUN cnpm install COPY . . RUN npm run build # deploy FROM nginx:stable-alpine as deploy COPY --from=build /app/dist/ /usr/nginx/wwwroot COPY /nginx/nginx.conf /etc/nginx/nginx.conf EXPOSE 80 CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] OK，第二个阶段，即 deploy，前端发布出来的产物是无法直接在浏览器里打开的，这一点你平时用 Vue.",
            "content": "大概一周前，在某个「微雨燕双飞」的下午，我正穿梭于熙熙攘攘的车流人海当中，而被雨水濯洗过的天空略显灰白，傍晚亮起的路灯恍惚中有种朝阳初升的错觉，内心更是涌现出一种「一蓑烟雨任平生」的豁达，我还没来得及给这场内心戏添油加醋，兴哥的电话突然打断了我的思绪。一番攀谈交心，我了解到，他想问的是前端容器化部署的相关问题。虽然，靠着兴哥的睿智、果敢，他第二天就想明白了整个事情的来龙去脉；但是，这完全不影响我水一篇博客出来。所以，今天这篇文章，我们来聊聊前端项目的容器化部署，并提供一个极简的实践教程，这里以 Vue.js 为例，希望对大家有所启发。\n你说，这像太阳吗？\r首先，我们来编写 Dockerfile，这里采用的是多阶段构建的做法，第一个阶段，即 build，主要是利用 node.js 基础镜像来实现前端项目的发布，所以，你可以看到 package.json、npm install 以及考虑到国情的 cnpm install 这些前端项目中喜闻乐见的东西，安装完依赖以后我们通过 npm run build 来完成打包，这取决于你项目中实际使用的脚本或者命令，如果你不喜欢 npm，你同样可以用 yarn 来编写这些指令，只要你喜欢就好。做人嘛，最重要的是开心！\n# build FROM node:lts-alpine as build WORKDIR /app COPY package*.json ./ RUN npm install -g cnpm --registry=https://registry.npm.taobao.org RUN cnpm install COPY . . RUN npm run build # deploy FROM nginx:stable-alpine as deploy COPY --from=build /app/dist/ /usr/nginx/wwwroot COPY /nginx/nginx.conf /etc/nginx/nginx.conf EXPOSE 80 CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] OK，第二个阶段，即 deploy，前端发布出来的产物是无法直接在浏览器里打开的，这一点你平时用 Vue.js 的脚手架的话应该会注意到。所以，此时我们需要一个静态文件服务器来托管这些产物，这些产物通常会被放到 dist 目录，因此，在这一阶段主要就是把这个目录里的内容拷贝到 Nginx 下面，这里我们用的是 wwwroot。当然，如果你还怀念曾经的 LAMP 組合，同样可以替换为 Apache。我们在这个世界的一切努力，无非是为了比别人多一种选择，甚至有时候你完全没有选择。可是在计算机的世界里，你可以尽情地去创造，而这则是我的选择，从我高中在班级电脑上写出第一个 Visual Basic 程序开始，我庆幸能一直坚持这份热爱到现在。\nworker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; root /usr/nginx/wwwroot; index index.html; } } 这里提到了 Nginx，那么，自然而然地，对于 Nginx 的配置问题就无可避免。可惜，事实是：每一个声称配置文件更灵活的人，从来都不会去摆弄配置文件，只有程序员天天和这些配置文件打交道。幸运的是，对于简单的静态文件服务器而言，它的配置并不算特别复杂，还记得前面的 wwwroot 吗？其实，它是在这里定义的，对应了 server 节点中的 root 属性，从这里我们可以看到，容器内部默认监听 80 端口，默认页面是 index.html。如果你现在还不太了解 Nginx 的配置文件，相信我，这几个配置已足够你快速上手啦！我不大愿意再写教程的原因是，我不喜欢每个步骤都要截图，并且还要在图上做好标记。显然，对于程序员而言，懒惰是一种美德。\nversion: \u0026#34;3.8\u0026#34; services: font_mock: build: context: ./ dockerfile: Dockerfile image: font_mock ports: - \u0026#34;50001:80\u0026#34; volumes: - \u0026#39;/etc/localtime:/etc/localtime:ro\u0026#39; networks: envoymesh: networks: envoymesh: {} 好了，现在万事具备，我们再来写一个 docker-compose.yaml 来对服务进行编排，可以注意到，我们把 50001 端口绑定到了 80 端口，这就和前面呼应上了，对不对？剩下的就没什么好说的啦，不再一一赘述，如果你看不懂，可以先去了解一下 docker-compose。此时，我们运行 docker-compose up 命令，就可以看到下面的结果：\n通过 Docker 部署前端项目\r没错，我为了节省写作时间，直接使用了 在 Vue.js 中使用 Mock.js 实现接口模拟 这篇文章里的项目，果然，我再次完美发扬了 “懒惰” 这种优秀的美德，总而言之，到这里我们已经成功地通过容器技术部署了一个前端项目，在这个基础上，你还可以接入 Envoy 这个代理层 或者 是为 Nginx 添加 SSL 证书等等\u0026hellip;当然，这些都是后话啦，各位比我聪明千倍、万倍的读者朋友们可以进一步去完善它，这篇短浅易懂的文章就当作诸位的入门教程好啦，愿「他山之石，可以攻玉」，谢谢大家，本文完！\n# 创建 CA 密钥 openssl genrsa -out ca.key 1024 # 利用 CA 密钥生成自签名 CA 证书 openssl req -new -x509 -days 3650 -key ca.key -out ca.crt # 创建服务器端证书密钥 openssl genrsa -des3 -out server.key 1024 # 生成服务器端证书 openssl req -new -key server.key -out server.csr # 利用 CA 证书对服务器端证书进行签名 openssl ca -in server.csr -out server.crt -cert ca.crt -keyfile ca.key 此时，我们会得到服务器端证书文件 server.crt 以及密钥文件 server.key，我们将其配置到 Nginx 中即可，下面是修改后的 Nginx 配置文件：\nworker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; listen 443 ssl; server_name localhost; # 证书文件 ssl_certificate /usr/nginx/ssl/server.crt; # 密钥文件 ssl_certificate_key /usr/nginx/ssl/server.key; # SSL 会话缓存大小为：1M ssl_session_cache shared:SSL:1m; # SSL 会话超时时间为：5min ssl_session_timeout 5m; # 支持 TLS1.0/1.1/1.2 三个版本 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; root /usr/nginx/wwwroot; index index.html; } } 需要注意的是，在创建证书时，对应的 Common Name 应该和网站的域名保持一致，这里的示例域名为：https://www.snowfly.com，在本地调试的时候，我们可以通过修改 hosts 文件来进行测试：\n通过 OpenSSL 创建证书\r通常，你需要把自己创建的证书导入到 受信任的根证书颁发机构 这个分类下面，这样，就可以通过 HTTPS 协议访问你的站点啦！\n在浏览器中查看 HTTPS 证书\r番外预告！！！接下来会写一篇关于 ASP.NET Core 单元/集成测试的文章，敬请期待！\n",
            "tags": ["容器","Vue","Nginx","Envoy"],
            "categories": ["前端开发"],
            "thumbnail": ""
        },
        {
            "title": "在 Vue.js 中使用 Mock.js 实现接口模拟",
            "date": "2022-04-15",
            "permalink": "http://localhost:1313/posts/interface-mock-implemention-using-mock.js-in-vue.js/",
            "summary": "最近这段时间，我一直在参与一个前端项目。每当我从庸碌的生活中赢得片刻喘息的时候，我不由得感慨，在程序员朴实无华且枯燥的职业生涯里，写自己喜欢的代码的机会少之又少，写别人喜欢的代码的机会俯拾皆是，更多的时候像是“为他人作嫁衣裳”。古人云，“遍身罗绮者，不是养蚕人”，当每天面对着被改得面目全非的代码的时候，内心固然早已波澜不惊、宠辱偕忘，可还是会期待美好的事情发生，因为从工程化的角度而言，每天都在思考的事情，其实就是怎么样做会更好一点。过去这些年里，微服务、前后端分离的呐喊声不绝于耳，实际应用过程中则是会遇到各种各样的问题。在今天这篇文章里，我想和大家聊聊 Vue.js 结合 Mock.js 实现接口模拟这个话题，为什么选择这个话题呢？我个人认为，它实际上触及了前后端分离的“灵魂”，并且由此可以引出像文档管理、流程控制等等一系列研发协同的问题。你或许会忍不住问道，前后端分离的“灵魂”是什么呢？各位看官们稍坐，且听我一一道来！\n问题现状 在谈到前后端分离这个话题的时候，在公司层面上对应地往往是组织架构的分离，典型的做法就是让前端和后端成为两个不同的团队，其中，前端团队负责表示层的实现，不限于页面布局、样式风格、交互逻辑等等；后端团队负责数据接口的实现，不限于数据库设计、接口设计、编写 API 等等。对应到 Vue.js 里，前端团队负责写各种各样的页面/组件、数据绑定，后端团队负责提供各种各样的数据接口，这听起来非常地合理，对不对？的确，主流的前后端分离实践都是这样讲的，所以，我们只要套用这个模型，就可以达到预期的效果，对不对？可惜，人类习惯于为这个世界寻找某种颠扑不破的真理，可恰恰人类本身才是这个世界里最不稳定的存在？疫情常态化的当下，每次都被病毒一通嘲讽，抄作业都不会抄啊！\n前后端分离模式下的协同开发\r首先，第一个问题，前、后端团队没有形成“契约”，前端团队拿到原型以后就开始设计页面，ViewModel 中的字段命名、定义完全是由前端团队凭“感觉”写出来的，人类离谱就离谱在，可以靠“感觉”这种玄之又玄的东西决定很多事情。这样做的后果就是，后面真正对接后端接口的时候，发现大量的字段没法对应上，不得不再折腾一遍数据绑定，如果是中途由别人来接手，那么面对的可能就是不同的数据结构间的映射转换。试想，后端程序员尚有 AutoMapper 和 Mapster 可以用，前端程序员可就没有那么幸运啦！更不必说，前端天生比后端面临更频繁的改动，只要涉及到页面布局、交互逻辑的变化，ViewModel 的修改基本无可避免，这样就导致同一个页面多次返工，我相信这个结果大家都不想看到。\n其次，当前、后端团队约定好接口文档以后，双方都按照这份接口文档去完成各自的开发工作，这样听起来简直不能更合理对不对？实际上，在后端团队完成接口开发以前，前端团队会有一段时间的“真空期”或者“黑写期”，因为前端并不知道这段代码能否在真实的环境下工作。此时，前端团队可能会造一点假数据来进行接口模拟，得益于 JavaScript 这门语言的高度灵活、自由，前端团队可能会直接调用一个本地函数来返回假数据，这意味着它并不会触发真实地 HTTP 请求。那么，当有一天后端团队完成了接口开发，你将会把这些本地函数替换为 Axios 的方法，甚至在更极端的情况下，前端团队不能访问后端团队的接口，此时，双方会就本地函数还是 Axios 方法产生一场拉锯战，你告诉我，还有什么比这更折磨一个人的吗？\n所以，综合下来，其实是两个非常普遍的问题：\n第一，前、后端团队如何制定一份对协同有利的接口文档，这份文档是通过工具生成还是人工编写。我个人是特别讨厌用 IM 或者邮件来发送接口文档的，因为没办法做到版本控制或者说让所有手中都有一份最新的接口的文档。\n第二，如何管理项目中用到的各种假数据，以及如何让项目在假数据和真实接口中“无痛”切换。前端项目的特点是所见即所得，这让它比看不见、摸不着的后端项目更受用户青睐，毕竟还有什么比能让用户亲眼看到更亲切的东西呢？\n在“小步快跑、快速迭代”的敏捷思想的驱使下，我们经常需要给用户演示各种功能。也许，在某个时刻，页面上的数据亦真亦假，你还会觉得，管理这些假数据没什么意义吗？而这正是驱使我了解 Mock.js 的动力所在，世上的很多事情，你未必能如愿以偿、做到最好，可你依然要了解什么是最好，“山不厌高，海不厌深”，向不那么完美的世界妥协是现实，永远值得去追寻更完美的世界是理想，这两者在我心目中并不冲突，你觉得呢？\n改进思路 OK，既然找到了问题的症结所在，我们逐一对症下药即可，就像“三过家门而不入”的大禹，选择用疏导的方式治水，让洪水通过疏通的河道流到大海中去，而不是靠一味地“堵”，程序中 90% 的代码都是在给用户“打补丁”，防止对方做出什么骚操作来，那么，是不是可以用某种方式去引导对方呢？我最讨厌听到的话就是，用户想要怎么怎么样，这是没有办法的事情，如果只需要一个传话筒，我们为什么不直接用传呼机呢？作为一个老古董，恐怕现在的 00 后都不知道什么是传呼机。你生命中当下流行或者推崇的东西，总有一天会过期。可即便如此，你还是要全力以赴。显然，这是个哀伤的故事。\nSwagger 对于接口文档的管理问题，我自始至终都推荐 Swagger 这个神器，因为我和这个世界上的绝大多数的程序员一样，都认同一种相对朴素的价值观，即 “懒惰是一种美德”。因为我不喜欢靠人工来维护接口文档，所以，只要有机会用上 Swagger，我一定会用 Swagger 来管理接口文档。不管是过去写 API 和 MVC，还是现在写 gRPC。对我来说，选择 Swagger 是一件自然而然的事情，因为我懒，因为我不理解为什么有人需要导出 Word 或者 Pdf 格式的接口文档。也许，Swagger 那千篇一律的页面风格会让人感到无所适从，喜欢的人非常喜欢，讨厌的人非常讨厌。在前、后端分离的项目中，有一份白纸黑字的接口文档，显然要比“口口相传”靠谱得多。当然，如果你有足以媲美 Swagger 的接口文档管理工具/平台，欢迎大家在评论区留言分享。下面是我曾经写过的关于 Swagger 的文章：\n通过 ApiExplorer 为 Swagger 提供 MVC 扩展 gRPC 搭配 Swagger 实现微服务文档化 .",
            "content": "最近这段时间，我一直在参与一个前端项目。每当我从庸碌的生活中赢得片刻喘息的时候，我不由得感慨，在程序员朴实无华且枯燥的职业生涯里，写自己喜欢的代码的机会少之又少，写别人喜欢的代码的机会俯拾皆是，更多的时候像是“为他人作嫁衣裳”。古人云，“遍身罗绮者，不是养蚕人”，当每天面对着被改得面目全非的代码的时候，内心固然早已波澜不惊、宠辱偕忘，可还是会期待美好的事情发生，因为从工程化的角度而言，每天都在思考的事情，其实就是怎么样做会更好一点。过去这些年里，微服务、前后端分离的呐喊声不绝于耳，实际应用过程中则是会遇到各种各样的问题。在今天这篇文章里，我想和大家聊聊 Vue.js 结合 Mock.js 实现接口模拟这个话题，为什么选择这个话题呢？我个人认为，它实际上触及了前后端分离的“灵魂”，并且由此可以引出像文档管理、流程控制等等一系列研发协同的问题。你或许会忍不住问道，前后端分离的“灵魂”是什么呢？各位看官们稍坐，且听我一一道来！\n问题现状 在谈到前后端分离这个话题的时候，在公司层面上对应地往往是组织架构的分离，典型的做法就是让前端和后端成为两个不同的团队，其中，前端团队负责表示层的实现，不限于页面布局、样式风格、交互逻辑等等；后端团队负责数据接口的实现，不限于数据库设计、接口设计、编写 API 等等。对应到 Vue.js 里，前端团队负责写各种各样的页面/组件、数据绑定，后端团队负责提供各种各样的数据接口，这听起来非常地合理，对不对？的确，主流的前后端分离实践都是这样讲的，所以，我们只要套用这个模型，就可以达到预期的效果，对不对？可惜，人类习惯于为这个世界寻找某种颠扑不破的真理，可恰恰人类本身才是这个世界里最不稳定的存在？疫情常态化的当下，每次都被病毒一通嘲讽，抄作业都不会抄啊！\n前后端分离模式下的协同开发\r首先，第一个问题，前、后端团队没有形成“契约”，前端团队拿到原型以后就开始设计页面，ViewModel 中的字段命名、定义完全是由前端团队凭“感觉”写出来的，人类离谱就离谱在，可以靠“感觉”这种玄之又玄的东西决定很多事情。这样做的后果就是，后面真正对接后端接口的时候，发现大量的字段没法对应上，不得不再折腾一遍数据绑定，如果是中途由别人来接手，那么面对的可能就是不同的数据结构间的映射转换。试想，后端程序员尚有 AutoMapper 和 Mapster 可以用，前端程序员可就没有那么幸运啦！更不必说，前端天生比后端面临更频繁的改动，只要涉及到页面布局、交互逻辑的变化，ViewModel 的修改基本无可避免，这样就导致同一个页面多次返工，我相信这个结果大家都不想看到。\n其次，当前、后端团队约定好接口文档以后，双方都按照这份接口文档去完成各自的开发工作，这样听起来简直不能更合理对不对？实际上，在后端团队完成接口开发以前，前端团队会有一段时间的“真空期”或者“黑写期”，因为前端并不知道这段代码能否在真实的环境下工作。此时，前端团队可能会造一点假数据来进行接口模拟，得益于 JavaScript 这门语言的高度灵活、自由，前端团队可能会直接调用一个本地函数来返回假数据，这意味着它并不会触发真实地 HTTP 请求。那么，当有一天后端团队完成了接口开发，你将会把这些本地函数替换为 Axios 的方法，甚至在更极端的情况下，前端团队不能访问后端团队的接口，此时，双方会就本地函数还是 Axios 方法产生一场拉锯战，你告诉我，还有什么比这更折磨一个人的吗？\n所以，综合下来，其实是两个非常普遍的问题：\n第一，前、后端团队如何制定一份对协同有利的接口文档，这份文档是通过工具生成还是人工编写。我个人是特别讨厌用 IM 或者邮件来发送接口文档的，因为没办法做到版本控制或者说让所有手中都有一份最新的接口的文档。\n第二，如何管理项目中用到的各种假数据，以及如何让项目在假数据和真实接口中“无痛”切换。前端项目的特点是所见即所得，这让它比看不见、摸不着的后端项目更受用户青睐，毕竟还有什么比能让用户亲眼看到更亲切的东西呢？\n在“小步快跑、快速迭代”的敏捷思想的驱使下，我们经常需要给用户演示各种功能。也许，在某个时刻，页面上的数据亦真亦假，你还会觉得，管理这些假数据没什么意义吗？而这正是驱使我了解 Mock.js 的动力所在，世上的很多事情，你未必能如愿以偿、做到最好，可你依然要了解什么是最好，“山不厌高，海不厌深”，向不那么完美的世界妥协是现实，永远值得去追寻更完美的世界是理想，这两者在我心目中并不冲突，你觉得呢？\n改进思路 OK，既然找到了问题的症结所在，我们逐一对症下药即可，就像“三过家门而不入”的大禹，选择用疏导的方式治水，让洪水通过疏通的河道流到大海中去，而不是靠一味地“堵”，程序中 90% 的代码都是在给用户“打补丁”，防止对方做出什么骚操作来，那么，是不是可以用某种方式去引导对方呢？我最讨厌听到的话就是，用户想要怎么怎么样，这是没有办法的事情，如果只需要一个传话筒，我们为什么不直接用传呼机呢？作为一个老古董，恐怕现在的 00 后都不知道什么是传呼机。你生命中当下流行或者推崇的东西，总有一天会过期。可即便如此，你还是要全力以赴。显然，这是个哀伤的故事。\nSwagger 对于接口文档的管理问题，我自始至终都推荐 Swagger 这个神器，因为我和这个世界上的绝大多数的程序员一样，都认同一种相对朴素的价值观，即 “懒惰是一种美德”。因为我不喜欢靠人工来维护接口文档，所以，只要有机会用上 Swagger，我一定会用 Swagger 来管理接口文档。不管是过去写 API 和 MVC，还是现在写 gRPC。对我来说，选择 Swagger 是一件自然而然的事情，因为我懒，因为我不理解为什么有人需要导出 Word 或者 Pdf 格式的接口文档。也许，Swagger 那千篇一律的页面风格会让人感到无所适从，喜欢的人非常喜欢，讨厌的人非常讨厌。在前、后端分离的项目中，有一份白纸黑字的接口文档，显然要比“口口相传”靠谱得多。当然，如果你有足以媲美 Swagger 的接口文档管理工具/平台，欢迎大家在评论区留言分享。下面是我曾经写过的关于 Swagger 的文章：\n通过 ApiExplorer 为 Swagger 提供 MVC 扩展 gRPC 搭配 Swagger 实现微服务文档化 .NET Core POCOController 在动态 Web API 中的应用 ASP.NET Core gRPC 打通前端世界的尝试 Mock.js 好了，下面我们来介绍今天这篇博客的主角：Mock.js。如果你有养成写单元测试的好习惯，那么，你一定对 Mock 的概念了如指掌。在 .NET 的生态中，有一个大名鼎鼎的模拟库：Moq，它可以让我们更加方便地模拟各种对象的行为。什么情况下需要模拟呢？我想，是目标对象不可用的时候。如果把这个认知迁移到前端开发中，我们就会发现前端依赖最多的其实是后端的接口。那么，有没有一种方案，可以让前端在后端接口不可用的情况下，模拟出调用后端接口的效果呢？而这，就是 Mock.js 存在的意义。事实上，我一开始就讲到，在后端团队完成接口开发以前，前端团队会有一段时间的“真空期”或者“黑写期”，而这段时间显然是最需要进行模拟的一个环节。综上所述，不管是技术层面还是流程层面，这个模拟的阶段都是真实存在着的，并非是人为捏造或者臆想出来的东西。\n在考虑引入 Mock.js 以前，项目中充斥着类似于下面这样的代码。你不得不承认，JavaScript 是一门灵活而且强大的编程语言，考虑到它返回的是一个 Promise，所以，它和调用后端接口相比没有任何区别，你可以直接在组件中调用这个方法：\nexport async function getMessages() { return new Promise(resolve =\u0026gt; { const data = [{ \u0026#39;id\u0026#39;: \u0026#39;065CB06C-E082-0E55-24A5-54917C4BD182\u0026#39;, \u0026#39;eventTime\u0026#39;: \u0026#39;2017-07-07 08:50:16\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;人生若只如初见，何事秋风悲画扇\u0026#39; },{ \u0026#39;id\u0026#39;: \u0026#39;C0C3298D-6A91-DE5B-EBD2-30F60E59E4EE\u0026#39;, \u0026#39;eventTime\u0026#39;: \u0026#39;2017-08-07 18:50:16\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;醉后不知天在水 满船清梦压星河\u0026#39; } //... ] return resolve({ code: 200, data: data }) }) } 这个方案最大的问题是，当后端接口准备就绪以后，你还需要用实际的请求过程比如 Axios 替换掉这个方法，虽然这个工作量并不算特别大，可我总觉得这不是一个正确的思路，虽然代码频繁地改动对程序员来说是家常便饭，我是个不大愿意在同一件事情上来回折腾的人，我更喜欢古龙武侠小说里那种一招制敌的感觉，我们来一起看看 Mock.js 是如何解决这个问题的：\n// 通过 npm 安装 npm install mockjs // 通过 yarn 安装 yarn add mockjs 首先，我们通过 npm 或者 yarn 安装 Mock.js，对于上面的这个例子，我们可以像下面这样改造。一开始我们有提到，如果去管理这些假数据，我这里的建议是按照模块建立相应的文件夹，并最终通过一个统一的入口 /mock/index.js 来导入这些假数据。\n// ./mock/message/index.js const Mock = require(\u0026#39;mockjs\u0026#39;); const Random = Mock.Random; // 生成 10 条消息并赋值给 data 字段 const messageList = Mock.mock({ \u0026#39;data|10\u0026#39;: [{ eventTime: () =\u0026gt; Random.datetime(), content: () =\u0026gt; Random.csentence(5, 10), id: () =\u0026gt; Random.guid() }] }) export function getMessageList() { return { code: 200, data: messageList.data } } 接下来，我们在 /mock/index.js 这个文件中注册相应的路由即可：\nconst Mock = require(\u0026#39;mockjs\u0026#39;); import { getMessageList, addMessage } from \u0026#39;./message\u0026#39; Mock.setup({ timeout: 500 }) Mock.mock(/\\/api\\/messages/, \u0026#39;get\u0026#39;, getMessageList) 需要注意的是，这里一定要使用 /\\/api\\/messages/ 这种带转义符的方式来定义路由，它表示这个请求会被拦截，实际调用的是 getMessageList 这个方法，如果我们直接写 /api/messages/ 会返回 404，这一点非常重要。现在，我们就可以直接在组件或者页面内访问这个路由：\nthis.$http.get(\u0026#34;/api/messages\u0026#34;).then((res) =\u0026gt; { let resData = res.data; if (resData.code == 200) { this.messageList = resData.data; } }); 其中，$http 是挂载到 Vue 原型上的 Axios 对象，显而易见，只要前、后端都严格按照文档来定义这个路由，那么，这个代码是不需要再调整的。等后端接口开发完成以后，我们只需要切换到正式地址联调即可。当然，前端和后端可能会部署在不同的服务器上，而这个对我们的影响，无外乎是修改一下 Axios 的 baseURL。那么，Vue 怎么知道什么时候调用 Mock，什么时候调用真实接口呢？答案就藏在入口文件 main.js 中，只要我们导入了 /mock/index.js 这个文件，Mock 就会生效：\nimport \u0026#39;./mock/index\u0026#39; 如下图所示，这里的的消息列表完全由 Mock.js 驱动，其中的消息内容、时间等都是随机生成的：\nVue.js 搭配 Mock.js 使用效果\r在平时的工作中，我常常听到一个词叫做“造数据”，本质上这依然属于 Mock 的范畴。现实生活中使用到的数据，毫无疑问会比这篇文章中的例子更加复杂和多样化，典型的有地址、邮箱、身份证号、IP 等等。对于这些，Mock.js 同样可以模拟，下面列举了常见的 API，更详细的可以参考官方文档中的 示例：\n// 随机生成省份、直辖市或者自治区 Random.province() Mock.mock(\u0026#39;@province\u0026#39;) Mock.mock(\u0026#39;@province()\u0026#39;) // 随机生成 URL Random.url() Mock.mock(\u0026#39;@url\u0026#39;) Mock.mock(\u0026#39;@url()\u0026#39;) // 随机生成中文段落 Random.cparagraph() Mock.mock(\u0026#39;@cparagraph\u0026#39;) Mock.mock(\u0026#39;@cparagraph()\u0026#39;) // 随机生成日期 Random.datetime() Mock.mock(\u0026#39;@datetime\u0026#39;) Mock.mock(\u0026#39;@datetime()\u0026#39; 如果用使用过 .NET 里的 Bogus 这个库，相信我，你会对这一切感到相当亲切。\n登高望远 OK，写到这里一切，从点题的角度来看，这篇文章已经可以画上完美的句号，因为 Mock.js 在 Vue 中的基本用法，其实已经完全讲完啦！当然，我们在这个基础上尝试更多的东西。比如，YAPI，这同样是一个功能强大的 API 管理平台，它可以导入 Swagger 格式的接口文档，并为每一个 API 接口创建对应的 Mock 接口，在这种情况下，前端可以使用对应的 Mock 接口完成前期开发。当然，我认为，一个逻辑上自洽的流程应该是，定义接口、生成文档、生成 Mock 接口、使用 Mock 接口、使用真实接口，大家觉得呢？\n通过 YAPI 管理 API 接口\r个人心目中，只有国产软件 Apifox 能勉强达到这一点，如图所示，每当我们填入一个 API 接口定义的时候，它会为我们创建对应的用例，每个用例对应一个 Mock 的地址，某种程度上讲，它的确做到它对外宣称的集 API 文档、API 调试、API Mock、API 自动化测试 等多种特性于一身，同样地，它可以导出各种格式的 API 文档，我个人觉得这个软件挺好用的，YAPI 每次都需要打开网页去增加新接口，这个从流程上讲更潜移默化一点。\n通过 Apifox 管理 API 接口\r虽然但是，有那么多的工具帮助你管理文档、Mock 接口，可你还是做不好一个项目，你说，这是为什么呢？我猛然间想起去年写过的一篇博客，《使用 HttpMessageHandler 实现 HttpClient 请求管道自定义》，里面就在写，如何借助 HttpMessageHandler 做 API 接口的 Mock，人啊，兜兜转转，大抵又回到了原点罢！\n",
            "tags": ["Vue","Mock","前端","研发"],
            "categories": ["前端开发"],
            "thumbnail": ""
        },
        {
            "title": "ASP.NET Core 搭载 Envoy 实现微服务的监控预警",
            "date": "2021-07-10",
            "permalink": "http://localhost:1313/posts/1519021197/",
            "summary": "在构建微服务架构的过程中，我们会接触到服务划分、服务编写以及服务治理这一系列问题。其中，服务治理是工作量最密集的一个环节，无论是服务发现、配置中心、故障转移、负载均衡、健康检查……等等，这一切的一切，本质上都是为了更好地对服务进行管理，尤其是当我们面对数量越来越庞大、结构越来越复杂的集群化环境的时候，我们需要一种科学、合理的管理手段。博主在上一家公司工作的时候，每次一出现线上故障，研发都要第一时间对问题进行排查和处理，而当时的运维团队，对于微服务的监控止步于内存和CPU，无法系统而全面的掌握微服务的运行情况，自然无法从运维监控的角度给研发部门提供方向和建议。所以，今天这篇文章，博主想和大家聊聊，如何利用Envoy来对微服务进行可视化监控。需要说明的是，本文的技术选型为Envoy + ASP.NET Core + Prometheus + Grafana，希望以一种无侵入的方式集成到眼下的业务当中。本文源代码已上传至 Github ，供大家学习参考。\n从 Envoy 说起 在介绍 Envoy 的时候，我们提到了一个词，叫做可观测的。什么叫可观测的呢？官方的说法是， Envoy 内置了stats模块，可以集成诸如prometheus/statsd等监控方案，可以集成分布式追踪系统，对请求进行追踪。对于这个说法，是不是依然有种云里雾里的感觉？博主认为，这里用Metrics这个词会更准确点，即可度量的，你可以认为， Envoy 提供了某种可度量的指标，通过这些指标我们可以对 Envoy 的运行情况进行评估。如果你使用过 Elastic Stack 中的 Kibana，就会对指标(Metrics)这个词汇印象深刻，因为 Kibana 正是利用日志中的各种指标进行图表的可视化的。庆幸的是，Grafana 中拥有与 Kibana 类似的概念。目前， Envoy 中支持三种类型的统计指标：\nCounter：即计数器，一种只会增加不会减少的无符号整数。例如，总请求数 Gauge：即计量，一种可以同时增加或者同时减少的无符整数。例如，状态码为200的有效请求数 Timer/Hitogram：即计时器/直方图，一种无符号整数，最终将产生汇总百分位值。Envoy 不区分计时器（通常以毫秒为单位）和 原始直方图（可以是任何单位）。 例如，上游请求时间（以毫秒为单位）。 在今天的这篇文章中，除了 Envoy 以外，我们还需要两位新朋友的帮助，它们分别是Prometheus 和 Grafana。其中，Prometheus 是一个开源的完整监控解决方案，其对传统监控系统如 Nagios、Zabbix 等的测试和告警模型进行了彻底的颠覆，形成了基于中央化的规则计算、统一分析和告警的新模型。可以说，Prometheus 是完整监控解决方案中当之无愧的后起之秀，它最为人所称道的是它强大的数据模型，在 Prometheus 中所有采集到的监控数据吗，都以指标(Metrics)的形式存储在时序数据库中。和传统的关系型数据库中使用的 SQL 不同，Prometheus 定义一种叫做 PromQL 的查询语言，来实现对监控数据的查询、聚合、可视化、告警等功能。\nPrometheus \u0026amp;amp; Grafana 的奇妙组合\r目前，社区中提供了大量的第三方系统的采集功能的实现，这使得我们可以轻易地对MySQL、PostgresSQL、Consul、HAProxy、RabbitMQ， Redis等进行监控。而 Grafana 则是目前主流的时序数据展示工具，正是因为这个原因， Grafana 总是和 Prometheus 同时出现， Prometheus 中采集到监控数据以后，就可以由 Grafana 赖进行可视化。相对应地，Grafana 中有数据源的概念，除了 Prometheus 以外，它还可以使用来自 Elasticsearch 、InfluxDB 、MySQL 、OpenTSDB 等等的数据。基于这样一种思路，我们需要 Envoy 提供指标信息给 Prometheus ，然后再由 Grafana 来展示这些信息。所以，我们面临的主要问题，其实是怎么拿到 Envoy 中的指标信息，以及怎么把这些指标信息给到 Prometheus 。",
            "content": "在构建微服务架构的过程中，我们会接触到服务划分、服务编写以及服务治理这一系列问题。其中，服务治理是工作量最密集的一个环节，无论是服务发现、配置中心、故障转移、负载均衡、健康检查……等等，这一切的一切，本质上都是为了更好地对服务进行管理，尤其是当我们面对数量越来越庞大、结构越来越复杂的集群化环境的时候，我们需要一种科学、合理的管理手段。博主在上一家公司工作的时候，每次一出现线上故障，研发都要第一时间对问题进行排查和处理，而当时的运维团队，对于微服务的监控止步于内存和CPU，无法系统而全面的掌握微服务的运行情况，自然无法从运维监控的角度给研发部门提供方向和建议。所以，今天这篇文章，博主想和大家聊聊，如何利用Envoy来对微服务进行可视化监控。需要说明的是，本文的技术选型为Envoy + ASP.NET Core + Prometheus + Grafana，希望以一种无侵入的方式集成到眼下的业务当中。本文源代码已上传至 Github ，供大家学习参考。\n从 Envoy 说起 在介绍 Envoy 的时候，我们提到了一个词，叫做可观测的。什么叫可观测的呢？官方的说法是， Envoy 内置了stats模块，可以集成诸如prometheus/statsd等监控方案，可以集成分布式追踪系统，对请求进行追踪。对于这个说法，是不是依然有种云里雾里的感觉？博主认为，这里用Metrics这个词会更准确点，即可度量的，你可以认为， Envoy 提供了某种可度量的指标，通过这些指标我们可以对 Envoy 的运行情况进行评估。如果你使用过 Elastic Stack 中的 Kibana，就会对指标(Metrics)这个词汇印象深刻，因为 Kibana 正是利用日志中的各种指标进行图表的可视化的。庆幸的是，Grafana 中拥有与 Kibana 类似的概念。目前， Envoy 中支持三种类型的统计指标：\nCounter：即计数器，一种只会增加不会减少的无符号整数。例如，总请求数 Gauge：即计量，一种可以同时增加或者同时减少的无符整数。例如，状态码为200的有效请求数 Timer/Hitogram：即计时器/直方图，一种无符号整数，最终将产生汇总百分位值。Envoy 不区分计时器（通常以毫秒为单位）和 原始直方图（可以是任何单位）。 例如，上游请求时间（以毫秒为单位）。 在今天的这篇文章中，除了 Envoy 以外，我们还需要两位新朋友的帮助，它们分别是Prometheus 和 Grafana。其中，Prometheus 是一个开源的完整监控解决方案，其对传统监控系统如 Nagios、Zabbix 等的测试和告警模型进行了彻底的颠覆，形成了基于中央化的规则计算、统一分析和告警的新模型。可以说，Prometheus 是完整监控解决方案中当之无愧的后起之秀，它最为人所称道的是它强大的数据模型，在 Prometheus 中所有采集到的监控数据吗，都以指标(Metrics)的形式存储在时序数据库中。和传统的关系型数据库中使用的 SQL 不同，Prometheus 定义一种叫做 PromQL 的查询语言，来实现对监控数据的查询、聚合、可视化、告警等功能。\nPrometheus \u0026amp;amp; Grafana 的奇妙组合\r目前，社区中提供了大量的第三方系统的采集功能的实现，这使得我们可以轻易地对MySQL、PostgresSQL、Consul、HAProxy、RabbitMQ， Redis等进行监控。而 Grafana 则是目前主流的时序数据展示工具，正是因为这个原因， Grafana 总是和 Prometheus 同时出现， Prometheus 中采集到监控数据以后，就可以由 Grafana 赖进行可视化。相对应地，Grafana 中有数据源的概念，除了 Prometheus 以外，它还可以使用来自 Elasticsearch 、InfluxDB 、MySQL 、OpenTSDB 等等的数据。基于这样一种思路，我们需要 Envoy 提供指标信息给 Prometheus ，然后再由 Grafana 来展示这些信息。所以，我们面临的主要问题，其实是怎么拿到 Envoy 中的指标信息，以及怎么把这些指标信息给到 Prometheus 。\n原理说明 首先，我们来简单阐述一下原理。在 Envoy 的早期版本中，通常是通过 statsd 来采集 Envoy 中的信息，这些信息会被存储在 Prometheus 中，然后由 Grafana 从 Prometheus 中读取数据并展示为图表。而在 Envoy 最新的版本中，Envoy 本身就可以输出 Prometheus 需要的数据格式，故而就不再需要 statsd 这样一个监控工具。关于第一种方案，大家可以参考这篇文章：Envoy Service Mesh、Prometheus和Grafana下的微服务监控。这里，为了简单起见，我们采用第二种方案来进行集成。在接下来的例子中，我们会部署下面四个服务，我们希望在调用 gRPC 服务的时候，可以在 Grafana 看到相关的监控指标：\nversion: \u0026#34;3\u0026#34; services: # prometheus prom: image: quay.io/prometheus/prometheus:latest volumes: - ./Prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:rw ports: - 2333:9090 # envoy_gateway envoy_gateway: build: Envoy/ ports: - \u0026#34;9090:9090\u0026#34; - \u0026#34;9091:9091\u0026#34; volumes: - ./Envoy/envoy.yaml:/etc/envoy/envoy.yaml # grpc_service grpc_service: build: GrpcService/GrpcService/ ports: - \u0026#34;8082:80\u0026#34; environment: ASPNETCORE_URLS: \u0026#34;http://+\u0026#34; ASPNETCORE_ENVIRONMENT: \u0026#34;Development\u0026#34; # grafana grafana: image: grafana/grafana ports: - \u0026#34;3000:3000\u0026#34; environment: - “GF_SECURITY_ADMIN_PASSWORD=Gz2020@” - “GF_INSTALL_PLUGINS=alexanderzobnin-zabbix-app” restart: always depends_on: - prom 接下来，为了让 Prometheus 可以直接读取 Envoy 中输出的指标数据，我们需要在其配置文件prometheus.yml中添加一个对应的任务：\nglobal: scrape_interval: 15s external_labels: monitor: \u0026#39;codelab-monitor\u0026#39; scrape_configs: - job_name: \u0026#39;prometheus\u0026#39; scrape_interval: 5s static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] - job_name: \u0026#39;envoy\u0026#39; metrics_path: \u0026#39;/stats/prometheus\u0026#39; scrape_interval: 15s scrape_timeout: 15s static_configs: - targets: [\u0026#39;localhost:9091\u0026#39;] 大家还记得 Envoy 中提供的管理接口吗？我们说 Envoy 提供了 Prometheus 格式的指标数据，其实就是指 Envoy 管理接口中的 stats/prometheus 接口，它对应的地址为：http://localhost:9091/stats/prometheus，直接访问这个地址，我们就可以得到下面的结果，这就是 Prometheus 需要的指标数据格式：\nPrometheus 需要的指标数据格式\r数据源与可视化 现在，万事俱备，我们通过docker-compose启动服务即可，默认情况下，Prometheus 使用9090端口，Grafana 使用3000端口，其中，Grafana 默认的账号为admin/admin，建议大家第一次登录后，及时修改默认的账号密码。\nPrometheus 运行效果展示\rGrafana 运行效果展示\r接下来，我们可以注意到，Prometheus 中两个 target 都已正常启动，这表示它们开始采集数据，我们还可以通过 Graph 菜单来查看当前采集到的数据。\nPrometheus 采集数据\r那么，数据采集到 Prometheus 以后，如何在 Grafana 中进行图表的可视化展示呢？首先，我们需要在 Grafana 中添加一个数据源，点击左侧第6个图标就可以找到入口。显然，这里的数据源就是 Prometheus ：\nGrafana 添加数据源\r接下来，我们可以到官方的 社区 里找一个 Envoy 的模板，这是一个别人做好的 Dashboard，我们暂时用这个模板来看看效果。随着学习的深入，我们会先从自定义图表开始做起，最终，我们会拥有一个属于自己的 Dashboard 。这里，我们选择一个 Dashboard 模板后，复制其ID，并在 Grafana 中进行导入，导入的时候需要选择数据源，我们选择 Prometheus 即可。接下来，就是见证奇迹的时刻：\nEnvoy 监控面板效果展示\r自定义图表 好了，如果大家阅读过官方文档，就会知道，除了 Prometheus ，像常见的 MySQL、Nginx 等，都可以作为 Grafana 的数据源，如果你需要监控 Nginx 的某个指标，这会是个非常不错的思路。那么。如何按照个人/领导的要求，对 Dashboard 进行进一步的定制呢？这就要说到 Grafana 的自定义图表，这里，我们通过下面的例子来进行说明：\nrate(envoy_http_rq_total{envoy_http_conn_manager_prefix=\u0026#34;grpc_json\u0026#34;, instance=\u0026#34;192.168.6.120:9902\u0026#34;}[5m]) 在 Prometheus 中，采用的是与 OpenTSDB 类似的时序格式：\n\u0026lt;metric name\u0026gt;{\u0026lt;label name\u0026gt;=\u0026lt;label value\u0026gt;, ...} 可以注意到，每一个指标含有多个键值形式的标签。例如，http_requests_total{method=\u0026quot;POST\u0026quot;}表示的是所有 HTTP 请求中的 POST 请求。\n此外，除了上文中提到过的 Counter 、 Gauge 、Histogram 这三种类型，Prometheus 还支持一种叫做 Summary 的类型。和大多数语言类似，这门被叫做 PromQL 的语言，(1)：支持常见的运算符，例如算术运算符、比较运算符、逻辑运算符、聚合运算符等等。(2)：支持大量的内置函数，例如，由浮点型转换为整型的floor和ceil，计算平均速率的rate等等：\nfloor(avg(http_requests_total{code=\u0026#34;200\u0026#34;})) ceil(avg(http_requests_total{code=\u0026#34;200\u0026#34;})) rate(http_requests_total[5m]) 在这里，我们给出的示例，它表示的是5分钟内 HTTP 请求的平均数目。我们可以在 Prometheus 中的 Graph 菜单对其结果进行查看：\n在 Prometheus 中查询 Envoy 指标数据\r通常，我们可以在这里对查询语句做简单的调试，而如果需要将其集成到 Grafana 中，我们就需要在 Grafana 新建一个图表，可以注意到，两者的语法是完全相同的，这里唯一的不同点在于，时间间隔从固定的5分钟变成了一个变量：\n在 Grafana 中查询 Envoy 指标数据\r此时，我们就完成了一个自定义图表的制作，其中的关键有两点，其一是了解每一个指标的含义，其二是了解每一个内置函数的用法。革命尚未成功，同志仍须努力。这些内容无法在一篇博客里全部讲到，如果需要做进一步的探索，还是建议大家去看官方文档，这里博主可以给大家推荐一个不错的中文文档。\n文本小结 本文介绍了利用 Prometheus 和 Grafana 对 Envoy 进行监控预警的方案。在 Envoy 的早期版本中，主流的方案都是通过 statsd 来采集 Envoy 的指标信息，而在 Envoy 最新版本中，它本身就可以输出 Prometheus 需要的数据格式，我们只需要在 Prometheus 的配置文件中指定stats/prometheus这个地址即可。Prometheus 采用了和 OpenTSDB 类似的时序格式，每一个指标均含有多个键值形式的标签。Prometheus 在此基础上提供了 PromQL 查询语言，我们可以利用这个查询语言在 Grafana 中制作自定义图表，这些自定义图表可以是一个瞬时数据、可以是一个区间数据，或者是一个纯量数字，因此，我们可以按照自己的喜好去定制整个仪表盘，结合实际的业务场景来决定要关注哪些指标。除此以外，我们还可以在 Prometheus 定义告警规则，当业务系统出现问题时，可以第一时间通知运维或者研发团队。在后端研发越来越服务化、集群化的今天，我们不能永远都盯着 CRUD 这一亩三分地，更普遍的，可能是针对 Docker、K8S、Redis、MySQL 等等基础设施的监控，扁鹊见蔡桓公的故事大家耳熟能详，防微杜渐，无论过去还是现在甚至将来都是一样的。好了，以上就是这篇博客的全部内容啦，谢谢大家！\n",
            "tags": ["微服务","Envoy","Prometheus","Grafana","监控"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "ASP.NET Core 搭载 Envoy 实现微服务的负载均衡",
            "date": "2021-07-05",
            "permalink": "http://localhost:1313/posts/3599307336/",
            "summary": "如果说，我们一定要找出一个词来形容这纷繁复杂的世界，我希望它会是熵。有人说，熵增定律是宇宙中最绝望的定律，所谓熵，即是指事物混乱或者无序的程度。在一个孤立系统下，熵是不断在增加的，当熵达到最大值时，系统就会出现严重混乱，直至最终走向死亡。从某种意义上来讲，它揭示了事物结构衰退的必然性，甚至于我们的人生，本来就是一场对抗熵增的旅程。熵增的不可逆性，正如时光无法倒流一般，古人说，“覆水难收”正是这个道理。同样地，当我们开始讨论微服务的划分/编写/治理的时候，当我们使用服务网格来定义微服务架构的时候……我们是否有意或者无意的增加了系统中的熵呢？**一个孤立的系统尚且会因为熵增而最终走向死亡，更何况是相互影响和制约的复杂系统呢？**现代互联网企业都在追求4个9(即99.99%)的高可用，这意味着年平均停机时长只有52.56分钟。在此之前。我们介绍过重试和熔断这两种故障转移的策略，而今天我们来介绍一种更朴素的策略：负载均衡。\n什么是负载均衡 负载均衡，即Load Banlancing，是一种计算机技术，用来在多个计算机(计算机集群)、网络连接、CPU、磁盘驱动器或其它资源中分配负载，以达到最优化资源使用、最大化吞吐率、最小化响应时间、避免过载的目的。\n我们可以注意到，在这个定义中，使用负载均衡技术的直接原因是避免过载，而根本原因则是为了优化资源使用，确保最大吞吐量、最小响应时间。所以，这本质上是一个局部最优解的问题，而具体的手段就是\u0026quot;多个\u0026quot;。有人说，技术世界不过是真实世界的一个镜像，联系生活中实际的案例，我们发现负载均衡比比皆是。譬如车站在面对春运高峰时增加售票窗口，银行通过多个业务窗口来为客户办理业务……等等。这样做的好处显而易见，可以大幅度地减少排队时间，增加\u0026quot;窗口\u0026quot;这个行为，在技术领域我们将其称为：水平扩展，因为有多个\u0026quot;窗口\u0026quot;，发生单点故障的概率就会大大降低，而这正是现在软件追求的三\u0026quot;高\u0026quot;：高性能、高可用、高并发。\n银行柜员窗口示意图\r每次坐地铁经过小寨，时常听到地铁工作人员举着喇叭引导人们往不同的出口方向走动。此时，工作人员就是一个负载均衡器，它要做的就是避免某一个出口人流量过载。**从熵的角度来看，人流量过载，意味着无序/混乱状态加剧，现代社会通过道德和法律来对抗熵增，人类个体通过自律来对抗熵增。**有时候，我会忍不住去想，大人与小孩儿愈发内卷的恶性竞争，除了给这个世界带来更多的熵以外，还能带来什么？如果参考社会达尔文主义的理论，在这个弱肉强食的世界里，增加熵是人为的选择，而同样的，你亦可以选择\u0026quot;躺平\u0026quot;。\n负载均衡器示意图\rOK，将思绪拉回到负载均衡，它所做的事情，本质上就是控制信息或者说流量流动的方向。一个网站，以集群的方式对外提供服务，你只需要输入一个域名，它就可以把请求分发到不同的机器上面去，而这就是所谓的负载均衡。目前，负载均衡器从种类上可以分为：基于DNS、基于MAC地址(二层)、基于IP(三层)、基于IP和Port(四层)、基于HTTP(七层)。\nOSI七层模型与TCP/IP五层模型\r譬如，博主曾经参与过伊利的项目，它们使用的就是一个四层的负载均衡器：F5。而像更常见Nginx、HAProxy，基本都是四层和七层的负载均衡器，而Envoy就厉害了，它可以同时支持三/四/七层。负载均衡器需要配合负载均衡算法来使用，典型的算法有：轮询法、随机法、哈希法、最小连接数法等等，而这些算法都可以结合加权算法引出新的变式，这里就不再一一列举啦。\nEnvoy中的负载均衡 通过上一篇博客，我们已经了解到，Envoy中一个HTTP请求的走向，大致会经历：客户端、侦听器(Listeners)、集群(Clusters)、终结点(Endpoints)、服务(ervices)这样几个阶段。其中，一个集群可以有多个终结点(Endpoints)。所以，这里天然地就存在着负载均衡的设计。因为，负载均衡本质上就是告诉集群，它应该选择哪一个终结点(Endpoints)来提供服务。而之所以我们需要负载均衡，一个核心的原因，其实是因为我们选择了分布式。\nEnvoy架构图：负载均衡器连接集群和服务\r如果类比RabbitMQ、Kafka和Redis，你就会发现，这些产品中或多或少地都会涉及到主(Leader)、从(Follower)以及推举Leader的实现，我个人更愿意将其看作是更广义的负载均衡。最直观的，它可以分担流量，简称分流，不至于让某一台服务器满负荷做运行。其次，它可以作为故障转移的一种方案，人生在世，多一个B计划，就多一种选择。同样地，多一台服务器，就多一分底气。最后，它可以指导某一个产品或者功能的推广，通过给服务器设置不同的权重，在必要的时候，将流量局部地导入某一个环境，腾讯和阿里这样的大厂，经常利用这种方式来做灰度测试。\nEnvoy中支持常用的负载均衡算法，譬如：ROUND_ROBIN(轮询)、LEAST_REQUEST(最少请求)、RING_HASH(哈希环)、RANDOM(随机)、MAGLEV(磁悬浮)、CLUSTER_PROVIDED等等。因为一个集群下可以有多个终结点，所以，在Envoy中配置负载均衡，本质上就是在集群下面增加终结点，而每个终结点则会对应一个服务，特殊的点在于，这些服务可能是通过同一个Dockerfile或者Docker镜像来构建的。所以，一旦理解了这一点，Envoy的负载均衡就再没有什么神秘的地方。例如，下面的代码片段展示了，如何为WeatherService这个集群应用负载均衡：\nclusters: # Weather Service - name: weatherservice connect_timeout: 0.25s type: STRICT_DNS # ROUND_ROBIN(轮询） # LEAST_REQUEST(最少请求) # RING_HASH(哈希环) # RANDOM(随机) # MAGLEV(磁悬浮) # CLUSTER_PROVIDED lb_policy: LEAST_REQUEST load_assignment: cluster_name: weatherservice endpoints: - lb_endpoints: - endpoint: address: socket_address: address: weatherservice1 port_value: 80 - endpoint: address: socket_address: address: weatherservice2 port_value: 80 是不是觉得特别简单？我想说，也许是Envoy更符合人的直观感受一些，理解起来本身没有太大的心智负担。最近看到一个缓存设计，居然还要依赖Kafka，使用者为了使用缓存这个功能，就必须先实现三个丑陋的委托，这就是所谓的心智负担，违背人类的直觉，使用缓存为什么要了解Kafka？到这里，你大概就能了解利用Envoy实现负载均衡的思路，首先是用同一个Dockerfile或者Docker镜像启动多个不同容器(服务)，然后将指定集群下面的终结点指定不同的服务，再告诉集群要用哪一种负载均衡策略即可。\n邂逅 ASP.NET Core OK，说了这么多，这里我们还是用ASP.NET Core写一个例子。可以预见到的是，我们需要一个Envoy网关，一个ASP.NET Core的服务。这里，我们还是用Docker-Compose来编排这些服务，下面是对应的docker-compose.yaml文件：",
            "content": "如果说，我们一定要找出一个词来形容这纷繁复杂的世界，我希望它会是熵。有人说，熵增定律是宇宙中最绝望的定律，所谓熵，即是指事物混乱或者无序的程度。在一个孤立系统下，熵是不断在增加的，当熵达到最大值时，系统就会出现严重混乱，直至最终走向死亡。从某种意义上来讲，它揭示了事物结构衰退的必然性，甚至于我们的人生，本来就是一场对抗熵增的旅程。熵增的不可逆性，正如时光无法倒流一般，古人说，“覆水难收”正是这个道理。同样地，当我们开始讨论微服务的划分/编写/治理的时候，当我们使用服务网格来定义微服务架构的时候……我们是否有意或者无意的增加了系统中的熵呢？**一个孤立的系统尚且会因为熵增而最终走向死亡，更何况是相互影响和制约的复杂系统呢？**现代互联网企业都在追求4个9(即99.99%)的高可用，这意味着年平均停机时长只有52.56分钟。在此之前。我们介绍过重试和熔断这两种故障转移的策略，而今天我们来介绍一种更朴素的策略：负载均衡。\n什么是负载均衡 负载均衡，即Load Banlancing，是一种计算机技术，用来在多个计算机(计算机集群)、网络连接、CPU、磁盘驱动器或其它资源中分配负载，以达到最优化资源使用、最大化吞吐率、最小化响应时间、避免过载的目的。\n我们可以注意到，在这个定义中，使用负载均衡技术的直接原因是避免过载，而根本原因则是为了优化资源使用，确保最大吞吐量、最小响应时间。所以，这本质上是一个局部最优解的问题，而具体的手段就是\u0026quot;多个\u0026quot;。有人说，技术世界不过是真实世界的一个镜像，联系生活中实际的案例，我们发现负载均衡比比皆是。譬如车站在面对春运高峰时增加售票窗口，银行通过多个业务窗口来为客户办理业务……等等。这样做的好处显而易见，可以大幅度地减少排队时间，增加\u0026quot;窗口\u0026quot;这个行为，在技术领域我们将其称为：水平扩展，因为有多个\u0026quot;窗口\u0026quot;，发生单点故障的概率就会大大降低，而这正是现在软件追求的三\u0026quot;高\u0026quot;：高性能、高可用、高并发。\n银行柜员窗口示意图\r每次坐地铁经过小寨，时常听到地铁工作人员举着喇叭引导人们往不同的出口方向走动。此时，工作人员就是一个负载均衡器，它要做的就是避免某一个出口人流量过载。**从熵的角度来看，人流量过载，意味着无序/混乱状态加剧，现代社会通过道德和法律来对抗熵增，人类个体通过自律来对抗熵增。**有时候，我会忍不住去想，大人与小孩儿愈发内卷的恶性竞争，除了给这个世界带来更多的熵以外，还能带来什么？如果参考社会达尔文主义的理论，在这个弱肉强食的世界里，增加熵是人为的选择，而同样的，你亦可以选择\u0026quot;躺平\u0026quot;。\n负载均衡器示意图\rOK，将思绪拉回到负载均衡，它所做的事情，本质上就是控制信息或者说流量流动的方向。一个网站，以集群的方式对外提供服务，你只需要输入一个域名，它就可以把请求分发到不同的机器上面去，而这就是所谓的负载均衡。目前，负载均衡器从种类上可以分为：基于DNS、基于MAC地址(二层)、基于IP(三层)、基于IP和Port(四层)、基于HTTP(七层)。\nOSI七层模型与TCP/IP五层模型\r譬如，博主曾经参与过伊利的项目，它们使用的就是一个四层的负载均衡器：F5。而像更常见Nginx、HAProxy，基本都是四层和七层的负载均衡器，而Envoy就厉害了，它可以同时支持三/四/七层。负载均衡器需要配合负载均衡算法来使用，典型的算法有：轮询法、随机法、哈希法、最小连接数法等等，而这些算法都可以结合加权算法引出新的变式，这里就不再一一列举啦。\nEnvoy中的负载均衡 通过上一篇博客，我们已经了解到，Envoy中一个HTTP请求的走向，大致会经历：客户端、侦听器(Listeners)、集群(Clusters)、终结点(Endpoints)、服务(ervices)这样几个阶段。其中，一个集群可以有多个终结点(Endpoints)。所以，这里天然地就存在着负载均衡的设计。因为，负载均衡本质上就是告诉集群，它应该选择哪一个终结点(Endpoints)来提供服务。而之所以我们需要负载均衡，一个核心的原因，其实是因为我们选择了分布式。\nEnvoy架构图：负载均衡器连接集群和服务\r如果类比RabbitMQ、Kafka和Redis，你就会发现，这些产品中或多或少地都会涉及到主(Leader)、从(Follower)以及推举Leader的实现，我个人更愿意将其看作是更广义的负载均衡。最直观的，它可以分担流量，简称分流，不至于让某一台服务器满负荷做运行。其次，它可以作为故障转移的一种方案，人生在世，多一个B计划，就多一种选择。同样地，多一台服务器，就多一分底气。最后，它可以指导某一个产品或者功能的推广，通过给服务器设置不同的权重，在必要的时候，将流量局部地导入某一个环境，腾讯和阿里这样的大厂，经常利用这种方式来做灰度测试。\nEnvoy中支持常用的负载均衡算法，譬如：ROUND_ROBIN(轮询)、LEAST_REQUEST(最少请求)、RING_HASH(哈希环)、RANDOM(随机)、MAGLEV(磁悬浮)、CLUSTER_PROVIDED等等。因为一个集群下可以有多个终结点，所以，在Envoy中配置负载均衡，本质上就是在集群下面增加终结点，而每个终结点则会对应一个服务，特殊的点在于，这些服务可能是通过同一个Dockerfile或者Docker镜像来构建的。所以，一旦理解了这一点，Envoy的负载均衡就再没有什么神秘的地方。例如，下面的代码片段展示了，如何为WeatherService这个集群应用负载均衡：\nclusters: # Weather Service - name: weatherservice connect_timeout: 0.25s type: STRICT_DNS # ROUND_ROBIN(轮询） # LEAST_REQUEST(最少请求) # RING_HASH(哈希环) # RANDOM(随机) # MAGLEV(磁悬浮) # CLUSTER_PROVIDED lb_policy: LEAST_REQUEST load_assignment: cluster_name: weatherservice endpoints: - lb_endpoints: - endpoint: address: socket_address: address: weatherservice1 port_value: 80 - endpoint: address: socket_address: address: weatherservice2 port_value: 80 是不是觉得特别简单？我想说，也许是Envoy更符合人的直观感受一些，理解起来本身没有太大的心智负担。最近看到一个缓存设计，居然还要依赖Kafka，使用者为了使用缓存这个功能，就必须先实现三个丑陋的委托，这就是所谓的心智负担，违背人类的直觉，使用缓存为什么要了解Kafka？到这里，你大概就能了解利用Envoy实现负载均衡的思路，首先是用同一个Dockerfile或者Docker镜像启动多个不同容器(服务)，然后将指定集群下面的终结点指定不同的服务，再告诉集群要用哪一种负载均衡策略即可。\n邂逅 ASP.NET Core OK，说了这么多，这里我们还是用ASP.NET Core写一个例子。可以预见到的是，我们需要一个Envoy网关，一个ASP.NET Core的服务。这里，我们还是用Docker-Compose来编排这些服务，下面是对应的docker-compose.yaml文件：\nversion: \u0026#39;3\u0026#39; services: envoygateway: build: Envoy/ ports: - \u0026#34;9090:9090\u0026#34; - \u0026#34;9091:9091\u0026#34; volumes: - ./Envoy/envoy.yaml:/etc/envoy/envoy.yaml weatherservice1: build: WeatherService/ ports: - \u0026#34;8082:80\u0026#34; environment: ASPNETCORE_URLS: \u0026#34;http://+\u0026#34; ASPNETCORE_ENVIRONMENT: \u0026#34;Development\u0026#34; weatherservice2: build: WeatherService/ ports: - \u0026#34;8084:80\u0026#34; environment: ASPNETCORE_URLS: \u0026#34;http://+\u0026#34; ASPNETCORE_ENVIRONMENT: \u0026#34;Development\u0026#34; 而对于Envoy来说，主要的工作是维护集群下的终结点信息这块儿。其实，这段配置在本文的上一节就出现过啦，你有多少个服务实例，就配置多少个终结点，请求落在哪一个实例上，就交给Envoy来决定好啦。故而，这里我们不再做更多的解释：\nstatic_resources: listeners: - address: socket_address: address: 0.0.0.0 port_value: 9090 filter_chains: - filters: - name: envoy.filters.network.http_connection_manager typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager codec_type: AUTO stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: backend domains: - \u0026#34;*\u0026#34; routes: - match: prefix: \u0026#34;/api/w\u0026#34; route: auto_host_rewrite: true prefix_rewrite: /Weather cluster: weatherservice http_filters: - name: envoy.filters.http.router clusters: # Weather Service - name: weatherservice connect_timeout: 0.25s type: STRICT_DNS lb_policy: LEAST_REQUEST load_assignment: cluster_name: weatherservice endpoints: - lb_endpoints: - endpoint: address: socket_address: address: weatherservice1 port_value: 80 - endpoint: address: socket_address: address: weatherservice2 port_value: 80 admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 0.0.0.0, port_value: 9091 } 使用docker compose up启动容器，我们可以注意到熟悉的ASP.NET Core的身影，这意味着，我们需要的服务都成功地跑起来了。简单调用下API看看，果然，可以正确地返回结果呢(逃……果然，我是一个喜新厌旧的人，自打用了Encoy以后，再不想用Nginx来做类似的事情(逃……\n通过管理接口查看集群的请求情况\r好了，如何验证我们选择的负载均衡策略呢？这是一个问题。不知道大家还记不记得Envoy的管理接口，它里面可以统计请求的次数，所以，我们只要看看两个服务各自请求了多少次里有好啦！这里以RANDON这个策略为例：\n两个实例拥有不同的请求次数\r可以注意到，两个的请求次数果然是随机的呢？而如果我们换成是ROUND_ROBIN，你就会发现这两个数值一前一后相互追赶。选择哪一种负载均衡策略，这个按大家实际的场景去决定就好。我倒觉得，按最少请求数的策略会好一点。虽然计算机永远不知疲倦地接收人们的指令，可考虑能者多劳这种人类世界里的策略，本身充满道德绑架的意味，使用负载均衡，站在计算机的角度来看，这是避免计算机内卷的一种方案。所以啊，老板们，请不要逮着一个老实人就拼了命的剥削，多学学负载均衡算法，卓别林心目中的摩登时代，我一点都不期待，技术世界总告诉我们要严谨、冷静，可充满人情味的世界，同样值得你去热爱啊。\n本文小结 本文结合物理学中的熵增定律引出了负载均衡这个话题，而**负载均衡的本质，就是在多个计算机(计算机集群)、网络连接、CPU、磁盘驱动器或其它资源中分配负载，以达到最优化资源使用、最大化吞吐率、最小化响应时间、避免过载的目的。**对于这一点，我们可以结合现实生活中的\u0026quot;窗口\u0026quot;排队来理解。负载均衡可以和水平扩展完美结合，通过降低单点的故障率，来达到提升整个系统可用性的目的。接下来，我们介绍了常见的负载均衡器分类及其算法，Envoy中的负载均衡配置，并结合ASP.NET Core编写了一个实际的案例。好了，以上就是这篇博客的全部内容啦，如果大家对于熵增定律或者负载均衡器有任何的看法，欢迎大家在评论区留言，谢谢大家！\n",
            "tags": ["Envoy","微服务","负载均衡","熵增定律"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "ASP.NET Core 搭载 Envoy 实现微服务的反向代理",
            "date": "2021-07-01",
            "permalink": "http://localhost:1313/posts/3599307335/",
            "summary": "回想起来，博主第一次接触到Envoy，其实是在微软的示例项目 eShopOnContainers，在这个示例项目中，微软通过它来为Ordering API、Catalog API、Basket API 等多个服务提供网关的功能。当时，博主并没有对它做深入的探索。此刻再度回想起来，大概是因为那个时候更迷恋领域驱动设计(DDD)的理念。直到最近这段时间，博主需要在一个项目中用到Envoy，终于决定花点时间来学习一下相关内容。所以，接下来这几篇博客，大体上会以记录我学习Envoy的历程为主。考虑到Envoy的配置项特别多，在写作过程中难免会出现纰漏，希望大家谅解。如对具体的配置项存在疑问，请以官方最新的 文档 为准，本文所用的示例代码已经上传至 Github，大家作为参考即可。对于今天这篇博客，我们来聊聊 ASP.NET Core 搭载 Envoy 实现微服务的反向代理 这个话题，或许你曾经接触过 Nginx 或者 Ocelot，这次我们不妨来尝试一点新的东西，譬如，通过Docker-Compose来实现服务编排，如果对我说的这些东西感兴趣的话，请跟随我的脚步，一起来探索这广阔无垠的技术世界吧！\n走近 Envoy Envoy 官网对Envoy的定义是：\nEnvoy 是一个开源边缘和服务代理，专为原生云应用设计。\n而更进一步的定义是：\nEnvoy 是专为大型现代服务导向架构设计的 L7 代理和通讯总线。\n这两个定义依然让你感到云里雾里？没关系，请看下面这张图：\nEnvoy架构图\r注：图片来源\n相信从这张图中，大家多少能看到反向代理的身影，即下游客户端发起请求，Envoy对请求进行侦听(Listeners)，并按照路由转发请求到指定的集群(Clusters)。接下来，每一个集群可以配置多个终结点，Envoy按照指定的负载均衡算法来筛选终结点，而这些终结点则指向了具体的上游服务。例如，我们熟悉的 Nginx ，使用listen关键字来指定侦听的端口，使用location关键字来指定路由，使用proxy_pass关键字来指定上游服务的地址。同样地，Ocelot 使用了类似的上下游(Upstream/Downstream)的概念，唯一的不同是，它的上下游的概念与这里是完全相反的。\n你可能会说，这个Envoy看起来“平平无奇”嘛，简直就像是“平平无奇”的古天乐一般。事实上，Envoy强大的地方在于：\n非侵入式的架构： 独立进程、对应用透明的Sidecar模式 Envoy 的 Sidecar 模式\rL3/L4/L7 架构：Envoy同时支持 OSI 七层模型中的第三层(网络层, IP 协议)、第四层(传输层，TCP / UDP 协议)、第七层(应用层，HTTP 协议) 顶级 HTTP/2 支持： 视 HTTP/2 为一等公民，且可以在 HTTP/2 和 HTTP/1.1间相互转换 gRPC 支持：Envoy 支持 HTTP/2，自然支持使用 HTTP/2 作为底层多路复用协议的 gRPC 服务发现和动态配置：与 Nginx 等代理的热加载不同，Envoy 可以通过 API 接口动态更新配置，无需重启代理。 特殊协议支持：Envoy 支持对特殊协议在 L7 进行嗅探和统计，包括：MongoDB、DynamoDB 等。 可观测性：Envoy 内置 stats 模块，可以集成诸如 prometheus/statsd 等监控方案。还可以集成分布式追踪系统，对请求进行追踪。 Envoy配置文件 Envoy通过配置文件来实现各种各样的功能，其完整的配置结构如下：",
            "content": "回想起来，博主第一次接触到Envoy，其实是在微软的示例项目 eShopOnContainers，在这个示例项目中，微软通过它来为Ordering API、Catalog API、Basket API 等多个服务提供网关的功能。当时，博主并没有对它做深入的探索。此刻再度回想起来，大概是因为那个时候更迷恋领域驱动设计(DDD)的理念。直到最近这段时间，博主需要在一个项目中用到Envoy，终于决定花点时间来学习一下相关内容。所以，接下来这几篇博客，大体上会以记录我学习Envoy的历程为主。考虑到Envoy的配置项特别多，在写作过程中难免会出现纰漏，希望大家谅解。如对具体的配置项存在疑问，请以官方最新的 文档 为准，本文所用的示例代码已经上传至 Github，大家作为参考即可。对于今天这篇博客，我们来聊聊 ASP.NET Core 搭载 Envoy 实现微服务的反向代理 这个话题，或许你曾经接触过 Nginx 或者 Ocelot，这次我们不妨来尝试一点新的东西，譬如，通过Docker-Compose来实现服务编排，如果对我说的这些东西感兴趣的话，请跟随我的脚步，一起来探索这广阔无垠的技术世界吧！\n走近 Envoy Envoy 官网对Envoy的定义是：\nEnvoy 是一个开源边缘和服务代理，专为原生云应用设计。\n而更进一步的定义是：\nEnvoy 是专为大型现代服务导向架构设计的 L7 代理和通讯总线。\n这两个定义依然让你感到云里雾里？没关系，请看下面这张图：\nEnvoy架构图\r注：图片来源\n相信从这张图中，大家多少能看到反向代理的身影，即下游客户端发起请求，Envoy对请求进行侦听(Listeners)，并按照路由转发请求到指定的集群(Clusters)。接下来，每一个集群可以配置多个终结点，Envoy按照指定的负载均衡算法来筛选终结点，而这些终结点则指向了具体的上游服务。例如，我们熟悉的 Nginx ，使用listen关键字来指定侦听的端口，使用location关键字来指定路由，使用proxy_pass关键字来指定上游服务的地址。同样地，Ocelot 使用了类似的上下游(Upstream/Downstream)的概念，唯一的不同是，它的上下游的概念与这里是完全相反的。\n你可能会说，这个Envoy看起来“平平无奇”嘛，简直就像是“平平无奇”的古天乐一般。事实上，Envoy强大的地方在于：\n非侵入式的架构： 独立进程、对应用透明的Sidecar模式 Envoy 的 Sidecar 模式\rL3/L4/L7 架构：Envoy同时支持 OSI 七层模型中的第三层(网络层, IP 协议)、第四层(传输层，TCP / UDP 协议)、第七层(应用层，HTTP 协议) 顶级 HTTP/2 支持： 视 HTTP/2 为一等公民，且可以在 HTTP/2 和 HTTP/1.1间相互转换 gRPC 支持：Envoy 支持 HTTP/2，自然支持使用 HTTP/2 作为底层多路复用协议的 gRPC 服务发现和动态配置：与 Nginx 等代理的热加载不同，Envoy 可以通过 API 接口动态更新配置，无需重启代理。 特殊协议支持：Envoy 支持对特殊协议在 L7 进行嗅探和统计，包括：MongoDB、DynamoDB 等。 可观测性：Envoy 内置 stats 模块，可以集成诸如 prometheus/statsd 等监控方案。还可以集成分布式追踪系统，对请求进行追踪。 Envoy配置文件 Envoy通过配置文件来实现各种各样的功能，其完整的配置结构如下：\n{ \u0026#34;node\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;static_resources\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;dynamic_resources\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;cluster_manager\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;hds_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;flags_path\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;stats_sinks\u0026#34;: [], \u0026#34;stats_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;stats_flush_interval\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;watchdog\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;tracing\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;runtime\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;layered_runtime\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;admin\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;overload_manager\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;enable_dispatcher_stats\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;header_prefix\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;stats_server_version_override\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;use_tcp_for_dns_lookups\u0026#34;: \u0026#34;...\u0026#34; } 这里我们主要对侦听器(Listeners)、集群(Clusters) 和 管理(Admin)这三个常用的部分来进行说明。其中，(Listeners)、集群(Clusters) 均位于 static_resources 节点下，而 管理(Admin) 则有一个单独的admin节点。\n侦听器(Listeners) 侦听器，顾名思义就是侦听一个或者多个端口：\nstatic_resources: listeners: - address: socket_address: address: 0.0.0.0 port_value: 9090 在这里，它表示的是侦听9090这个端口，这里的listeners是一个数组，所以，你可以同时侦听多个端口。\n过滤器(Filters) 我们知道，单单侦听一个或者多个端口，是无法完成一个HTTP请求的，因为它还不具备处理HTTP请求的能力。\nEnvoy Filters 架构图\r在 Envoy 中，这一工作由一个或者多个过滤器组成的过滤器链(Filter Chains) 来完成：\nstatic_resources: listeners: - address: socket_address: address: 0.0.0.0 port_value: 9090 filter_chains: - filters: - name: envoy.filters.network.http_connection_manager typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager codec_type: AUTO stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: backend domains: - \u0026#34;*\u0026#34; routes: - match: prefix: \u0026#34;/api/w\u0026#34; route: auto_host_rewrite: true prefix_rewrite: /Weather cluster: weatherservice - match: prefix: \u0026#34;/api/c\u0026#34; route: auto_host_rewrite: true prefix_rewrite: /City cluster: cityservice http_filters: - name: envoy.filters.http.router 在这段配置中，Http Connection Manager表示的是位于 L3(网络层)/L4(传输层) 的过滤器，这个过滤器连接的下一个过滤器是envoy.filters.http.router，表示的是 L7(应用层) 的关于路由的过滤器。个人感觉，这和我们通常所说的中间件相当接近。注意到，我们在 L3/L4 这个层级上做了什么事情呢？其实应该是 TCP/IP 层面上请求转发，这里定义的路由规则如下：\n当外部调用者访问/api/w时，请求会被转发到WeatherService。 当外部调用者访问/api/c时，请求会被转发到CityService。 集群(Clusters) 在过滤器部分，我们定义了路由，那么，请求的最终去向是哪里呢？这里 Envoy 将其称之为 集群：:\nstatic_resources: clusters: # City Service - name: cityservice connect_timeout: 0.25s type: STRICT_DNS lb_policy: ROUND_ROBIN 集群本身，其实只是一个名字，并没有实际的意义，真正工作的其实是指向上游服务的终结点，我们可以为每一个集群指定一个负载均衡策略，让它决定选择哪一个终结点来提供服务。\n负载均衡(Load Assignment) 目前，Envoy支持以下负载均衡算法：\nROUND_ROBIN：轮询 LEAST_REQUEST：最少请求 RING_HASH：哈希环 RANDOM：随机 MAGLEV：磁悬浮 CLUSTER_PROVIDED 下面的示例展示了如何为某个集群配置负载均衡：\nstatic_resources: clusters: # Weather Service - name: weatherservice connect_timeout: 0.25s type: STRICT_DNS lb_policy: LEAST_REQUEST load_assignment: cluster_name: weatherservice endpoints: - lb_endpoints: - endpoint: address: socket_address: address: weatherservice1 port_value: 80 - endpoint: address: socket_address: address: weatherservice2 port_value: 80 其中，weatherservice1和weatherservice2是同一个服务的两个容器实例，当我们使用Docker-Compose进行构建的时候，不需要显式地去指定每一个服务的IP地址，只要对应docker-compose.yaml文件中的服务名称即可。\n管理(Admin) 管理(Admin)这块儿相对简单一点，主要用来指定Envoy的的管理接口的端口号、访问日志存储路径等等：\nadmin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 0.0.0.0, port_value: 9091 } 服务编排 好了，在正确配置Envoy以后，我们来考虑如何对这些服务进行编排，在本文的 例子 中，我们有两个后端服务，WeatherService 和 CityService，它们本质上是两个ASP.NET Core应用，我们希望通过Envoy来实现反向代理功能。这样做的好处是，后端服务的架构不会直接暴露给外部使用者。所以，你会注意到，在微服务架构的设计中，网关经常扮演着重要的角色。那么，如何对服务进行编排呢？这里我们使用Docker-Compose来完成这个工作：\nversion: \u0026#39;3\u0026#39; services: envoygateway: build: Envoy/ ports: - \u0026#34;9090:9090\u0026#34; - \u0026#34;9091:9091\u0026#34; volumes: - ./Envoy/envoy.yaml:/etc/envoy/envoy.yaml cityservice: build: CityService/ ports: - \u0026#34;8081:80\u0026#34; environment: ASPNETCORE_URLS: \u0026#34;http://+\u0026#34; ASPNETCORE_ENVIRONMENT: \u0026#34;Development\u0026#34; weatherservice: build: WeatherService/ ports: - \u0026#34;8082:80\u0026#34; environment: ASPNETCORE_URLS: \u0026#34;http://+\u0026#34; ASPNETCORE_ENVIRONMENT: \u0026#34;Development\u0026#34; 可以注意到，这里需要部署3个服务，其中，Envoy负责监听9090端口，即对外的网关。而两个后端服务，WeatherService 和 CityService则被分别部署在8082和8081端口。这里最重要的是envoy.yaml这个配置文件，我们在上一节编写的配置文件会挂在到容器目录：/etc/envoy/envoy.yaml。Envoy将如何使用这个配置文件呢？事实上，这个Dockerfile是这样编写的：\nFROM envoyproxy/envoy-alpine:v1.16-latest COPY ./envoy.yaml /etc/envoy.yaml RUN chmod go+r /etc/envoy.yaml CMD [\u0026#34;/usr/local/bin/envoy\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;/etc/envoy.yaml\u0026#34;, \u0026#34;--service-cluster\u0026#34;, \u0026#34;reverse-proxy\u0026#34;] 除此之外，Envoy通过9091端口提供管理功能，我们可以通过这个端口来获得集群、请求、统计等信息，这一特性我们将会在接下来的文章里用到：\nEnvoy 管理界面功能一览\r这里，想分享一个关于Envoy的小技巧，当我们在指定集群的地址时，可以使用docker-compose.yaml中定义的服务的名称，这会比填入一个固定的IP地址要更优雅一点，理由非常简单，我们不希望每次都来维护这个地址。\n# Weather Service - name: weatherservice connect_timeout: 0.25s type: STRICT_DNS lb_policy: ROUND_ROBIN load_assignment: cluster_name: weatherservice endpoints: - lb_endpoints: - endpoint: address: socket_address: # 建议使用 docker-compose.yaml 文件中对应的服务名称来代替IP地址 address: weatherservice port_value: 80 接下来，如果每一个服务的Dockerfile都编写正确的话，我们就可以通过docker compose up命令启动这一组服务，通过命令行下打印出来的信息，我们可以确认，基于Envoy的网关服务、两个基于ASP.NET Core的后端服务都已经成功运行起来：\n在 Docker-Compose 中成功启动服务\r还记得我们的路由规则是是如何定义的吗？\n当外部调用者访问/api/w时，请求会被转发到WeatherService。 当外部调用者访问/api/c时，请求会被转发到CityService。 实际的情况如何呢？我们不妨来验证一下：\n通过 Envoy 调用 WeatcherService 可以注意到，不管是浏览器返回的结果，还是容器内部输出的日志，都表明请求确实被转发到对应的服务上面去了，这说明我们设计的网关已经生效。至此，我们实现了基于Envoy的反向代理功能，有没有觉得比Nginx要简单一点？重要的是，基于Docker-Compose的服务编排使用起来真的舒适度爆棚，这意味着你会有更多的属于程序员的贤者时间。前段时间热映的电视剧《觉醒时代》，鲁迅先生写完《狂人日记》后那一滴眼泪令人动容。也许，这种如匠人一般反复雕琢、臻于完美的心境是互通的，即使人类的悲欢并不相通，对美和极致的追求竟然出奇的相似。\n本文小结 本文主要介绍了 ASP.NET Core 搭载 Envoy 实现反向代理这一过程。对于 Envoy，有两个重要的定义：第一、Envoy 是一个开源边缘和服务代理，专为原生云应用设计。第二、Envoy 是专为大型现代服务导向架构设计的 L7 代理和通讯总线。相比 Nginx 和 Ocelot ，Envoy 提供了 L7 级别的代理服务，支持 HTTP/2 和 gRPC，无侵入式的Sidecar模式可以提供与应用进程完全隔离的代理服务。接下来，博主对 Envoy 配置文件的结构及主要的配置项进行了说明，对于常见的 API 网关，我们应该重点关注侦听器(Listeners) 和 集群(Clusters)。最终，我们结合Docker-Compose对服务进行了编排，并由此构建出了一个基本的反向代理的方案。本文的源代码已托管至 Github ，大家可以在此基础上做进一步的探索。好了，以上就是这篇博客的的全部内容啦，欢迎大家就本文中提出的方案、代码等进行讨论，如果大家有任何意见或者建议，欢迎在评论区进行留言，谢谢大家！\n参考文档 Envoy中文指南 Envoy官方文档 eShopOnContainers 知多少[12]：Envoy Gateways ",
            "tags": ["Envoy","微服务","网关","反向代理"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "使用 HttpMessageHandler 实现 HttpClient 请求管道自定义",
            "date": "2021-04-28",
            "permalink": "http://localhost:1313/posts/2070070822/",
            "summary": "最近，博主偶然间在 博客园 看到一篇文章：ASP.NET Core 扩展库之 Http 请求模拟，它里面介绍了一种利用 HttpMessageHandler 来实现 Http 请求模拟的方案。在日常工作中，我们总是不可避免地要和第三方的服务或者接口打交道，尤其是当我们需要面对“联调”这样一件事情的时候。通常，我们可以通过类似 YAPI 这样的工具来对尚在开发中的接口进行模拟。可是，因为这种方式会让我们的测试代码依赖于一个外部工具，所以，从严格意义上讲，它其实应该属于“集成测试”的范畴。在接触前端开发的过程中，对于其中的 Mock.js 印象深刻。故而，当看到 .NET 中有类似实现的时候，好奇心驱使我对其中的核心，即 HttpMessageHandler 产生了浓厚的兴趣。平时，我们更多的是使用 Moq 这样的库来模拟某一个对象的行为，而对一个 Http 请求进行模拟，可以说是开天辟地头一遭。带着这些问题出发，就有了今天这篇博客，通过 HttpMessageHandler 实现 HttpClient 请求管道的自定义。\n什么是 HttpMessageHandler？ 相信大家读过我提到的文章以后，都能找到这里面最核心的一个点：HttpMessageHandler。于是，我们今天要面对的第一个问题就是，什么是 HttpMessageHandler？此时，我们需要一张历久弥新的示意图，来自 微软官方。这里，我们重点关注的是 DelegatingHandler，它继承自 HttpMessageHandler。通过这张图，我们能够获得哪些信息呢？\n我认为，主要有以下几点：第一，HttpMessageHandler 处于整个 Http 请求管道的第一梯队，每一个路由匹配的请求都会从这里“进入”和“离开”；第二，HttpMessageHandler 可以是全局配置或者针对某个特定的路由，只要这个路由被匹配到就会执行；第三，HttpMessageHandler 可以直接构造 Http 响应并且返回，跳过剩余的管道流程。不知道大家看到这里会想到什么？坦白讲，我联想到了.NET Core 中的中间件，而唯一不同的地方或许是，中间件是 ASP.NET Core 里的概念，这里则是 ASP.NET Web API 里的概念。尤其是第三点，它对于我们的意义非常重大，因为它，我们才可以做到对一个 Http 请求进行模拟。\nHttpMessageHandler 与 ASP.NET Web API\r而事实上，在 ASP.NET Web API 的设计中，它是由一组 HttpMessageHandler 经过“首尾相连”而成，这种管道式的设计使得框架本身具有很高的扩展性。虽然，作为一个服务端框架，ASP.NET Web API 最主要的作用是就是“处理请求、响应回复”，可具体采用的处理策略会因具体场景的不同而不同。所以，管道式设计的本质，就是让某一个 Handler 只负责某个单一的消息处理功能，在根据具体场景的不同，选择需要的 Handler 并将其串联成一个完整的消息处理通道。而在这里，这个负责单一的消息处理功能的 Handler 其实就是 HttpMessageHandler，因为它不单单可以对请求消息(HttpRequestMessage)进行处理，同时还可以对响应消息(HttpResponseMessage)进行处理。此时，我们就不难理解 HttpMessageHandler 的定义：",
            "content": "最近，博主偶然间在 博客园 看到一篇文章：ASP.NET Core 扩展库之 Http 请求模拟，它里面介绍了一种利用 HttpMessageHandler 来实现 Http 请求模拟的方案。在日常工作中，我们总是不可避免地要和第三方的服务或者接口打交道，尤其是当我们需要面对“联调”这样一件事情的时候。通常，我们可以通过类似 YAPI 这样的工具来对尚在开发中的接口进行模拟。可是，因为这种方式会让我们的测试代码依赖于一个外部工具，所以，从严格意义上讲，它其实应该属于“集成测试”的范畴。在接触前端开发的过程中，对于其中的 Mock.js 印象深刻。故而，当看到 .NET 中有类似实现的时候，好奇心驱使我对其中的核心，即 HttpMessageHandler 产生了浓厚的兴趣。平时，我们更多的是使用 Moq 这样的库来模拟某一个对象的行为，而对一个 Http 请求进行模拟，可以说是开天辟地头一遭。带着这些问题出发，就有了今天这篇博客，通过 HttpMessageHandler 实现 HttpClient 请求管道的自定义。\n什么是 HttpMessageHandler？ 相信大家读过我提到的文章以后，都能找到这里面最核心的一个点：HttpMessageHandler。于是，我们今天要面对的第一个问题就是，什么是 HttpMessageHandler？此时，我们需要一张历久弥新的示意图，来自 微软官方。这里，我们重点关注的是 DelegatingHandler，它继承自 HttpMessageHandler。通过这张图，我们能够获得哪些信息呢？\n我认为，主要有以下几点：第一，HttpMessageHandler 处于整个 Http 请求管道的第一梯队，每一个路由匹配的请求都会从这里“进入”和“离开”；第二，HttpMessageHandler 可以是全局配置或者针对某个特定的路由，只要这个路由被匹配到就会执行；第三，HttpMessageHandler 可以直接构造 Http 响应并且返回，跳过剩余的管道流程。不知道大家看到这里会想到什么？坦白讲，我联想到了.NET Core 中的中间件，而唯一不同的地方或许是，中间件是 ASP.NET Core 里的概念，这里则是 ASP.NET Web API 里的概念。尤其是第三点，它对于我们的意义非常重大，因为它，我们才可以做到对一个 Http 请求进行模拟。\nHttpMessageHandler 与 ASP.NET Web API\r而事实上，在 ASP.NET Web API 的设计中，它是由一组 HttpMessageHandler 经过“首尾相连”而成，这种管道式的设计使得框架本身具有很高的扩展性。虽然，作为一个服务端框架，ASP.NET Web API 最主要的作用是就是“处理请求、响应回复”，可具体采用的处理策略会因具体场景的不同而不同。所以，管道式设计的本质，就是让某一个 Handler 只负责某个单一的消息处理功能，在根据具体场景的不同，选择需要的 Handler 并将其串联成一个完整的消息处理通道。而在这里，这个负责单一的消息处理功能的 Handler 其实就是 HttpMessageHandler，因为它不单单可以对请求消息(HttpRequestMessage)进行处理，同时还可以对响应消息(HttpResponseMessage)进行处理。此时，我们就不难理解 HttpMessageHandler 的定义：\npublic abstract class HttpMessageHandler : IDisposable { protected HttpMessageHandler(); public void Dispose(); protected virtual void Dispose(bool disposing); protected internal virtual HttpResponseMessage Send( HttpRequestMessage request, CancellationToken cancellationToken ); protected internal abstract Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken ); } 也许，你会忍不住问这样一个问题：DelegatingHandler 和 HttpMessageHandler 的区别是什么？ 其实，只要你稍微仔细一点，你就会发现，两者最大的区别是 DelegatingHandler 里新增一个叫做 InnerHandler 的成员，它本身就是一个 HttpMessageHandler。所以，聪明的你又联想到什么呢？我想，或许是一个叫做 RequestDelegate 的委托，还记得我们写中间件是一直都少不了的 Next 吗？不得不说，这里越来越有中间件的味道了。你可以立马想到的一件事情是，除了最后一个 Handler 是 HttpMessageHandler 以外，剩下的前面的所有的 Handler 都是 DelegatingHandler。为什么这样说呢？因为前面的 n-1 个 Handler 都需要串联下一个 Handler，只有第 n 个 Handler可以允许短路，所以，大概就相当于 Use() 和 Run() 的区别？\npublic abstract class DelegatingHandler : HttpMessageHandler { protected DelegatingHandler(); protected DelegatingHandler(HttpMessageHandler innerHandler); // InnerHandler是实现管道式设计的关键 public HttpMessageHandler? InnerHandler { get; set; } protected override void Dispose(bool disposing); protected internal override HttpResponseMessage Send( HttpRequestMessage request, CancellationToken cancellationToken ); protected internal override Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken ); } 所以，此时此刻，你能否为 HttpMessageHandler 下一个清晰的定义呢？我想，或许可以这样理解，一种可以对 请求消息(HttpRequestMessage) 和 响应消息(HttpResponseMessage) 进行处理，同时多个 HttpMessageHandler 可以组成一个完整的消息处理通道的中间件。屏幕前的你又是如何理解的呢？欢迎大家在评论区留言，留下你对于 HttpMessageHandler 的想法或者认识。\n实现自定义请求管道 好了，搞清楚 HttpMessageHandler 是什么以后，我们就可以考虑自定义请求管道的实现啦！让我们从一个最简单的示例开始，假设我们这里定义了两个自定义的 Handler，它们分别是： HandlerA 和 HandlerB，我们应该如何将其应用到具体的 HttpClient上呢？\n// Handler A public class HandlerA : DelegatingHandler { private readonly ILogger\u0026lt;HandlerA\u0026gt; _logger; public HandlerA(ILogger\u0026lt;HandlerA\u0026gt; logger) { _logger = logger; } protected override Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken ) { _logger.LogInformation(\u0026#34;This is Handler A\u0026#34;); return base.SendAsync(request, cancellationToken); } } // Handler B public class HandlerB : DelegatingHandler { private readonly ILogger\u0026lt;HandlerB\u0026gt; _logger; public HandlerB(ILogger\u0026lt;HandlerB\u0026gt; logger) { _logger = logger; } protected override Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken ) { _logger.LogInformation(\u0026#34;This is Handler B\u0026#34;); return base.SendAsync(request, cancellationToken); } } 这里，我们考虑两种场景，依赖注入 和 非依赖注入。对于依赖注入的场景，我们只需要调用AddHttpMessageHandler()方法按顺序注册即可，不需要处理InnerHandler，这里遵循先注册后使用的原则；对于非依赖注入的场景，需要处理InnerHandler，并在构造HttpClient的时候作为参数传入。\n// 依赖注入 var services = new ServiceCollection(); services.AddTransient\u0026lt;HandlerA\u0026gt;(); services.AddTransient\u0026lt;HandlerB\u0026gt;(); services.AddHttpClient(\u0026#34;MyClient\u0026#34;, options =\u0026gt; { options.BaseAddress = new Uri(\u0026#34;https://blog.yuanpei.me/\u0026#34;); }) .AddHttpMessageHandler\u0026lt;HandlerA\u0026gt;() .AddHttpMessageHandler\u0026lt;HandlerB\u0026gt;(); // 非依赖注入 var handler = new HandlerA() { InnerHandler = new HandlerB() }; var client = new HttpClient(handler) 此时，我们就可以得到下面的结果，可以注意到的是，两个Handler的执行顺序与注册顺序一致：\nHandler执行顺序与注册顺序\r好了，热身环节到此结束！下面，我们来开始实战，这里展示的是 HttpMessageHandler 在日志记录、请求重试 和 接口模拟等方面的应用。\n日志记录 对于 Http 请求的日志，我们希望记录请求的Url、Http动词、请求时长等信息，而这一点，在一个大量接入第三方接口的系统或者是以 Http 驱动的微服务架构中，常常是不可或缺的一环，对于我们排查故障、监控服务非常有用。\nprotected override async Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync(HttpRequestMessage request, CancellationToken cancellationToken) { var correlationId = GetCorrelationId(request); using (_logger.BeginScope($\u0026#34;correlationId={correlationId}\u0026#34;)) { var sw = Stopwatch.StartNew(); _logger.LogInformation($\u0026#34;Start Processing HTTP Request {request.Method} {request.RequestUri} [Correlation: {correlationId}]\u0026#34;); var response = base.Send(request, cancellationToken); _logger.LogInformation($\u0026#34;End Processing HTTP Request in {sw.ElapsedMilliseconds}ms {response.StatusCode}, [Correlation: {correlationId}]\u0026#34;); return response; } } // GetCorrelationId private string GetCorrelationId(HttpRequestMessage request) { if (request.Headers.TryGetValues(\u0026#34;X-Correlation-ID\u0026#34;, out var values)) return values.First(); var correlationId = Guid.NewGuid().ToString(); request.Headers.Add(\u0026#34;X-Correlation-ID\u0026#34;, correlationId); return correlationId; } 此时，我们可以得到下面的结果：\nHttpMessageHandler 实现日志记录\r请求重试 我们知道，一个系统中接入的外部因素越多，则整个系统的稳定性越低。而国内的产品通常都喜欢\u0026quot;大而全\u0026quot;的\u0026quot;万物互联\u0026quot;，所以，最实际的问题，其实就是调用一个第三方的接口，如何保证其可靠性。所以，考虑请求的故障恢复就显得非常有意义，为此，我们可以引入Polly，在实现SendAsync()方法的时候，通过Polly中的超时、重试等机制对其做一层包装：\npublic class RetryableHttpMessageHandler : DelegatingHandler { private readonly ILogger\u0026lt;RetryableHttpMessageHandler\u0026gt; _logger; private readonly IAsyncPolicy\u0026lt;HttpResponseMessage\u0026gt; _retryPolicy; public RetryableHttpMessageHandler( ILogger\u0026lt;RetryableHttpMessageHandler\u0026gt; logger ) { _logger = logger; _retryPolicy = Policy\u0026lt;HttpResponseMessage\u0026gt; .Handle\u0026lt;HttpRequestException\u0026gt;() .Or\u0026lt;TimeoutException\u0026gt;() .OrResult(x =\u0026gt; (int)x.StatusCode \u0026gt;= 400) .RetryAsync(3, (ret, index) =\u0026gt; { _logger.LogInformation($\u0026#34;调用接口异常：{ret.Exception?.Message}，状态码：{ret.Result.StatusCode}, 正在进行第{index}次重试\u0026#34;); }); } protected override Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken ) { return _retryPolicy.ExecuteAsync(() =\u0026gt; base.SendAsync(request, cancellationToken)); } } 同样地，我们这里通过HttpClient来请求指定的接口。因为，下面的接口实际上是不存在的。所以，理论上它会返回404这个状态码。而我们的重试策略是，在发生HttpRequestException或者TimeoutException异常以及 Http 响应的状态码大于 400 时，自动触发 3 次重试。\nvar client = _clientFactory.CreateClient(\u0026#34;ApiMock\u0026#34;); var response = await client.GetAsync(\u0026#34;/api/fail\u0026#34;); 此时，我们可以得到下面的结果：\nHttpMessageHandler 实现请求重试\r可以发现，不多不少刚好是 3 次。除了重试以外，Polly还支持类似超时、断路器等等不同的策略，甚至可以将它们组合起来使用，这些都属于Polly的内容，不作为本文的重点内容来讲解，感兴趣的朋友可以查阅这篇文章：.NET 开源项目 Polly 介绍。需要说明的是，微软官方提供的 Microsoft.Extensions.Http.Polly，它在IHttpClientBuilder上添加了一个名为AddPolicyHandler()的扩展方法，这里的例子可以被简化为下面这样，它和我们这里举的例子是完全一致的：\n// 定义重试策略 var retryPolicy = Policy\u0026lt;HttpResponseMessage\u0026gt; .Handle\u0026lt;HttpRequestException\u0026gt;() .Or\u0026lt;TimeoutException\u0026gt;() .OrResult(x =\u0026gt; (int)x.StatusCode \u0026gt;= 400) .RetryAsync(3, (ret, index) =\u0026gt; { Console.WriteLine($\u0026#34;调用接口异常：{ret.Exception?.Message}，状态码：{ret.Result.StatusCode}, 正在进行第{index}次重试\u0026#34;); }); // 注册HttpClient并指定重试策略 services.AddHttpClient(\u0026#34;ApiMock\u0026#34;, options =\u0026gt; { options.BaseAddress = new Uri(\u0026#34;https://blog.yuanpei.me\u0026#34;); }) .AddPolicyHandler(retryPolicy); 接口模拟 在集成第三方接口时，在双方确定好接口以后，接口消费方会有一段时间的“黒写”时期。因为在接口提供方的接口没有正式提供前，接口消费方始终只能通过“模拟”的方式来进行测试。考虑到单元测试对 YAPI 存在耦合，所以，接口模拟同样是一件意义非凡的事情。这里的思路是利用 HttpMessageHandler 的“短路”功能，即构造一个 HttpResponseMessage 并返回。\n首先，我们定义一个MockItem类型，它含有两个委托类型的属性RouteSelector和Executor。其中，前者用来匹配路由，而后者则用来处理接口返回值。\npublic class MockItem { public Func\u0026lt;HttpRequestMessage, bool\u0026gt; RouteSelector { get; set; } public Func\u0026lt;HttpRequestMessage, HttpResponseMessage, Task\u0026gt; Executor { get; set; } } 接下来，我们需要定义相应的Handler，这里是ApiMockHttpMessageHandler：\npublic class ApiMockHttpMessageHandler: DelegatingHandler { private readonly ILogger\u0026lt;ApiMockHttpMessageHandler\u0026gt; _logger; private readonly IEnumerable\u0026lt;MockItem\u0026gt; _routes; public ApiMockHttpMessageHandler( ILogger\u0026lt;ApiMockHttpMessageHandler\u0026gt; logger, IEnumerable\u0026lt;MockItem\u0026gt; routes) { _logger = logger; _routes = routes; } protected override async Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken ) { // 匹配路由并调用其Executor属性 var route = _routes.FirstOrDefault(x =\u0026gt; x.RouteSelector?.Invoke(request)); if (route != null) { var response = new HttpResponseMessage(); await route.Executor?.Invoke(request, response); return response; } return base.Send(request, cancellationToken); } } 我们的思路是，对于所有注入到Ioc容器中的MockItem，检查其路由是否匹配，如果路由匹配，则通过其指定的Executor对HttpResponseMessage进行加工并返回。为了更加方便地在Ioc容器中进行注入，我们为IServiceCollection编写了相应的扩展方法：\npublic static IServiceCollection AddMock\u0026lt;TReturn\u0026gt;( this IServiceCollection services, string url, HttpMethod method, TReturn @return ) { var mockItem = new MockItem(); mockItem.Executor = BuildExecutor\u0026lt;TReturn\u0026gt;(@return); mockItem.RouteSelector = BuildRouteSelector(url, method); return services.AddTransient\u0026lt;MockItem\u0026gt;(sp =\u0026gt; mockItem); } public static IServiceCollection AddMock\u0026lt;TReturn\u0026gt;( this IServiceCollection services, Func\u0026lt;HttpRequestMessage, bool\u0026gt; routeSelector, Func\u0026lt;HttpRequestMessage, HttpResponseMessage, Task\u0026gt; executor ) { var mockItem = new MockItem(); mockItem.Executor = executor; mockItem.RouteSelector = routeSelector; return services.AddTransient\u0026lt;MockItem\u0026gt;(sp =\u0026gt; mockItem); } private static Func\u0026lt;HttpRequestMessage, bool\u0026gt; BuildRouteSelector( string url, HttpMethod method ) { Func\u0026lt;HttpRequestMessage, bool\u0026gt; selector = request =\u0026gt; { if (url == \u0026#34;*\u0026#34;) return true; return url.ToLower() == res.RequestUri.AbsolutePath.ToLower() \u0026amp;\u0026amp; method == res.Method; }; return selector; } private static Func\u0026lt;HttpRequestMessage, HttpResponseMessage, Task\u0026gt; BuildExecutor\u0026lt;TReturn\u0026gt;(TReturn @return) { Func\u0026lt;HttpRequestMessage, HttpResponseMessage, Task\u0026gt; executor = (request, response) =\u0026gt; { response.StatusCode = System.Net.HttpStatusCode.OK; if (@return is HttpStatusCode) response.StatusCode = (HttpStatusCode)Enum.Parse( typeof(HttpStatusCode), @return.ToString() ); else if (@return is Exception) throw @return as Exception; else if (@return is string) response.Content = new StringContent(@return as string); else response.Content = new StringContent(@return == null ? \u0026#34;\u0026#34; : JsonConvert.SerializeObject(@return) ); return Task.CompletedTask; }; return executor; } 此时，我们就可以在单元测试中对接口进行模拟，这样就实现了真正意义上的单元测试：\nvar services = new ServiceCollection(); // 添加 HttpClient并注册ApiMockHttpMessageHandler services.AddHttpClient(\u0026#34;ApiMock\u0026#34;, options =\u0026gt; { options.BaseAddress = new Uri(\u0026#34;https://blog.yuanpei.me\u0026#34;); }) .AddHttpMessageHandler\u0026lt;ApiMockHttpMessageHandler\u0026gt;(); // 添加3个模拟接口 services.AddMock(\u0026#34;/api/status\u0026#34;, HttpMethod.Get, HttpStatusCode.OK); services.AddMock(\u0026#34;/api/query\u0026#34;, HttpMethod.Post, new Exception(\u0026#34;帅哥你谁啊\u0026#34;)); services.AddMock(\u0026#34;/api/order\u0026#34;, HttpMethod.Get, new { OrderId = \u0026#34;OR09874\u0026#34;, CreatedBy = \u0026#34;张三\u0026#34; }); var serviceProvider = services.BuildServiceProvider(); var httpClientFactory = serviceProvider.GetRequiredService\u0026lt;IHttpClientFactory\u0026gt;(); var httpClient = httpClientFactory.CreateClient(\u0026#34;ApiMock\u0026#34;); // 调用/api/order接口 var response = await httpClient.GetAsync(\u0026#34;/api/order\u0026#34;); 下图是模拟接口返回的结果，与我们期望的完全一致：\nHttpMessageHandler 实现接口模拟\r本文小结 古人云：他山之石，可以攻玉。原本被接口模拟(Mock)所吸引的博主，意外地收获了 HttpMessageHandler 这个令人兴奋的知识点。博主认为，它是一种可以对 请求消息(HttpRequestMessage) 和 响应消息(HttpResponseMessage) 进行处理，同时多个 HttpMessageHandler 可以组成一个完整的消息处理通道的中间件。在此基础上，我们实现了诸如日志记录、请求重试、接口模拟等等的扩展性功能。除此以外，它还可以应用到 Http认证头处理 、客户端负载均衡等方面。\n其实，从 ASP.NET、OWIN、Nancy、ASP.NET Core 这样一路走过来，你会发现，管道的概念一直都存在，无非是以不同的形式存在着，譬如 ASP.NET Core 里的中间件，其实是替代了曾经的 HttpHandler 和 HttpModule，就像时间一直都在那里，不快不慢，觉得物是人非、喜新厌旧的多半还是我们。对我而言，写到这里，最大的感慨或许是，曾经试图实现的类似 Servlet 的 Http Server ，现在想起来还是太年轻、太朴实了，可年轻或者朴实，难道不好吗？好了，以上就是这篇博客的全部内容了，如果你觉得这篇博客对你有所帮助或者启发，希望你可以毫不吝啬地给个一键三连。如果你对这篇博客里的内容有意见或者建议，欢迎你评论区留下你的足迹和声音，谢谢大家！\n",
            "tags": ["HttpClient","Mock","管道","扩展"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "ABP vNext 对接 Ant Design Vue 实现分页查询",
            "date": "2021-04-07",
            "permalink": "http://localhost:1313/posts/3670340170/",
            "summary": "在 上一篇 博客中，博主和大家分享了如何在 EF Core 中实现多租户架构。在这一过程中，博主主要参考了 ABP vNext 这个框架。从上个月开始，我个人发起了一个项目，基于 ABP vNext 和 Ant Design Vue 来实现一个通用的后台管理系统，希望以此来推进 DDD 和 Vue 的学习，努力打通前端与后端的“任督二脉”。因此，接下来的这段时间内，我写作的主题将会围绕 ABP vNext 和 Ant Design Vue。而在今天的这篇博客中，我们来说说 ABP vNext 对接 Ant Design Vue 实现分页查询的问题，希望能让大家在面对类似问题时有所帮助。我不打算写一个系列教程，更多的是从我个人的关注点出发，如果大家有更多想要交流的话题，欢迎大家通过评论或者邮件来留言，谢谢大家！\nABP vNext中的分页查询 OK，当大家接触过 ABP vNext 以后，就会了解到这样一件事情，即，ABP vNext 中默认提供的分页查询接口，在大多数情况下，通常都会是下面这样的风格。这里以角色查询的接口为例，它对应的请求地址是：/api/identity/roles?SkipCount=0\u0026amp;MaxResultCount=10。此时，我们可以注意到，返回的数据结构中含有totalCount和items两个属性。其中，totalCount表示记录的总数目，items表示当前页对应的记录。\n{ \u0026#34;totalCount\u0026#34;: 2, \u0026#34;items\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Admin\u0026#34;, \u0026#34;isDefault\u0026#34;: false, \u0026#34;isStatic\u0026#34;: true, \u0026#34;isPublic\u0026#34;: true, \u0026#34;concurrencyStamp\u0026#34;: \u0026#34;cb53f2d7-159e-452d-9d9c-021629b500e0\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;39fb19e8-fb34-dfbd-3c70-181f604fd5ff\u0026#34;, \u0026#34;extraProperties\u0026#34;: {} }, { \u0026#34;name\u0026#34;: \u0026#34;Manager\u0026#34;, \u0026#34;isDefault\u0026#34;: false, \u0026#34;isStatic\u0026#34;: false, \u0026#34;isPublic\u0026#34;: false, \u0026#34;concurrencyStamp\u0026#34;: \u0026#34;145ec550-7fe7-4c80-85e3-f317a168e6b6\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;39fb6216-2803-20c6-7211-76f8fe38b90e\u0026#34;, \u0026#34;extraProperties\u0026#34;: {} } ] } 事实上，ABP vNext 中自带的分页查询，主要是通过SkipCount和MaxResultCount两个参数来实现。假设MaxResultCount，即分页大小为m，则第n页对应的SkipCount应该为(n-1) * m。如果大家对于LINQ非常熟悉的话，应该可以自然而然地联想到Skip()和Take()两个方法，这是一个非常自然的联想，因为 ABP vNext 就是这样实现分页查询的。这里以博主的“数据字典”分页查询接口为例：",
            "content": "在 上一篇 博客中，博主和大家分享了如何在 EF Core 中实现多租户架构。在这一过程中，博主主要参考了 ABP vNext 这个框架。从上个月开始，我个人发起了一个项目，基于 ABP vNext 和 Ant Design Vue 来实现一个通用的后台管理系统，希望以此来推进 DDD 和 Vue 的学习，努力打通前端与后端的“任督二脉”。因此，接下来的这段时间内，我写作的主题将会围绕 ABP vNext 和 Ant Design Vue。而在今天的这篇博客中，我们来说说 ABP vNext 对接 Ant Design Vue 实现分页查询的问题，希望能让大家在面对类似问题时有所帮助。我不打算写一个系列教程，更多的是从我个人的关注点出发，如果大家有更多想要交流的话题，欢迎大家通过评论或者邮件来留言，谢谢大家！\nABP vNext中的分页查询 OK，当大家接触过 ABP vNext 以后，就会了解到这样一件事情，即，ABP vNext 中默认提供的分页查询接口，在大多数情况下，通常都会是下面这样的风格。这里以角色查询的接口为例，它对应的请求地址是：/api/identity/roles?SkipCount=0\u0026amp;MaxResultCount=10。此时，我们可以注意到，返回的数据结构中含有totalCount和items两个属性。其中，totalCount表示记录的总数目，items表示当前页对应的记录。\n{ \u0026#34;totalCount\u0026#34;: 2, \u0026#34;items\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Admin\u0026#34;, \u0026#34;isDefault\u0026#34;: false, \u0026#34;isStatic\u0026#34;: true, \u0026#34;isPublic\u0026#34;: true, \u0026#34;concurrencyStamp\u0026#34;: \u0026#34;cb53f2d7-159e-452d-9d9c-021629b500e0\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;39fb19e8-fb34-dfbd-3c70-181f604fd5ff\u0026#34;, \u0026#34;extraProperties\u0026#34;: {} }, { \u0026#34;name\u0026#34;: \u0026#34;Manager\u0026#34;, \u0026#34;isDefault\u0026#34;: false, \u0026#34;isStatic\u0026#34;: false, \u0026#34;isPublic\u0026#34;: false, \u0026#34;concurrencyStamp\u0026#34;: \u0026#34;145ec550-7fe7-4c80-85e3-f317a168e6b6\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;39fb6216-2803-20c6-7211-76f8fe38b90e\u0026#34;, \u0026#34;extraProperties\u0026#34;: {} } ] } 事实上，ABP vNext 中自带的分页查询，主要是通过SkipCount和MaxResultCount两个参数来实现。假设MaxResultCount，即分页大小为m，则第n页对应的SkipCount应该为(n-1) * m。如果大家对于LINQ非常熟悉的话，应该可以自然而然地联想到Skip()和Take()两个方法，这是一个非常自然的联想，因为 ABP vNext 就是这样实现分页查询的。这里以博主的“数据字典”分页查询接口为例：\npublic async Task\u0026lt;PagedResultDto\u0026lt;DataDictionaryQueryDto\u0026gt;\u0026gt; GetCategories( GetDataDictionaryRequestInput input ) { var totalCount = (await _dataDictRepository.GetQueryableAsync()) .WhereIf(!string.IsNullOrEmpty(input.Name), x =\u0026gt; x.Name.Contains(input.Name) || x.Name == input.Name) .WhereIf(!string.IsNullOrEmpty(input.Description), x =\u0026gt; x.Description.Contains(input.Description) || x.Description == input.Description) .Count(); var items = (await _dataDictRepository.GetQueryableAsync()) .WhereIf(!string.IsNullOrEmpty(input.Name), x =\u0026gt; x.Name.Contains(input.Name) || x.Name == input.Name) .WhereIf(!string.IsNullOrEmpty(input.Description), x =\u0026gt; x.Description.Contains(input.Description) || x.Description == input.Description) .Skip(input.SkipCount) .Take(input.MaxResultCount) .ToList(); return new PagedResultDto\u0026lt;DataDictionaryQueryDto\u0026gt;() { TotalCount = totalCount, Items = ObjectMapper.Map\u0026lt;List\u0026lt;DataDictionary\u0026gt;, List\u0026lt;DataDictionaryQueryDto\u0026gt;\u0026gt;(items) }; } 可以注意到，在 ABP vNext 中我们只需要构造好TotalCount和Items这两个属性即可。\nSTable组件中的分页查询 接下来，在 Ant Design Vue 的 Pro 版本中，我们使用STable组件来展示列表类的数据，关于这个组件的使用方法，大家可以参考 官方文档。按照最小化可行产品(MVP)的理念，一个最简单的STable组件的使用，如下面所示：\n\u0026lt;template\u0026gt; \u0026lt;s-table ref=\u0026#34;table\u0026#34; size=\u0026#34;default\u0026#34; :rowKey=\u0026#34;(record) =\u0026gt; record.data.id\u0026#34; :columns=\u0026#34;columns\u0026#34; :data=\u0026#34;loadData\u0026#34; :rowSelection=\u0026#34;{ selectedRowKeys: selectedRowKeys, onChange: onSelectChange }\u0026#34; \u0026gt; \u0026lt;/s-table\u0026gt; \u0026lt;/template\u0026gt; 对于这个组件而言，其中最重要的地方当属data属性，它接受一个函数，该函数的返回值为Promise对象，并且有一个参数：\n\u0026lt;script\u0026gt; import STable from \u0026#39;@/components\u0026#39; export default { components: { STable }, data() { return { // 表格列名 columns: [], // 查询条件 queryParam: { }, // 加载数据方法，必须为 Promise 对象 loadData: parameter =\u0026gt; { return getRoles(Object.assign({}, this.queryParam, parameter)) .then(res =\u0026gt; { return res.result }) }, // ... selectedRowKeys: [], selectedRows: [] } } } \u0026lt;/script\u0026gt; 也许，你会好奇这个parameter到底是个什么东西？可如果我们将其打印出来，就会发现它其实是分页查询相关的参数：Object { pageNo: 1, pageSize: 10 }，而更进一步，如果深入到这个组件的源代码中，我们会注意到组件内部有一个loadData()方法：\nloadData (pagination, filters, sorter) { this.localLoading = true const parameter = Object.assign({ pageNo: (pagination \u0026amp;\u0026amp; pagination.current) || this.showPagination \u0026amp;\u0026amp; this.localPagination.current || this.pageNum, pageSize: (pagination \u0026amp;\u0026amp; pagination.pageSize) || this.showPagination \u0026amp;\u0026amp; this.localPagination.pageSize || this.pageSize }, (sorter \u0026amp;\u0026amp; sorter.field \u0026amp;\u0026amp; { sortField: sorter.field }) || {}, (sorter \u0026amp;\u0026amp; sorter.order \u0026amp;\u0026amp; { sortOrder: sorter.order }) || {}, { ...filters } ) const result = this.data(parameter) // 对接自己的通用数据接口需要修改下方代码中的 r.pageNo, r.totalCount, r.data 可以注意到，在STable组件内部，它会将分页、排序和过滤三种不同类型的参数，通过Object.assign()方法聚合到一个对象上，这个对象实际上就是我们刚刚打印出来的parameter。为什么这样说呢？因为它接下来就要调用data属性指向的方法啦！还记得这个data是什么吗？不错，它是一个函数，既然是一个函数，当然可以直接调用。到这里，我们可以获得第一个信息，即，ABP vNext 中的表格组件STable，本身封装了分页查询相关的参数，只要将这些参数传递给后端就可以实现分页查询。\n实现参数转换层 既然，这个参数和 ABP vNext 需要的参数不同，为了不修改已有的接口，我们考虑在这中间加一层转换。为此，我们定义下面的函数：\n// 默认列表查询条件 const baseListQuery = { page: 1, limit: 20 } // 查询条件转化 export function transformAbpListQuery (query) { query.filter = query.filter === \u0026#39;\u0026#39; ? undefined : query.filter if (window.isNaN(query.pageSize)) { query.pageSize = baseListQuery.limit } if (window.isNaN(query.pageNo)) { query.pageNo = baseListQuery.page } const abpListQuery = { maxResultCount: query.pageSize, skipCount: (query.pageNo - 1) * query.pageSize, sorting: \u0026#39;\u0026#39;, filter: \u0026#39;\u0026#39;, ...query } if (typeof (query.sortField) !== \u0026#39;undefined\u0026#39; \u0026amp;\u0026amp; query.sortField !== null) { abpListQuery.sorting = query.sortOrder === \u0026#39;ascend\u0026#39; ? query.sortField : `${query.sortField} Desc` } return abpListQuery } 代码非常简单，通过transformAbpListQuery函数，我们就实现了从STable到ABP vNext的参数转换。需要说明的是，这里的排序使用到了 System.Linq.Dynamic.Core 这个库，它可以实现IQueryable级别的、基于字符串的动态表达式构建功能，使用方法如下：\nvar resultSingle = queryable.OrderBy\u0026lt;User\u0026gt;(\u0026#34;NumberProperty\u0026#34;); var resultSingleDescending = queryable.OrderBy\u0026lt;User\u0026gt;(\u0026#34;NumberProperty DESC\u0026#34;); var resultMultiple = queryable.OrderBy\u0026lt;User\u0026gt;(\u0026#34;NumberProperty, StringProperty\u0026#34;); 所以，当它为降序排序时，我们在排序字段的后面添加DESC即可。关于filter参数，我准备做一套通用性更强的方案，所以，这里就暂时留空啦！接下来，如果大家足够细心的话，会发现STable组件对返回值同样有一定的要求，它要求返回值中至少含有pageNo、totalCount, data三个属性，而这，是我们获得的第二个信息：\n// 对接自己的通用数据接口需要修改下方代码中的 r.pageNo, r.totalCount, r.data // eslint-disable-next-line if ((typeof result === \u0026#39;object\u0026#39; || typeof result === \u0026#39;function\u0026#39;) \u0026amp;\u0026amp; typeof result.then === \u0026#39;function\u0026#39;) { result.then(r =\u0026gt; { this.localPagination = this.showPagination \u0026amp;\u0026amp; Object.assign({}, this.localPagination, { current: r.pageNo, // 返回结果中的当前分页数 total: r.totalCount, // 返回结果中的总记录数 showSizeChanger: this.showSizeChanger, pageSize: (pagination \u0026amp;\u0026amp; pagination.pageSize) || this.localPagination.pageSize }) || false this.localDataSource = r.data // 返回结果中的数组数据 this.localLoading = false }) } 依样画葫芦，我们继续编写转换层的代码，返回值格式参考了 Ant Design Vue 中Mock接口的返回值格式：\n// 查询结果转化 export function transformAbpQueryResult (data, message, code = 0, headers = {}) { const responseBody = { } responseBody.result = data if (message !== undefined \u0026amp;\u0026amp; message !== null) { responseBody.message = message } if (code !== undefined \u0026amp;\u0026amp; code !== 0) { responseBody.code = code responseBody._status = code } if (headers !== null \u0026amp;\u0026amp; typeof headers === \u0026#39;object\u0026#39; \u0026amp;\u0026amp; Object.keys(headers).length \u0026gt; 0) { responseBody._headers = headers } responseBody.timestamp = new Date().getTime() return responseBody } // 分页查询结果转化 export function buildPagingQueryResult (queryParam, data) { for (const item of data.items) { // Ant Design Vue 中要求每行数据中必须存在字段：key item.key = item.id } const pagedResult = { pageSize: queryParam.pageSize, pageNo: queryParam.pageNo, totalCount: data.totalCount, totalPage: data.totalCount / queryParam.pageSize, data: data.items } return transformAbpQueryResult(pagedResult) } 对于分页结果而言，我们会将分页大小、当前页数、总页数、总记录数及其对应的数据，统一封装到一个对象中，然后再将其传递给返回值中的result属性。\n最终对接效果 好了，写了这么多，我们到底实现了一个什么效果呢？对于一开始的角色查询接口，我们可以这样封装到前端的服务层：\nexport function getRoles (query) { const queryParam = transformAbpListQuery(query) return axios({ url: AppConsts.resourceService.baseUrl + \u0026#39;/api/identity/roles\u0026#39;, method: \u0026#39;get\u0026#39;, params: queryParam }).then(data =\u0026gt; { return buildPagingQueryResult(queryParam, data) }) } 接下来，我们只需要实现loadData()方法即可：\nimport { getRoles, updateRole, createRole, deleteRole } from \u0026#39;@/api/recipe/abp.role\u0026#39; loadData: parameter =\u0026gt; { return getRoles(Object.assign({}, parameter, this.queryParam)) .then(res =\u0026gt; { return res.result }) }, 此时，我们可以注意到，ABP vNext 与 Ant Design Vue 完美地集成在一起，并且参数的转换完全符合我们的预期。这样做的好处显而易见，我们只需要遵循 ABP vNext 的规范进行开发即可，考虑到 ABP vNext 可以直接将ApplicationService暴露为 API 接口，这意味着我们写完了接口，就可以立即开始前后端的联调工作，这无疑可以加快我们的研发效率！\nABP vNext 与 Ant Design Vue 完成整合\r好了，以上就是这篇博客的全部内容啦！这篇博客要实现的功能其实并不复杂，唯一的难点是，需要在前端和后端两个技术栈上频繁地切换上下文，这可能就是全栈开发者面临的最大挑战，因为技术世界浩如烟海，而一个人的精力终究有限，古人云：朝闻道，夕死可矣，人生百年，吾道不孤，还是请你继续努力哦！\n",
            "tags": ["ABP","Vue","分页","前端"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "低代码，想说爱你不容易",
            "date": "2021-02-15",
            "permalink": "http://localhost:1313/posts/2637069146/",
            "summary": "一直想写篇文章，聊一聊“低代码”这个话题。一方面，“低代码”这个概念确实非常火，其热度丝毫不亚于曾经的“中台”。有人说，2021 年是属于“云原生”的时代，看起来我们每一年都在被技术的“娱乐圈”抛弃，明明连 Kubernetes 都还没有入门呢？人们已然在欢呼雀跃般地声称要抛弃 Docker 。这个世界有时就是如此地魔幻，明明我们生活在一个拥有大量基础设施的时代，我们不必再像前辈们“刀耕火种”一般地去开发软件，可我们的生存空间为什么就越来越狭窄了呢？拼多多事件过去没有多久，腾讯的阳光普照奖再次让“打工魂”觉醒，也许果真像大鱼海棠里设定的一样，人的记忆只有 7 秒。而另一方面，我想结合我最近开发“工作流”的感受，来吐槽下这个看起来美好的“低代码”。也许，对企业而言，引入“低代码”的确能减少研发成本，可博主并不认为，它会降低业务本身的复杂性，如果所有声称“低代码”或者“无代码”的项目，最终依然需要研发人员来作为收场。对此，我想说，对不起，这不是我想要的“低代码”。\n低代码发展现状 或许，一个人成熟的标志就是，在面对一个未知的事物的时候，决不会不由分说地一通吐槽，就像一个人在职场上，你不能永远都只是学会抱怨，相对于抱怨，人们更希望听到的是解决方案。所以，一个人的成长，本质上就是不断学会为自己、为别人找解决方案的过程，前者是为了认识自我，而后者是为了交换资源。所以，在听我吐槽“低代码”前，不妨先一起来看看低代码的发展现状。\n低代码产品发展现状\r国外趋势 有人认为，“低代码”的兴起源于钉钉的低代码应用 易搭 的落地。诚然，巨头企业的每一个动向都引领着整个行业的风潮，可低代码这个概念最早要追溯到 1980 年。彼时，IBM 的快速应用程序开发工具(RAD)被冠以新的名字——低代码，这是低代码这个概念首次面向大众，此后的 40 年里，国外诞生了诸如 Outsystem 、Mendix 、 Zoho Creator 等等的产品，整体发展相对缓慢。直到 2015 年以后，AWS、Google、Microsoft 和 Oracle 等巨头开始入局低代码领域。2018 年，西门子更是宣布以 6 亿欧元收购低代码应用开发领域的领导者 Mendix 、快速应用开发的低代码平台 Outsystem 获得 3.6 亿美金的投资，低代码平台市场开始火爆起来，我们所熟悉的 Power Platform，其实就是微软的低代码开发平台，低代码领域通常都需要大量的积累和研发，需要有 10 到 20 年左右的技术沉淀。\n国内风云 国内的低代码领域，相比国外发展起步较晚，可依然涌现出像牛刀、APICloud、iVX、搭搭云、氚云、简道云、云表、宜搭云等等产品。从整体上而言，这类这类产品基本上都提供了可视化搭建环境，都声称无需编码即可完成业务系统的搭建。其实，从一名程序员的初心出发，我们所做的一切努力都是为了以后不写代码。经常有人问，怎么样可以做到零缺陷、零 Bug ，其实不写代码就好啦！我们并不担心低代码让我们失业，相反地，如果低代码可以消化掉 30% 的垃圾项目，那么，我们将会有更多的时间去做些有意义的事情，而不是在一个“劣币驱逐良币”的市场里，靠着 996 来争个你死我活。而从低代码的商业价值角度来看，Salesforce、Appian、Joget 这三家公司均已上市，Mendix 和 Outsystem 更是估值 10 亿美元以上的独角兽公司，这正是巨头们入局低代码的原因所在。\n低代码领域，目前关注的重点主要集中在：表单生成和处理、工作流生成和管理、办公协作、人力资源、客户关系、ERP 等企业应用上，就如同 SAP 、金蝶、 SCM 等企业软件一样，每一个软件都曾声称能帮助企业解决某一类问题，低代码领域同样遵循“二八原则”，即 80% 的场景，通过定义的方法论、方式、工具集能够实现；而剩下的 20% 的场景或许实现不了，需要使用者通过扩展的方式来自行解决。譬如，针对大多数企业都存在的 CRUD 的需求，通过在线的 Excel 表格来实现基于表的业务驱动。例如 SeaTable 就是这类主打协同工作的产品；针对大多数企业都存在的审批类的需求，则可以通过可视化的工作流设计系统来完成。例如 葡萄城 的 SpreadJS 和 活字格 ，同样可以视为低代码平台，甚至早期的 .",
            "content": "一直想写篇文章，聊一聊“低代码”这个话题。一方面，“低代码”这个概念确实非常火，其热度丝毫不亚于曾经的“中台”。有人说，2021 年是属于“云原生”的时代，看起来我们每一年都在被技术的“娱乐圈”抛弃，明明连 Kubernetes 都还没有入门呢？人们已然在欢呼雀跃般地声称要抛弃 Docker 。这个世界有时就是如此地魔幻，明明我们生活在一个拥有大量基础设施的时代，我们不必再像前辈们“刀耕火种”一般地去开发软件，可我们的生存空间为什么就越来越狭窄了呢？拼多多事件过去没有多久，腾讯的阳光普照奖再次让“打工魂”觉醒，也许果真像大鱼海棠里设定的一样，人的记忆只有 7 秒。而另一方面，我想结合我最近开发“工作流”的感受，来吐槽下这个看起来美好的“低代码”。也许，对企业而言，引入“低代码”的确能减少研发成本，可博主并不认为，它会降低业务本身的复杂性，如果所有声称“低代码”或者“无代码”的项目，最终依然需要研发人员来作为收场。对此，我想说，对不起，这不是我想要的“低代码”。\n低代码发展现状 或许，一个人成熟的标志就是，在面对一个未知的事物的时候，决不会不由分说地一通吐槽，就像一个人在职场上，你不能永远都只是学会抱怨，相对于抱怨，人们更希望听到的是解决方案。所以，一个人的成长，本质上就是不断学会为自己、为别人找解决方案的过程，前者是为了认识自我，而后者是为了交换资源。所以，在听我吐槽“低代码”前，不妨先一起来看看低代码的发展现状。\n低代码产品发展现状\r国外趋势 有人认为，“低代码”的兴起源于钉钉的低代码应用 易搭 的落地。诚然，巨头企业的每一个动向都引领着整个行业的风潮，可低代码这个概念最早要追溯到 1980 年。彼时，IBM 的快速应用程序开发工具(RAD)被冠以新的名字——低代码，这是低代码这个概念首次面向大众，此后的 40 年里，国外诞生了诸如 Outsystem 、Mendix 、 Zoho Creator 等等的产品，整体发展相对缓慢。直到 2015 年以后，AWS、Google、Microsoft 和 Oracle 等巨头开始入局低代码领域。2018 年，西门子更是宣布以 6 亿欧元收购低代码应用开发领域的领导者 Mendix 、快速应用开发的低代码平台 Outsystem 获得 3.6 亿美金的投资，低代码平台市场开始火爆起来，我们所熟悉的 Power Platform，其实就是微软的低代码开发平台，低代码领域通常都需要大量的积累和研发，需要有 10 到 20 年左右的技术沉淀。\n国内风云 国内的低代码领域，相比国外发展起步较晚，可依然涌现出像牛刀、APICloud、iVX、搭搭云、氚云、简道云、云表、宜搭云等等产品。从整体上而言，这类这类产品基本上都提供了可视化搭建环境，都声称无需编码即可完成业务系统的搭建。其实，从一名程序员的初心出发，我们所做的一切努力都是为了以后不写代码。经常有人问，怎么样可以做到零缺陷、零 Bug ，其实不写代码就好啦！我们并不担心低代码让我们失业，相反地，如果低代码可以消化掉 30% 的垃圾项目，那么，我们将会有更多的时间去做些有意义的事情，而不是在一个“劣币驱逐良币”的市场里，靠着 996 来争个你死我活。而从低代码的商业价值角度来看，Salesforce、Appian、Joget 这三家公司均已上市，Mendix 和 Outsystem 更是估值 10 亿美元以上的独角兽公司，这正是巨头们入局低代码的原因所在。\n低代码领域，目前关注的重点主要集中在：表单生成和处理、工作流生成和管理、办公协作、人力资源、客户关系、ERP 等企业应用上，就如同 SAP 、金蝶、 SCM 等企业软件一样，每一个软件都曾声称能帮助企业解决某一类问题，低代码领域同样遵循“二八原则”，即 80% 的场景，通过定义的方法论、方式、工具集能够实现；而剩下的 20% 的场景或许实现不了，需要使用者通过扩展的方式来自行解决。譬如，针对大多数企业都存在的 CRUD 的需求，通过在线的 Excel 表格来实现基于表的业务驱动。例如 SeaTable 就是这类主打协同工作的产品；针对大多数企业都存在的审批类的需求，则可以通过可视化的工作流设计系统来完成。例如 葡萄城 的 SpreadJS 和 活字格 ，同样可以视为低代码平台，甚至早期的 .NET 开发者被人“黑”只会拖控件，这难道不是广义上的低代码吗？\n低代码产品形态 搞清楚整个低代码的发展现状以后，那么，整个低代码领域主要的产品形态有哪些呢？了解其主要的产品形态，对于我们形成低代码的直观印象非常有帮助。在我看来，主要分为四类：\n表单生成类：以 宜搭云 和 JNPF 为代表，主张通过可视化的设计器来完成页面布局、编排、设计，即所谓的“所见即所得”，类似的还有 iVX。 工作流生成类：以 Mendix 和 Outsystems 为代表，提供组件式的服务，通过编排工作流来实现特定的业务，即通过流程图的方式来实现业务逻辑部分，不同的节点代表不同的功能，不同的线条代表不同的分支。 协同工作类：以 SeaTable 为代表，基于表的业务驱动开发平台，可以以不同的维度管理数据、对数据可视化、共享协作等等，同时具备自动化规则、脚本运行等能力。 服务聚合类：以 APICloud 为代表，基于 API 聚合的组件市场工具，通过流程管理工具，可以管理整个应用的开发周期，从产品、设计开始，到研发测试和运营。 所以，整体而言，低代码产品的核心是表单引擎 和 流程引擎(BPM)，外围支撑是BI 引擎、*协同工作、服务聚合等等，目前，市面上主流的低代码产品，表单引擎和流程引擎(BPM)基本是标配，所以，严格地说起来，上面的分类并不严谨，因为基本上都是混合式的产品形态。下面是部分低代码产品的截图：\n某“低代码”二维码应用\r某“低代码”人力资源管理系统\r某“低代码”可视化搭建系统\r低代码研发痛点 相信大家都知道了，接下来的内容是本文真正的重点。为什么要这样说呢？这主要和博主自身的工作有关系，简单来说，公司需要一个想象中的可视化设计器，业务人员只需要通过拖拽就可以完成业务逻辑的编排，而开发人员则需要负责对外输出组件供业务人员使用。这听起来特别像我们刚刚讨论的第二种产品形态对不对？听起来非常美好对不对？我承认这个想法真的符合潮流、非常的“低代码”。所以，我们前期采用了微软的 Windows Workflow Foundation 框架，使用以后的效果大概是下面这个样子：\nWindows Workflow Foundation 设计器\r多人协作不便 那么，我们在这个过程中到底遇到了哪些问题呢？首先，这种可视化编辑的场景，遇到的第一个问题就是多人协作，如果你使用过腾讯文档、钉钉文档这类在线文档类产品，你应该能领悟到我说的这个点。微软的这个框架是采用XMAL这种格式来储存数据的，虽然理论上可以通过 Git 实现多人协作，实际维护起来表示非常地麻烦，所以，我们最终由单人去维护这些工作流。那么，更广义上的低代码又该如何解决这个问题呢？流程图这种东西，就是一种看起来非常清晰，改起来非常麻烦的东西，就像一条锁链一样，你要不停地断开和接上。\n孱弱的表达能力 其次，是流程图这种表现方式的“表达”问题，就像你如果需要在SQL里表示循环要用到游标一样，这类工作流都无法表达程序三个结构中的循环，更不用说表达力孱弱的表达式啦，所以，这就造成一个非常尴尬的问题，你在流程图里写不了太复杂的表达式，一旦业务人员写不出来，就需要开发人员去写辅助性质的代码，类似正则、字符串插值、字符串处理、格式化等等的函数或者 API 非常缺乏。当然，我最无法忍受的，就是组件与组件间传值的方式，你除了返回 JSON 和写表再没有其它方式，更何况这个 JSON 返回给某个组件了，人家还未必能直接解析直接使用呢？因为编辑器无法绑定这种复杂的数据结构。\n混乱的变量和参数 接下来，我最想吐槽的是，关于全局变量和参数的问题，在流程图中你经常需要各个分支的标志位(Flag)或者是临时变量，然后你就看到了那种“变量满天飞”的混乱局面，简直像极了你刚开始写的代码，你需要顺着每个线条，逐个点开每个组件的属性面板，查看它都使用了哪些参数或者变量，至此，你终于明白了它的数据是如何流动的。从前，乡愁是成千上万行的代码；现在，乡愁是剪不断理还乱的“蜘蛛网”。多年前，我对虚幻引擎(Unreal)的蓝图功能有多么憧憬；多年后，我对这种基于流程引擎的低代码就有多排斥。尤其是，当我需要复用某一段逻辑的时候，我只能小心翼翼地选中节点和线条，然后再拷贝过去。\n动态计算/事件顺序/黑盒子 最后，我参考了一位被 Power Apps 所折磨的朋友的意见，除了上面提到的这些问题， 属性面板或者公式无法使用动态计算的值，类似Vue 里面的计算属性，从实际使用的体验来看，这类以流程引擎和表单引擎为主要卖点的低代码工具，其实都会存在这样的问题，而面对这种问题，一般只能通过trick的手段来解决。同样地，Power Apps 事件顺序的不确定问题，因为低代码实际上是框架提供了某种机制，可以帮你完成某个事情，所以，低代码内部对于使用者来说，完全就是一个黑盒子，譬如 Power Apps 在无网络的环境下使用会卡顿，调试起来非常不便等等。\n本文小结 坦白来讲，这篇博客实在没什么“技术含量”，无非是按照一个月前的计划在整理内容。我对“低代码”持一种中立的态度，作为程序员，我是希望有这样的技术来简化流程，可以让研发人员从枯燥的“增删改查”中解放出来，留出时间去做更多有意义、有价值的事情。当我了解了低代码和零代码的差异以后，我突然明白，我需要的其实是零代码，因为我希望那帮业务人员能自己搞定，这样就不用再来烦我，可经历这段时间的“低代码”，我清醒地认识到，这个想法根本不现实。一来业务人员并不像他们想象的那样，除了不会写代码以外无所不能；二来业务的复杂性满足守恒定律，它永远不会消失，只会从一种形式变成另一种形式。也许，低代码真的能帮企业省不少钱；也许，企业最喜欢做的事情，就是花点小钱招人外包做这种事情。但我依然想告诫开发者们，不要去追逐这些看起来美好的东西，对企业来说，它今天使用 A 技术，明天使用 B 技术，完全无关紧要。可对于个人而言，这个选择显得非常重要。看一看曾经的 SAP 咨询顾问就知道了，如果有一天 SAP 都倒闭了，你掌握着这些只有在 SAP 上能发挥作用的技术有什么用呢？对技术人员来说，学习通用型的知识和技能，永远比把鸡蛋放在一个篮子里要更保险。\n",
            "tags": ["低代码","行业","编程","感悟"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "一道 HashSet 面试题引发的蝴蝶效应",
            "date": "2020-10-20",
            "permalink": "http://localhost:1313/posts/3411909634/",
            "summary": "没错，我又借着“面试题”的名头来搞事情了，今天要说的是 HashSet ，而这确实是一个实际面试中遇到的问题。当时的场景大概是这样的，面试官在了解了你的知识广度以后，决心来考察一番你的基本功底，抛出了一个看起来平平无奇的问题：说一说你平时工作中都用到了哪些数据结构。你心想，这还不简单，Array、ArrayList、List、Dictionary、HashSet、Stack、Queue\u0026hellip;等等各种集合类简直如数家珍，甚至你还能说出这些数据结构间的优劣以及各自使用的场景。可没想到，面试官话锋一转，直接来一句，“你能说说 HashSet 去重的原理吗”，好家伙，你这简直不按套路出牌啊\u0026hellip;本着每次面试都有一点收获的初心，于是就有了今天这篇博客，不同的是，顺着这个思路继续深挖下去，博主又发现了几个平时关注不到的技术盲点，所以，博主称之为：一道 HashSet 面试题引发的蝴蝶效应。\nHashSet 源代码解读 OK，首先，我们来回答第一个问题，即：HashSet 去重的原理是什么？。为此，博主翻阅了 HashSet 的 源代码。首先，我们会注意到 HashSet 的构造函数，它需要一个类型为IEqualityComparer\u0026lt;T\u0026gt;的参数。从这个命名上我们就可以知道，这是一个用于相等性比较的接口，我们初步推测，HashSet 去重应该和这个接口有关：\npublic HashSet() : this(EqualityComparer\u0026lt;T\u0026gt;.Default) { } public HashSet(int capacity) : this(capacity, EqualityComparer\u0026lt;T\u0026gt;.Default) { } public HashSet(IEqualityComparer\u0026lt;T\u0026gt; comparer) { } public HashSet(IEnumerable\u0026lt;T\u0026gt; collection) : this(collection, EqualityComparer\u0026lt;T\u0026gt;.Default) { } public HashSet(IEnumerable\u0026lt;T\u0026gt; collection, IEqualityComparer\u0026lt;T\u0026gt; comparer) : this(comparer) { } 我们都知道 HashSet 可以去重，比如，我们向 HashSet 添加多个相同的元素，实际上 HashSet 中最终只会有一个元素。所以，我们自然而然地想到，看看 HashSet 中的 Add() 方法呗，或许能从这里看出一点端倪。HashSet 中一共有两个 Add() 方法，它们内部都调用了 AddIfNotPresent() 方法：\nvoid ICollection\u0026lt;T\u0026gt;.",
            "content": "没错，我又借着“面试题”的名头来搞事情了，今天要说的是 HashSet ，而这确实是一个实际面试中遇到的问题。当时的场景大概是这样的，面试官在了解了你的知识广度以后，决心来考察一番你的基本功底，抛出了一个看起来平平无奇的问题：说一说你平时工作中都用到了哪些数据结构。你心想，这还不简单，Array、ArrayList、List、Dictionary、HashSet、Stack、Queue\u0026hellip;等等各种集合类简直如数家珍，甚至你还能说出这些数据结构间的优劣以及各自使用的场景。可没想到，面试官话锋一转，直接来一句，“你能说说 HashSet 去重的原理吗”，好家伙，你这简直不按套路出牌啊\u0026hellip;本着每次面试都有一点收获的初心，于是就有了今天这篇博客，不同的是，顺着这个思路继续深挖下去，博主又发现了几个平时关注不到的技术盲点，所以，博主称之为：一道 HashSet 面试题引发的蝴蝶效应。\nHashSet 源代码解读 OK，首先，我们来回答第一个问题，即：HashSet 去重的原理是什么？。为此，博主翻阅了 HashSet 的 源代码。首先，我们会注意到 HashSet 的构造函数，它需要一个类型为IEqualityComparer\u0026lt;T\u0026gt;的参数。从这个命名上我们就可以知道，这是一个用于相等性比较的接口，我们初步推测，HashSet 去重应该和这个接口有关：\npublic HashSet() : this(EqualityComparer\u0026lt;T\u0026gt;.Default) { } public HashSet(int capacity) : this(capacity, EqualityComparer\u0026lt;T\u0026gt;.Default) { } public HashSet(IEqualityComparer\u0026lt;T\u0026gt; comparer) { } public HashSet(IEnumerable\u0026lt;T\u0026gt; collection) : this(collection, EqualityComparer\u0026lt;T\u0026gt;.Default) { } public HashSet(IEnumerable\u0026lt;T\u0026gt; collection, IEqualityComparer\u0026lt;T\u0026gt; comparer) : this(comparer) { } 我们都知道 HashSet 可以去重，比如，我们向 HashSet 添加多个相同的元素，实际上 HashSet 中最终只会有一个元素。所以，我们自然而然地想到，看看 HashSet 中的 Add() 方法呗，或许能从这里看出一点端倪。HashSet 中一共有两个 Add() 方法，它们内部都调用了 AddIfNotPresent() 方法：\nvoid ICollection\u0026lt;T\u0026gt;.Add(T item) { AddIfNotPresent(item); } public bool Add(T item) { return AddIfNotPresent(item); } 继续循着蛛丝马迹一路 F12 ，我们来看看这个方法的具体实现：\nprivate bool AddIfNotPresent(T value) { if (m_buckets == null) { Initialize(0); } int hashCode = InternalGetHashCode(value); int bucket = hashCode % m_buckets.Length; #if FEATURE_RANDOMIZED_STRING_HASHING \u0026amp;\u0026amp; !FEATURE_NETCORE int collisionCount = 0; #endif for (int i = m_buckets[hashCode % m_buckets.Length] - 1; i \u0026gt;= 0; i = m_slots[i].next) { if (m_slots[i].hashCode == hashCode \u0026amp;\u0026amp; m_comparer.Equals(m_slots[i].value, value)) { return false; } #if FEATURE_RANDOMIZED_STRING_HASHING \u0026amp;\u0026amp; !FEATURE_NETCORE collisionCount++; #endif } int index; if (m_freeList \u0026gt;= 0) { index = m_freeList; m_freeList = m_slots[index].next; } else { if (m_lastIndex == m_slots.Length) { IncreaseCapacity(); // this will change during resize bucket = hashCode % m_buckets.Length; } index = m_lastIndex; m_lastIndex++; } m_slots[index].hashCode = hashCode; m_slots[index].value = value; m_slots[index].next = m_buckets[bucket] - 1; m_buckets[bucket] = index + 1; m_count++; m_version++; #if FEATURE_RANDOMIZED_STRING_HASHING \u0026amp;\u0026amp; !FEATURE_NETCORE if (collisionCount \u0026gt; HashHelpers.HashCollisionThreshold \u0026amp;\u0026amp; HashHelpers.IsWellKnownEqualityComparer(m_comparer)) { m_comparer = (IEqualityComparer\u0026lt;T\u0026gt;) HashHelpers.GetRandomizedEqualityComparer(m_comparer); SetCapacity(m_buckets.Length, true); } #endif // FEATURE_RANDOMIZED_STRING_HASHING return true; } 可以注意到，在这段代码中，首先，会通过 InternalGetHashCode() 方法计算一个 HashCode。其中，Lower31BitMask 是一个常量 0x7FFFFFFF ：\nprivate int InternalGetHashCode(T item) { if (item == null) { return 0; } return m_comparer.GetHashCode(item) \u0026amp; Lower31BitMask; } 接下来，在 HashSet 内部使用了Slot 这个结构来存储元素，该结构设计上类似于链表，每一个 Slot 中都记录对应元素的值、HashCode 以及下一个元素的索引。所以，只需要对它做一次遍历，如果对应元素的 HashCode 和 值 都相等，则认为该元素在 HashSet中已经存在了。此时，AddIfNotPresent() 方法会返回 false。这就是 HashSet 去重的原理啦。在比较元素的值是否相等的时候，我们前面提到的 IEqualityComparer\u0026lt;T\u0026gt; 终于登场，它提供的 Equals() 方法恰好可以比较两个元素是否相等：\ninternal struct Slot { internal int hashCode; // Lower 31 bits of hash code, -1 if unused internal int next; // Index of next entry, -1 if last internal T value; } public interface IEqualityComparer\u0026lt;in T\u0026gt; { bool Equals(T x, T y); int GetHashCode(T obj); } 再接下来，如果对应元素的 HashCode 或 值 都不相等，则认为该元素在 HashSet 中不存在。此时，需要考虑 HashSet 的容量是否足以放得下这个新元素。在容量不满足的情况下，就需要对 HashSet 进行扩容。值得一提的是，这里是使用质数进行扩容的：\nprivate void IncreaseCapacity() { Debug.Assert(m_buckets != null, \u0026#34;IncreaseCapacity called on a set with no elements\u0026#34;); int newSize = HashHelpers.ExpandPrime(m_count); if (newSize \u0026lt;= m_count) { throw new ArgumentException(SR.GetString(SR.Arg_HSCapacityOverflow)); } // Able to increase capacity; copy elements to larger array and rehash SetCapacity(newSize, false); } public static int ExpandPrime(int oldSize) { int newSize = 2 * oldSize; // Allow the hashtables to grow to maximum possible size (~2G elements) before encoutering capacity overflow. // Note that this check works even when _items.Length overflowed thanks to the (uint) cast if ((uint)newSize \u0026gt; MaxPrimeArrayLength \u0026amp;\u0026amp; MaxPrimeArrayLength \u0026gt; oldSize) { Contract.Assert( MaxPrimeArrayLength == GetPrime(MaxPrimeArrayLength), \u0026#34;Invalid MaxPrimeArrayLength\u0026#34;); return MaxPrimeArrayLength; } return GetPrime(newSize); } IEqualityComparer接口 OK，现在我们知道了，HashSet 之所以可以去重，一个重要的原因是 IEqualityComparer\u0026lt;T\u0026gt; 。而回到这个接口本身呢，它只有 Equals() 和 GetHashCode()，这其实非常符合我们的认知，因为这两个方法在对象相等的场景中十分常见，有一个准则是：如果重写了 Equals() 方法，那么，应该同时去重写 GetHashCode() 方法，即，两者在表达相等这个含义时应该具有一致性。这里可能会有一点疑问，那就是，我们平时使用 HashSet 的时候，完全不需要指定 IEqualityComparer\u0026lt;T\u0026gt; ，它一样可以正常工作啊？没错，这是因为微软提供了一个默认的实现：EqualityComparer\u0026lt;T\u0026gt;.Default。我们同样来看看它的实现：\nprivate static EqualityComparer\u0026lt;T\u0026gt; CreateComparer() { Contract.Ensures(Contract.Result\u0026lt;EqualityComparer\u0026lt;T\u0026gt;\u0026gt;() != null); RuntimeType t = (RuntimeType)typeof(T); // Specialize type byte for performance reasons if (t == typeof(byte)) { return (EqualityComparer\u0026lt;T\u0026gt;)(object)(new ByteEqualityComparer()); } // If T implements IEquatable\u0026lt;T\u0026gt; return a GenericEqualityComparer\u0026lt;T\u0026gt; if (typeof(IEquatable\u0026lt;T\u0026gt;).IsAssignableFrom(t)) { return (EqualityComparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(GenericEqualityComparer\u0026lt;int\u0026gt;), t); } // If T is a Nullable\u0026lt;U\u0026gt; where U implements IEquatable\u0026lt;U\u0026gt; return a NullableEqualityComparer\u0026lt;U\u0026gt; if (t.IsGenericType \u0026amp;\u0026amp; t.GetGenericTypeDefinition() == typeof(Nullable\u0026lt;\u0026gt;)) { RuntimeType u = (RuntimeType)t.GetGenericArguments()[0]; if (typeof(IEquatable\u0026lt;\u0026gt;).MakeGenericType(u).IsAssignableFrom(u)) { return (EqualityComparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(NullableEqualityComparer\u0026lt;int\u0026gt;), u); } } // See the METHOD__JIT_HELPERS__UNSAFE_ENUM_CAST and METHOD__JIT_HELPERS__UNSAFE_ENUM_CAST_LONG cases in getILIntrinsicImplementation if (t.IsEnum) { TypeCode underlyingTypeCode = Type.GetTypeCode(Enum.GetUnderlyingType(t)); // Depending on the enum type, we need to special case the comparers so that we avoid boxing // Note: We have different comparers for Short and SByte because for those types we need to make sure we call GetHashCode on the actual underlying type as the // implementation of GetHashCode is more complex than for the other types. switch (underlyingTypeCode) { case TypeCode.Int16: // short return (EqualityComparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(ShortEnumEqualityComparer\u0026lt;short\u0026gt;), t); case TypeCode.SByte: return (EqualityComparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(SByteEnumEqualityComparer\u0026lt;sbyte\u0026gt;), t); case TypeCode.Int32: case TypeCode.UInt32: case TypeCode.Byte: case TypeCode.UInt16: //ushort return (EqualityComparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(EnumEqualityComparer\u0026lt;int\u0026gt;), t); case TypeCode.Int64: case TypeCode.UInt64: return (EqualityComparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(LongEnumEqualityComparer\u0026lt;long\u0026gt;), t); } } // Otherwise return an ObjectEqualityComparer\u0026lt;T\u0026gt; return new ObjectEqualityComparer\u0026lt;T\u0026gt;(); } 在这里，EqualityComparer\u0026lt;T\u0026gt; 类是一个抽象类，实现了 IEqualityComparer\u0026lt;T\u0026gt; 接口。简单来说，对于简单类型如整型、字节型等，微软实现了相应的 IEqualityComparer\u0026lt;T\u0026gt; 接口；而对于复杂的类型，微软提供了 ObjectEqualityComparer\u0026lt;T\u0026gt; 这一实现：\ninternal class ObjectEqualityComparer\u0026lt;T\u0026gt;: EqualityComparer\u0026lt;T\u0026gt; { [Pure] public override bool Equals(T x, T y) { if (x != null) { if (y != null) return x.Equals(y); return false; } if (y != null) return false; return true; } [Pure] public override int GetHashCode(T obj) { if (obj == null) return 0; return obj.GetHashCode(); } } 所以，现在又回到我们刚刚聊起的话题，为什么说一个类型的 Equals() 和 GetHashCode() 方法非常重要呢？因为如果我们不能正确地实现这两个方法，微软实现的这个 ObjectEqualityComparer\u0026lt;T\u0026gt; 就会出现问题，导致 HashSet 在判断元素是否存在时出现问题，所以，这是一系列的连锁反应。有人可能会问，博主你说的这个好夸张耶，像我就从来没有重写过这两个方法。OK，现在来回答我的一个问题，如果你定义了一个类型 Foo ，并尝试用它作为一个字典中的 Key ，那么，你觉得这个字典应该怎么判断这个 Key 是否存在呢？我觉得这是一个好问题，因为它引发了我们在 .NET 知识体系中的蝴蝶效应。\n排序与去重是亲家 排序与去重，在我看来是亲家关系，因为两者都需要“比较”。所以，下面我想从 .NET 中选取一部分接口来阐述我的观点，以及当我们有了 LINQ 以后，是否就应该抛弃它们。可能这些接口大家平时都用不到多少，但我还是想花点时间来梳理这些知识盲点，因为我发现，与其为整个行业 35 岁的的职业生涯而焦虑，倒不如重新捡起这个行业的初心，好好地学一学数据结构、算法和数学。整个行业的火热，容易让每一个人都陷入一种“我非常厉害”的错觉，我写博客的时候，在心里想了这样一句话：战士上战场，整天就知道 CRUD，连 HashSet 都不知道，早晚是个死。用王布斯的口吻说出来，会不会有一种紧迫感呢？\nIEquatable接口 IEquatable\u0026lt;T\u0026gt; 接口在微软官方文档中的定义是，定义值类型或类实现的通用方法，以创建用于确定实例相等性的类型特定方法。我承认，这不是一个特别好的定义，不过，我们可以换个角度来审视这个接口存在的意义。虽然 Object 这个基类提供了 Equals() 方法，但是这个方法只能接受一个 object 类型的参数， 所以，它本身会面临类型安全性缺失和装箱两个问题。为了解决这个问题，就必须要定义一个新的 Equals() 方法，确保它可以接收和当前类型一致的参数，所以就需要这样一个接口，你可以理解为它是 Equals() 方法的泛型版本，而众所周知 C# 是一门不支持多继承的语言，所以，这里只能以接口的形式提供出来。这里说一下我的结论，IEquatable\u0026lt;T\u0026gt; 接口对值类型更有用一点，相反，对引用类型就没有那么有用，因为它没有考虑到协变的问题，对引用类型的继承相对无力。下面是一个简单的例子：\n//定义类型Foo，实现IEquatable\u0026lt;Foo\u0026gt;接口 public class Foo : IEquatable\u0026lt;Foo\u0026gt; { public decimal Value { get; set; } public decimal Weight { get; set; } public override bool Equals(object other) { return Equals(other as Foo); } public bool Equals(Foo other) { if (other == null) return false; return (this.Value == other.Value \u0026amp;\u0026amp; this.Weight == other.Weight); } } //平平无奇的代码 var foo1 = new Foo() { Value = 10, Weight = 1.0M }; var foo2 = new Foo() { Value = 10, Weight = 1.0M }; Assert.AreEqual(true, foo1.Equals(foo2)); ICompareable/ICompareable接口 ICompareable 和 ICompareable\u0026lt;T\u0026gt;是是同一个接口的非泛型与泛型版本，都需要实现 CompareTo() 方法。可能大家会觉得这几个接口都差不多啊，实际上，大家细心观察就能发现它们的区别，“相等”这一类的接口的返回值是布尔型，关注的是两个对象是否相等；而“比较”这一类的接口的返回值是整数型，关注的是哪个大哪个小。我们继续以 Foo 这个类型为例，分别实现IComparerable 和 IComparerable\u0026lt;T\u0026gt;两个接口：\n//继续实现IComparable, IComparable\u0026lt;Foo\u0026gt;接口 public class Foo : IEquatable\u0026lt;Foo\u0026gt;, IComparable, IComparable\u0026lt;Foo\u0026gt; { public decimal Value { get; set; } public decimal Weight { get; set; } public override bool Equals(object other) { return Equals(other as Foo); } public bool Equals(Foo other) { if (other == null) return false; return (this.Value == other.Value \u0026amp;\u0026amp; this.Weight == other.Weight); } public int CompareTo(object obj) { var other = obj as Foo; return CompareTo(other); } public int CompareTo([AllowNull] Foo other) { if (other == null) return 1; return (int)((Value * Weight) - (other.Value * other.Weight)); } } //平平无奇的代码 var foo1 = new Foo() { Value = 10, Weight = 1.0M }; var foo2 = new Foo() { Value = 20, Weight = 1.0M }; Assert.IsTrue(foo2.CompareTo(foo1) \u0026gt; 0); IComparer接口 对于排序来说，理论上有ICompareable 和 ICompareable\u0026lt;T\u0026gt;这两个接口就可以了，为什么还要再定义一组接口呢？其实，我们结合生活中的场景就能想明白，不管是判断两个对象是否相等，还是对两个对象进行排序，这些条件都属于“变量”。ICompareable 和 ICompareable\u0026lt;T\u0026gt;这两个接口设计上的确没什么问题，但这都是一锤子买卖，一旦实现了对应的接口，就意味着如何比较两个对象的逻辑是确定好了的。可生活常识告诉我们，同一组信息不同的人考虑的维度是不一样的，譬如学生的成绩，可以按照某一个科目的成绩来排序，还可以按照各个科目的总成绩甚至是平均分来排序。对于上面的类型 Foo，我们不妨考虑按照 Value 和 Weight 分别进行排序，此时可以这样写：\n//按Value排序 public class FooValueComparer : IComparer\u0026lt;Foo\u0026gt; { public int Compare([AllowNull] Foo x, [AllowNull] Foo y) { if (x == null \u0026amp;\u0026amp; y == null) return 0; if (x != null \u0026amp;\u0026amp; y == null) return 1; if (x == null \u0026amp;\u0026amp; y != null) return -1; return (int)(x.Value - y.Value); } } //按Weight排序 public class FooWeightComparer : IComparer\u0026lt;Foo\u0026gt; { public int Compare([AllowNull] Foo x, [AllowNull] Foo y) { if (x == null \u0026amp;\u0026amp; y == null) return 0; if (x != null \u0026amp;\u0026amp; y == null) return 1; if (x == null \u0026amp;\u0026amp; y != null) return -1; return (int)(x.Weight - y.Weight); } } //平平无奇的代码 var list= new List\u0026lt;Foo\u0026gt;{ new Foo() { Value = 10, Weight = 2.0M }, new Foo() { Value = 10, Weight = 1.0M } }; //使用默认的排序器 list.Sort(); //按Value进行排序 list.Sort(new FooValueComparer()); list.OrderBy(x =\u0026gt; x.Value); //按Weight进行排序 list.Sort(new FooWeightComparer()); list.OrderBy(x =\u0026gt; x.Weight); 在这里有一个点是，在不指定排序器的时候，微软帮我们提供了一个默认的排序器。而这个默认排序器会遵循这样的策略。如果类型 T 实现了 IComparable\u0026lt;T\u0026gt; 接口，则返回 GenericComparer\u0026lt;int\u0026gt; 实例；如果类型 T 实现是一个可空类型 Nullable\u0026lt;U\u0026gt; 并且类型 U 实现了 IComparable\u0026lt;T\u0026gt; 接口，则返回 NullableComparer\u0026lt;int\u0026gt; 实例；否则返回 ObjectComparer\u0026lt;T\u0026gt; 实例。\nprivate static Comparer\u0026lt;T\u0026gt; CreateComparer() { RuntimeType t = (RuntimeType)typeof(T); // If T implements IComparable\u0026lt;T\u0026gt; return a GenericComparer\u0026lt;T\u0026gt; #if FEATURE_LEGACYNETCF // Pre-Apollo Windows Phone call the overload that sorts the keys, not values this achieves the same result if (CompatibilitySwitches.IsAppEarlierThanWindowsPhone8) { if (t.ImplementInterface(typeof(IComparable\u0026lt;T\u0026gt;))) { return (Comparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(GenericComparer\u0026lt;int\u0026gt;), t); } } else #endif if (typeof(IComparable\u0026lt;T\u0026gt;).IsAssignableFrom(t)) { return (Comparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(GenericComparer\u0026lt;int\u0026gt;), t); } // If T is a Nullable\u0026lt;U\u0026gt; where U implements IComparable\u0026lt;U\u0026gt; return a NullableComparer\u0026lt;U\u0026gt; if (t.IsGenericType \u0026amp;\u0026amp; t.GetGenericTypeDefinition() == typeof(Nullable\u0026lt;\u0026gt;)) { RuntimeType u = (RuntimeType)t.GetGenericArguments()[0]; if (typeof(IComparable\u0026lt;\u0026gt;).MakeGenericType(u).IsAssignableFrom(u)) { return (Comparer\u0026lt;T\u0026gt;)RuntimeTypeHandle.CreateInstanceForAnotherGenericParameter((RuntimeType)typeof(NullableComparer\u0026lt;int\u0026gt;), u); } } // Otherwise return an ObjectComparer\u0026lt;T\u0026gt; return new ObjectComparer\u0026lt;T\u0026gt;(); } 更有意思的是，GenericComparer\u0026lt;T\u0026gt; 就是利用 IComparable\u0026lt;T\u0026gt; 的 CompareTo() 方法来说实现的：\ninternal class GenericComparer\u0026lt;T\u0026gt; : Comparer\u0026lt;T\u0026gt; where T: IComparable\u0026lt;T\u0026gt; { public override int Compare(T x, T y) { if (x != null) { if (y != null) return x.CompareTo(y); return 1; } if (y != null) return -1; return 0; } // Equals method for the comparer itself. public override bool Equals(Object obj){ GenericComparer\u0026lt;T\u0026gt; comparer = obj as GenericComparer\u0026lt;T\u0026gt;; return comparer != null; } public override int GetHashCode() { return this.GetType().Name.GetHashCode(); } } 在我们有了 LINQ 以后，通过 OrderBy 和 OrderByDescending 就可以进行排序，如果这个排序字段是一个简单类型，比如字符型、整型、日期型，这些简单类型微软都已经实现了相应的“排序”逻辑，而如果这个排序字段是一个复杂类型，比如一个自定义的类或者结构，此时，为了让这些方法能够“适应”这些复杂类型，最好的还是去实现 IComparer\u0026lt;T\u0026gt; 或者 ICompareable\u0026lt;T\u0026gt; 接口，然后传递给这两个排序方法。类似地，还有 Distinct 这个方法，它接收一个 IEqualityComparer\u0026lt;T\u0026gt; 类型的参数，所以，当你对一个列表进行去重(Distinct)操作时，千万不要想当然地人为它会按照你的期望去去重，如果结果不符合你的期望，很大原因是你没有给它提供一个合适的IEqualityComparer\u0026lt;T\u0026gt; 。所以，你看，我们绕了一大圈，从 HashSet 说到 IEqualityComparer\u0026lt;T\u0026gt;，又从排序说到去重，最终又回到了起点，这是多么有趣的一件事情。而去重(Distinct)这件事情，其实涉及到Dictionary 和 HashSet 两个数据结构，通过结构来推演性质，又通过性质来扫清盲点，这可能是这段时间刷 LeetCode 最大的一个收获吧！\n本文小结 面试中偶然遇到的 HashSet 问题，让我发现自己的知识体系中存在着盲点。通过解读 HashSet 源代码，我们认识到 HashSet 可以去重的一个重要原因是IEqualityComparer\u0026lt;T\u0026gt; 接口，它决定了两个对象的实例在什么情况下可以被判定为相等。而这个接口，不单单在 HashSet 出现，在 Dictionary 中同样会出现，甚至在我们最熟悉不过的去重(Distinct)中还会出现，所以，通过 HashSet 这一个点上的疑问，我搞清楚了很多相关联的内容，这不是蝴蝶效应又是什么呢？而与去重(Distinct)相关联的则是排序，在此基础上，对 IEquatable\u0026lt;T\u0026gt; 接口、ICompareable/ICompareable\u0026lt;T\u0026gt; 接口、IComparer\u0026lt;T\u0026gt; 接口等知识盲点进行梳理。总而言之，排序需要关注的是 ICompareable/ICompareable\u0026lt;T\u0026gt; 接口、IComparer\u0026lt;T\u0026gt; 接口，去重需要关注的是 IEqualityComparer\u0026lt;T\u0026gt; 接口。好了，今天的这只蝴蝶就飞到这里，欢迎大家在博客中留言，谢谢大家！\n",
            "tags": ["HashSet","编程","面试","源码"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "当姜子牙遇见朱一旦",
            "date": "2020-10-18",
            "permalink": "http://localhost:1313/posts/1085014581/",
            "summary": "当导演张策宣布，不再为朱一旦系列担任编剧和配音时，我终于意识到，“十佳员工”不再是一个梗，而是一个活生生的人。也许，身为老板的“朱一旦”，永远都没有读懂这些黑色幽默背后的含义。而显然，站在普通人对立面的资本家们，终究不会因此而洗心革面，代表劳苦大众向这个时代发声。不管是后浪还是非浪，资本家们不会选择和钱过不去，所以，即使有像鲁迅一般针砭时弊的张策，可在一个“屁股决定脑袋”的世界里，“十佳员工”突然就变成一个不再好笑的词汇，因为，这个人可以是你，可以是我，可以是我们中的任何一个。在新冠疫情肆虐的时候，『一块劳力士的回家之路』让我们感受到了现实的魔幻，可此时此刻，我们终于知道，“艺术来源于生活，而往往高于生活”，果然，如有雷同，是不胜荣幸的了。可能是因为我此刻在经历着同样的事情，所以，难免感同身受地想到 C 座 802 里这群真实存在着的人们。\n我有一位为公司奉献 11 年青春的同事，可当他离开这家公司时，并没有我想象的中那样充满不舍，大概“鸟尽弓藏”、“大地茫茫真干净”这些句子，从古至今就是这样子的吧！马老师说，“996 是一种福报”，而此前的一位马老师则说，“资本家生来就是剥削劳动者的一切剩余价值”。历史像个任人打扮的小姑娘，你方唱罢我登场，文过饰非，到底谁又讲得清对错？有人说，一个人开始成熟，就是从学习这几千年来的厚黑学、阴谋论开始，的确啊，连封神都开始变成一场阴谋，不同的是，这次的因果都落在原始天尊身上，每一个人都渴望像姜子牙一样，断天梯、破枷锁，似乎一定要执著于什么东西，这样的人生会显得更真实一点。可每个人自以为最完美的安排，终究无法让每一个人信服啊，正如“朱一旦”们喜欢“非洲安排”，马小策与张策，说到底不过是一种代号而已，当这种“安排”无法调和的时候，人和神仙一样，都会暴走，都会变身，唯一不同的是，人是要吃饭的，而神仙们早已学会辟谷。\n所以，在“救一个人还是救苍生”这个问题上，其实谁都没有错，我特别喜欢李诞在『奇葩说』中表达的一个观点，“以自私却不伤害别人的方式活着，才能维持世界的运转。而正是那些为了宏图伟业不计后果的牺牲‘小猫’的人，频频地让我们的世界陷入「大火」”。朱一旦不想再做“任人摆布”的老板，张策不想再做“默默无名”的幕后英雄，马老师早已看破这一切，“钱没给到位”，“心委屈了”，身在其位时榨干身体的“996”，人走茶凉时送瘟神般“高效”，一冷一热，果然是“环球同此凉热”呢。在全球变暖的趋势下，如果我们以自私却不伤害别人的方式活着，虽然活得有一点清冷、没有人情味，但这样是不是会更安心一点，骨子里与生俱来就带着“竞争”的基因的我们，是不是一定要学会“狠”、学会“不择手段”、学会“伤害”。我二十多岁的时候，想努力去照顾好一个人，而等到我快要三十岁的时候，我终于能勉强照顾好自己，这简直是一种幸运。\n这种感慨在某个场景下会更加明显，譬如一个人去看电影的时候，虽然我很喜欢和邻座的小朋友说话，可对方父母一句友善的“叔叔”，终于还是让两个人产生了距离。譬如找工作面试的时候，发觉三十上下的“哥哥姐姐”们，都开始面对“总监”级别职位时的恍惚感。也许，我们这一代人真的已经老了吧，而那个人早已离开你很久很久，我无意去对立资本家与劳动者间积怨久矣的矛盾，更无意去揣测封神台下蛰伏已久的阴谋。回想以前，乐无异在『古剑奇谭 2』中说出一句，“众生虽苦，还请诸恶莫作”，当时大概只是觉得这句话酷到不行，倘若议论公平，C 座 802 诸如三濑子、马小玲、马小浩等等角色，每一个都带着无数的梗，没有他们就没有整个朱一旦宇宙，当人们为张策惋惜的时候，是不是就选择性地遗忘了他们呢？朱一旦不会成为劳苦大众的代言人，而且任何人都不会，因为一切的流量到最后都是生意。\n我在 B 站关注过一位阿婆主，起初，他在厂里打工，下班后的“入味儿”是他主要的拍摄内容。后来，因为疫情的原因，他开始学别人拍做菜的视频。再后来，发现他变成了一位外卖小哥。世人皆苦，家家有本难念的经，可我们除了祝福以外，又能做一点什么呢？成人世界里，利益、立场、观点……，该有的一切都有，唯独没有对错，希望一个组织有一致的步伐、一致的声音，可偏偏人是一根会思考的芦苇，我知道，当一个人在某一种身份下，他必须要去推动一种文化形成，可如果这些声音连他自己都不信，这种文化的底蕴应该不会特别丰富，很容易成为政治博弈的牺牲品。我从前天真地以为，在互联网这样一个相对开放的环境里，不会存在政治这种产物。而出于对这种东西的逃避，我没有选择成为三线小城市里的一个公务员，实际上我尝试过，结果证明我真的不适合。可后来我发现我错了，只要有人的地方就会存在政治，无论是公司还是社区，每天都有人宣扬这样或者那样的“文化”，这个时候，我希望我们每一个人都去用心甄别这些概念，因为作为人的自觉，他只会说对自己有利的话，正如择偶标准是最毫无标准可言的标准一样，王垠说编程世界里充满宗派，就是最好的证明。\n所以，我不大愿意去统一什么东西，充满多样性、充满个性的世界，才是一个正常的世界，以结果论的观点而言，只要能送大家都目的地，是飞机还是高铁还是火车，真的重要吗？如果非要去统一什么，我希望是“语言”或者“领域语言”，因为，我们的沟通，因为存在太多的翻译而逐渐失真、甚至被曲解，我们一般把这样的沟通称之为扯皮，就像土味情话虽然美妙动听，但它携带了大量无用的信息。所以，即使冒着成为“钢铁直男”的危险，我依然想成为一个表达清晰的人。有人说，姜子牙就不能和原始天尊好好商量一下吗？非要自断天梯逼得鸿钧老祖出手吗？人类啊，归根到底，只愿意相信自己相信的，只愿意看见自己看见的，这种意念在成年后往往更加强烈，有多少遗憾就是得不到有效沟通造成的呢？九尾狐自觉被原始天尊欺骗、过河拆桥，而原始天尊认为“非我族类，其心必异”，都是选择性地相信了自己愿意去相信的东西。有人说，姜子牙有强迫症，为什么会任由师尊披头散发？因为不是每个人都能像约翰·纳什那样，在最亲密的人面前直抒胸臆，人类就是这么奇怪，和陌生人玩什么真心话大冒险，在亲人面前反而含蓄、羞怯起来，可能是因为某种特殊的磁力限制了声道发声吧，科学与玄学往往就是这么切换自如。\n思绪就像一个无底黑洞，姜子牙与朱一旦，两个八竿子不十竿子都打不着的人，就这么神奇地在我脑海里，完成了一次对话。如果思维存在奇点，将会坍陷于何处，苏格拉有没有底不重要，马老师们谁说得对同样不重要，甚至你看我这满纸荒唐言依然不重要，它仅仅表明我此时此刻在思考，我是一个活生生的人，所谓“我思故我在”，无非给枯燥的人生多一点无用的点缀罢了，你说朱一旦都不枯燥了，我们却还停留在这里，你说，还有比这个更枯燥的事情吗？申公豹形神俱灭，从头开始修行，居然连基因都发生了突变，大概，在这世间，没有什么可以永恒。",
            "content": "当导演张策宣布，不再为朱一旦系列担任编剧和配音时，我终于意识到，“十佳员工”不再是一个梗，而是一个活生生的人。也许，身为老板的“朱一旦”，永远都没有读懂这些黑色幽默背后的含义。而显然，站在普通人对立面的资本家们，终究不会因此而洗心革面，代表劳苦大众向这个时代发声。不管是后浪还是非浪，资本家们不会选择和钱过不去，所以，即使有像鲁迅一般针砭时弊的张策，可在一个“屁股决定脑袋”的世界里，“十佳员工”突然就变成一个不再好笑的词汇，因为，这个人可以是你，可以是我，可以是我们中的任何一个。在新冠疫情肆虐的时候，『一块劳力士的回家之路』让我们感受到了现实的魔幻，可此时此刻，我们终于知道，“艺术来源于生活，而往往高于生活”，果然，如有雷同，是不胜荣幸的了。可能是因为我此刻在经历着同样的事情，所以，难免感同身受地想到 C 座 802 里这群真实存在着的人们。\n我有一位为公司奉献 11 年青春的同事，可当他离开这家公司时，并没有我想象的中那样充满不舍，大概“鸟尽弓藏”、“大地茫茫真干净”这些句子，从古至今就是这样子的吧！马老师说，“996 是一种福报”，而此前的一位马老师则说，“资本家生来就是剥削劳动者的一切剩余价值”。历史像个任人打扮的小姑娘，你方唱罢我登场，文过饰非，到底谁又讲得清对错？有人说，一个人开始成熟，就是从学习这几千年来的厚黑学、阴谋论开始，的确啊，连封神都开始变成一场阴谋，不同的是，这次的因果都落在原始天尊身上，每一个人都渴望像姜子牙一样，断天梯、破枷锁，似乎一定要执著于什么东西，这样的人生会显得更真实一点。可每个人自以为最完美的安排，终究无法让每一个人信服啊，正如“朱一旦”们喜欢“非洲安排”，马小策与张策，说到底不过是一种代号而已，当这种“安排”无法调和的时候，人和神仙一样，都会暴走，都会变身，唯一不同的是，人是要吃饭的，而神仙们早已学会辟谷。\n所以，在“救一个人还是救苍生”这个问题上，其实谁都没有错，我特别喜欢李诞在『奇葩说』中表达的一个观点，“以自私却不伤害别人的方式活着，才能维持世界的运转。而正是那些为了宏图伟业不计后果的牺牲‘小猫’的人，频频地让我们的世界陷入「大火」”。朱一旦不想再做“任人摆布”的老板，张策不想再做“默默无名”的幕后英雄，马老师早已看破这一切，“钱没给到位”，“心委屈了”，身在其位时榨干身体的“996”，人走茶凉时送瘟神般“高效”，一冷一热，果然是“环球同此凉热”呢。在全球变暖的趋势下，如果我们以自私却不伤害别人的方式活着，虽然活得有一点清冷、没有人情味，但这样是不是会更安心一点，骨子里与生俱来就带着“竞争”的基因的我们，是不是一定要学会“狠”、学会“不择手段”、学会“伤害”。我二十多岁的时候，想努力去照顾好一个人，而等到我快要三十岁的时候，我终于能勉强照顾好自己，这简直是一种幸运。\n这种感慨在某个场景下会更加明显，譬如一个人去看电影的时候，虽然我很喜欢和邻座的小朋友说话，可对方父母一句友善的“叔叔”，终于还是让两个人产生了距离。譬如找工作面试的时候，发觉三十上下的“哥哥姐姐”们，都开始面对“总监”级别职位时的恍惚感。也许，我们这一代人真的已经老了吧，而那个人早已离开你很久很久，我无意去对立资本家与劳动者间积怨久矣的矛盾，更无意去揣测封神台下蛰伏已久的阴谋。回想以前，乐无异在『古剑奇谭 2』中说出一句，“众生虽苦，还请诸恶莫作”，当时大概只是觉得这句话酷到不行，倘若议论公平，C 座 802 诸如三濑子、马小玲、马小浩等等角色，每一个都带着无数的梗，没有他们就没有整个朱一旦宇宙，当人们为张策惋惜的时候，是不是就选择性地遗忘了他们呢？朱一旦不会成为劳苦大众的代言人，而且任何人都不会，因为一切的流量到最后都是生意。\n我在 B 站关注过一位阿婆主，起初，他在厂里打工，下班后的“入味儿”是他主要的拍摄内容。后来，因为疫情的原因，他开始学别人拍做菜的视频。再后来，发现他变成了一位外卖小哥。世人皆苦，家家有本难念的经，可我们除了祝福以外，又能做一点什么呢？成人世界里，利益、立场、观点……，该有的一切都有，唯独没有对错，希望一个组织有一致的步伐、一致的声音，可偏偏人是一根会思考的芦苇，我知道，当一个人在某一种身份下，他必须要去推动一种文化形成，可如果这些声音连他自己都不信，这种文化的底蕴应该不会特别丰富，很容易成为政治博弈的牺牲品。我从前天真地以为，在互联网这样一个相对开放的环境里，不会存在政治这种产物。而出于对这种东西的逃避，我没有选择成为三线小城市里的一个公务员，实际上我尝试过，结果证明我真的不适合。可后来我发现我错了，只要有人的地方就会存在政治，无论是公司还是社区，每天都有人宣扬这样或者那样的“文化”，这个时候，我希望我们每一个人都去用心甄别这些概念，因为作为人的自觉，他只会说对自己有利的话，正如择偶标准是最毫无标准可言的标准一样，王垠说编程世界里充满宗派，就是最好的证明。\n所以，我不大愿意去统一什么东西，充满多样性、充满个性的世界，才是一个正常的世界，以结果论的观点而言，只要能送大家都目的地，是飞机还是高铁还是火车，真的重要吗？如果非要去统一什么，我希望是“语言”或者“领域语言”，因为，我们的沟通，因为存在太多的翻译而逐渐失真、甚至被曲解，我们一般把这样的沟通称之为扯皮，就像土味情话虽然美妙动听，但它携带了大量无用的信息。所以，即使冒着成为“钢铁直男”的危险，我依然想成为一个表达清晰的人。有人说，姜子牙就不能和原始天尊好好商量一下吗？非要自断天梯逼得鸿钧老祖出手吗？人类啊，归根到底，只愿意相信自己相信的，只愿意看见自己看见的，这种意念在成年后往往更加强烈，有多少遗憾就是得不到有效沟通造成的呢？九尾狐自觉被原始天尊欺骗、过河拆桥，而原始天尊认为“非我族类，其心必异”，都是选择性地相信了自己愿意去相信的东西。有人说，姜子牙有强迫症，为什么会任由师尊披头散发？因为不是每个人都能像约翰·纳什那样，在最亲密的人面前直抒胸臆，人类就是这么奇怪，和陌生人玩什么真心话大冒险，在亲人面前反而含蓄、羞怯起来，可能是因为某种特殊的磁力限制了声道发声吧，科学与玄学往往就是这么切换自如。\n思绪就像一个无底黑洞，姜子牙与朱一旦，两个八竿子不十竿子都打不着的人，就这么神奇地在我脑海里，完成了一次对话。如果思维存在奇点，将会坍陷于何处，苏格拉有没有底不重要，马老师们谁说得对同样不重要，甚至你看我这满纸荒唐言依然不重要，它仅仅表明我此时此刻在思考，我是一个活生生的人，所谓“我思故我在”，无非给枯燥的人生多一点无用的点缀罢了，你说朱一旦都不枯燥了，我们却还停留在这里，你说，还有比这个更枯燥的事情吗？申公豹形神俱灭，从头开始修行，居然连基因都发生了突变，大概，在这世间，没有什么可以永恒。\n",
            "tags": ["感悟","电影","生活","随笔"],
            "categories": ["生活感悟"],
            "thumbnail": ""
        },
        {
            "title": "使用 Dynamic Linq 构建动态 Lambda 表达式",
            "date": "2020-05-08",
            "permalink": "http://localhost:1313/posts/118272597/",
            "summary": "相信大家都有这样一种感觉，Linq和Lambda是.NET 中一以贯之的存在，从最早的 Linq to Object 到 Linq to SQL，再到 EF/EF Core 甚至如今的.NET Core，我们可以看到Lambda表达式的身影出现地越来越频繁。虽然 Linq to Object 和 Linq to SQL，分别是以IEnumerable\u0026lt;T\u0026gt;和IQueryable \u0026lt;T\u0026gt;为基础来实现的。我个人以为，Lambda呢，其实就是匿名委托的“变种”，而Linq则是对Lambda的进一步封装。在System.Linq.Expressions命名空间下，提供大量关于表达式树的 API，而我们都知道，这些表达式树最终都会被编译为委托。所以，动态创建 Lambda 表达式，实际上就是指从一个字符串生成对应委托的过程，而一旦这个委托被生成，可以直接传递给 Where()方法作为参数，显然，它可以对源数据进行过滤，这正是我们想要的结果。\n事出有因 在今天这篇博客中，我们主要介绍System.Linq.Dynamic.Core这个库，即我所说的 Dynamic Linq。本着“艺术源于生活的态度”，在介绍它的用法之前，不妨随博主一起看看，一个“简单“的查询是如何随着业务演进而变得越来越复杂。从某种意义上来说，正是它让博主想起了 Dynamic Linq。我们为客户编写了一个生成订单的接口，它从一张数据表中“消费”订单数据。最开始，它只需要过滤状态为“未处理”的记录，对应的 CRUD 可以表示为这样：\nvar orderInfos = repository.GetByQuery\u0026lt;tt_wg_order\u0026gt;(x =\u0026gt; x.STATUS == 10); 后来，因为业务方存在重复/错误下单的情况，业务数据有了“软删除”的状态，相应地查询条件再次发生变化，这看起来还行对吧：\nvar orderInfos = repository.GetByQuery\u0026lt;tt_wg_order\u0026gt;(x =\u0026gt; x.STATUS == 10 \u0026amp;\u0026amp; x.Isdelete == 0); 再后来，因为接口处理速度不理想，无法满足客户的使用场景，公司大佬们建议“加机器”，而为了让每台服务器上消费的订单数据不同(据说是为了避免发生并发)，大佬们要求博主开放所有字段作为查询条件，这样，每台服务器上可以配置不同查询条件。自此，又双叒叕改：\nvar repository = container.Resolve\u0026lt;CrudRepositoryBase\u0026gt;(); var searchParameters = new SearchParameters() { PageInfo = new PageInfo() { PageSize = parameters.",
            "content": "相信大家都有这样一种感觉，Linq和Lambda是.NET 中一以贯之的存在，从最早的 Linq to Object 到 Linq to SQL，再到 EF/EF Core 甚至如今的.NET Core，我们可以看到Lambda表达式的身影出现地越来越频繁。虽然 Linq to Object 和 Linq to SQL，分别是以IEnumerable\u0026lt;T\u0026gt;和IQueryable \u0026lt;T\u0026gt;为基础来实现的。我个人以为，Lambda呢，其实就是匿名委托的“变种”，而Linq则是对Lambda的进一步封装。在System.Linq.Expressions命名空间下，提供大量关于表达式树的 API，而我们都知道，这些表达式树最终都会被编译为委托。所以，动态创建 Lambda 表达式，实际上就是指从一个字符串生成对应委托的过程，而一旦这个委托被生成，可以直接传递给 Where()方法作为参数，显然，它可以对源数据进行过滤，这正是我们想要的结果。\n事出有因 在今天这篇博客中，我们主要介绍System.Linq.Dynamic.Core这个库，即我所说的 Dynamic Linq。本着“艺术源于生活的态度”，在介绍它的用法之前，不妨随博主一起看看，一个“简单“的查询是如何随着业务演进而变得越来越复杂。从某种意义上来说，正是它让博主想起了 Dynamic Linq。我们为客户编写了一个生成订单的接口，它从一张数据表中“消费”订单数据。最开始，它只需要过滤状态为“未处理”的记录，对应的 CRUD 可以表示为这样：\nvar orderInfos = repository.GetByQuery\u0026lt;tt_wg_order\u0026gt;(x =\u0026gt; x.STATUS == 10); 后来，因为业务方存在重复/错误下单的情况，业务数据有了“软删除”的状态，相应地查询条件再次发生变化，这看起来还行对吧：\nvar orderInfos = repository.GetByQuery\u0026lt;tt_wg_order\u0026gt;(x =\u0026gt; x.STATUS == 10 \u0026amp;\u0026amp; x.Isdelete == 0); 再后来，因为接口处理速度不理想，无法满足客户的使用场景，公司大佬们建议“加机器”，而为了让每台服务器上消费的订单数据不同(据说是为了避免发生并发)，大佬们要求博主开放所有字段作为查询条件，这样，每台服务器上可以配置不同查询条件。自此，又双叒叕改：\nvar repository = container.Resolve\u0026lt;CrudRepositoryBase\u0026gt;(); var searchParameters = new SearchParameters() { PageInfo = new PageInfo() { PageSize = parameters.PAGE_SIZE.Value }}; searchParameters.QueryModel.Items.Add(new ConditionItem { Field = \u0026#34;STATUS\u0026#34;, Method = QueryMethod.Equal, Value = 10 }); searchParameters.QueryModel.Items.Add(new ConditionItem { Field = \u0026#34;Isdelete\u0026#34;, Method = QueryMethod.Equal, Value = 0 }); //此处省略更多的查询条件:) var orderInfos = repository.GetByPage\u0026lt;tt_wg_order\u0026gt;(searchParameters); 可以想象得出，终极终终极的查询会变成下面这张图。这种方式看起来很美好对不对？可谁能想到，就在五一放假前的某一天里，博主还在替某个“刁钻”客户排查一组同样“刁钻”的过滤条件为什么没有生效。显然，我需要有一种更友好的方式，它可以从一个字符串变成一个委托，就像 JavaScript 里\u0026quot;邪恶\u0026quot;的 Eval()函数一样，说它邪恶，是因为它的输入是不可控的，\u0026ldquo;机智\u0026quot;的人类习惯把事件万物都当成 SQL 语句，其实，RESTful 接口里传 SQL、调存储过程难道不可以吗？同样，是因为这种做法太\u0026quot;邪恶\u0026rdquo;。\n![过滤条件在风中凌乱]](https://i.loli.net/2020/05/11/QEDHwA9bZUTInJY.png)\nParseLambda 首先，通过nuget安装：System.Linq.Dynamic.Core。这里主要介绍的是介绍的是其中的 ParseLambda()方法，顾名思义，它可以把一个字符串转换为指定类型的委托，一起来看下面的例子。首先，我们定义一个通用方法 BuildLambda：\nFunc\u0026lt;T, bool\u0026gt; BuildLambda\u0026lt;T\u0026gt;(string exps) { var sourceType = typeof(T); var sourceParameter = Expression.Parameter(sourceType); var lambdaExps = DynamicExpressionParser.ParseLambda( new[] { sourceParameter }, typeof(bool), exps ); return lambdaExps.Compile() as Func\u0026lt;T, bool\u0026gt;; } var students = new List\u0026lt;Student\u0026gt;() { new Student() { Name = \u0026#34;长安书小妆\u0026#34;, Age = 25, Address = \u0026#34;洛阳市洛龙区\u0026#34;, Teacher = new Teacher() { Name = \u0026#34;孔子\u0026#34; } }, new Student() { Name = \u0026#34;飞鸿踏雪\u0026#34;, Age = 28, Address = \u0026#34;宁夏中卫市\u0026#34;, Teacher = new Teacher() { Name = \u0026#34;孔子\u0026#34; } }, }; var exps = \u0026#34;Age\u0026lt;=25 \u0026amp;\u0026amp; Address.Contains(\\\u0026#34;洛阳市\\\u0026#34;) \u0026amp;\u0026amp; Teacher.Name=\\\u0026#34;孟子\\\u0026#34;\u0026#34;; var lambda = BuildLambda\u0026lt;Student\u0026gt;(exps); var results = students.Where(lambda); 注意到，核心的代码其实只有DynamicExpressionParser.ParseLambda()这一句，这充分暴露了博主“调包侠”的本质。按照示例代码中的过滤条件，我们知道给定数据中是没有符合条件的数据的。假如你真的运行了这段代码，你就会得到真正的结果：我说的是对的(逃\nOne More Thing 其实，我们今天所说这一切，从本质上来讲，还是属于表达式树的范畴，因为上面的例子，我们同样可以使用表达式树来编写，无非是这个第三方库帮我们隐藏了这部分细节。对于上面这个例子，如果用表达式树来写，会是什么样子的呢？相信熟悉表达式树的朋友，可以非常容易地写出下面的代码：\n//x var parameter = Expression.Parameter(typeof(tt_wg_order), \u0026#34;x\u0026#34;); //x.STATUS == 10 var condStatus = Expression.Equal(Expression.Property(parameter, \u0026#34;STATUS\u0026#34;), Expression.Constant(10)); //x.Isdelete == 0 var condIsDelete = Expression.Equal(Expression.Property(parameter, \u0026#34;Isdelete\u0026#34;), Expression.Constant(0)); //x.STATUS == 10 \u0026amp;\u0026amp; x.Isdelete == 0 var condAndAlso = Expression.AndAlso(condStatus, condIsDelete); //x =\u0026gt; x.STATUS == 10 \u0026amp;\u0026amp; x.Isdelete == 0 var lambda = Expression.Lambda\u0026lt;Func\u0026lt;tt_wg_order,bool\u0026gt;\u0026gt;(condAndAlso, parameter); 我们可以注意到，一个 Lmabda 表达式，可以抽象为:参数(Parameter)和函数体(Body)两部分，而Body实际上是由一个操作符和一个值组成。譬如这里的第一个条件：x.STATUS == 10。在这里基础上，我们可以定义一个类型：SearchParameters，它将每个条件抽象为字段(Field)、查询方法(QueryMethod)、值(Value)和或分组(OrGroup)。所以，它的处理逻辑就是，将相同 OrGroup 的条件放在一起用 Or 连接，然后再和其它条件放在一起用 And 连接。故而，它可以通过表达式构造出一个 Predict类型的委托，而我们的数据持久层是使用 EF 来实现的，所以，它可以顺利成章地和 IQueryable搭配使用，这就是我们这个 SearchParameters 的实现原理，它唯一让我觉得不好的地方是，字段(Field)不能通过一个 Lambda 表达式去构造，而必须传入一个字符串，这给了使用者写错字段名称的机会(逃：\npublic static class LambdaExpressionBuilder { private static Expression GetExpression (ParameterExpression parameter, Condition condition) { var propertyParam = Expression.Property (parameter, condition.Field); var propertyInfo = propertyParam.Member as PropertyInfo; if (propertyInfo == null) throw new ArgumentException ($\u0026#34;Invalid field \\\u0026#34;{condition.Field}\\\u0026#34;\u0026#34;); var realPropertyType = Nullable.GetUnderlyingType (propertyInfo.PropertyType) ?? propertyInfo.PropertyType; if (condition.Op != Operation.StdIn \u0026amp;\u0026amp; condition.Op != Operation.StdNotIn) condition.Value = Convert.ChangeType (condition.Value, realPropertyType); var constantParam = Expression.Constant (condition.Value); switch (condition.Op) { case Operation.Equals: return Expression.Equal (propertyParam, constantParam); case Operation.NotEquals: return Expression.NotEqual (propertyParam, constantParam); case Operation.Contains: return Expression.Call (propertyParam, \u0026#34;Contains\u0026#34;, null, constantParam);; case Operation.NotContains: return Expression.Not (Expression.Call (propertyParam, \u0026#34;Contains\u0026#34;, null, constantParam)); case Operation.StartsWith: return Expression.Call (propertyParam, \u0026#34;StartsWith\u0026#34;, null, constantParam); case Operation.EndsWith: return Expression.Call (propertyParam, \u0026#34;EndsWith\u0026#34;, null, constantParam); case Operation.GreaterThen: return Expression.GreaterThan (propertyParam, constantParam); case Operation.GreaterThenOrEquals: return Expression.GreaterThanOrEqual (propertyParam, constantParam); case Operation.LessThan: return Expression.LessThan (propertyParam, constantParam); case Operation.LessThanOrEquals: return Expression.LessThanOrEqual (propertyParam, constantParam); case Operation.StdIn: return Expression.Call (typeof (Enumerable), \u0026#34;Contains\u0026#34;, new Type[] { realPropertyType }, new Expression[] { constantParam, propertyParam }); case Operation.StdNotIn: return Expression.Not (Expression.Call (typeof (Enumerable), \u0026#34;Contains\u0026#34;, new Type[] { realPropertyType }, new Expression[] { constantParam, propertyParam })); } return null; } private static Expression GetGroupExpression (ParameterExpression parameter, List\u0026lt;Condition\u0026gt; orConditions) { if (orConditions.Count == 0) return null; var exps = orConditions.Select (c =\u0026gt; GetExpression (parameter, c)).ToList (); return exps.Aggregate\u0026lt;Expression, Expression\u0026gt; (null, (left, right) =\u0026gt; { if (left == null) return right; return Expression.OrElse (left, right); }); } public static Expression\u0026lt;Func\u0026lt;T, bool\u0026gt;\u0026gt; BuildLambda\u0026lt;T\u0026gt; (IEnumerable\u0026lt;Condition\u0026gt; conditions) { if (conditions == null || !conditions.Any ()) return x =\u0026gt; true; var parameter = Expression.Parameter (typeof (T), \u0026#34;x\u0026#34;); //简单条件 var simpleExps = conditions.ToList ().FindAll (c =\u0026gt; string.IsNullOrEmpty (c.OrGroup)) .Select (c =\u0026gt; GetExpression (parameter, c)) .ToList (); //复杂条件 var complexExps = conditions.ToList ().FindAll (c =\u0026gt; !string.IsNullOrEmpty (c.OrGroup)) .GroupBy (x =\u0026gt; x.OrGroup) .Select (g =\u0026gt; GetGroupExpression (parameter, g.ToList ())) .ToList (); var exp = simpleExps.Concat (complexExps).Aggregate\u0026lt;Expression, Expression\u0026gt; (null, (left, right) =\u0026gt; { if (left == null) return right; return Expression.AndAlso (left, right); });; return Expression.Lambda\u0026lt;Func\u0026lt;T, bool\u0026gt;\u0026gt; (exp, parameter); } } 接下来，我们就可以以一种优雅的方式来对编写查询条件：\nvar searchParameters = new SearchParameters(); searchParameters.Query = new QueryModel(); searchParameters.Query.Add(new Condition() { Field = \u0026#34;IntValue\u0026#34;, Op = Operation.LessThan, Value = 30 }); searchParameters.Query.Add(new Condition() { Field = \u0026#34;StringValue\u0026#34;, Op = Operation.Contains, Value = \u0026#34;山\u0026#34;, OrGroup = \u0026#34;StringValue\u0026#34; }); searchParameters.Query.Add(new Condition\u0026lt;Foo\u0026gt;() { Field = x =\u0026gt; x.StringValue, Op = Operation.Contains, Value = \u0026#34;有朋\u0026#34;, OrGroup = \u0026#34;StringValue\u0026#34; }); var lambda = LambdaExpressionBuilder.BuildLambda\u0026lt;Foo\u0026gt;(searchParameters.Query); var where = lambda.Compile(); var result = list.Where(where); 这种实现可以说相当巧妙啦，因为通过有限的条件，我们就可以覆盖到大部分查询的场景，而如果直接去解析一个 Lambda 表达式，难度显然会增加不少。这里是以一个普通的泛型列表作为示例的，而在实际使用中，常常是结合 EntityFramework 这类 ORM 来使用的。相应地，我们只需要为 IQueryable 接口扩展出支持 SearchParameter 作为参数进行查询地扩展方法即可，这分别对应了我们在文章一开头所提到的IEnumerable\u0026lt;T\u0026gt;和IQueryable \u0026lt;T\u0026gt;。\n可如果遇上 Dapper 这样的轻量级 ORM，我们要考虑的问题就变成了怎么通过 Lambda 表达式生成 SQL 语句，所以，通过 Dapper 来扩展功能的时候，最困难的地方，往往在于没法儿像 EF/EF Core 一样去随心所欲地 Where()，像 Dapper.Contrib 则只能先查询出所有结果再去做进一步的过滤，这种在数据量特别大的时候就会出问题。通过 Lambda 生成 SQL，最难的地方是，你压根不知道，人家会写一个什么样的表达式，而这个表达式，又怎么通过 SQL 去表达。那么，退而求其次，我们继续用 SearchParameters 来实现，因为它里面的 QueryMethod 是有限的，下面给出一个简单的实现：\npublic static class SearchParametersExtension { public static (string, Dictionary\u0026lt;string, object\u0026gt;) BuildSqlWhere (this SearchParameters searchParameters) { var conditions = searchParameters.Query; if (conditions == null || !conditions.Any ()) return (string.Empty, null); var sqlExps = new List\u0026lt;string\u0026gt; (); var sqlParam = new Dictionary\u0026lt;string, object\u0026gt; (); //构建简单条件 var simpleConditions = conditions.FindAll (x =\u0026gt; string.IsNullOrEmpty (x.OrGroup)); sqlExps.Add (simpleConditions.BuildSqlWhere (ref sqlParam)); //构建复杂条件 var complexConditions = conditions.FindAll (x =\u0026gt; !string.IsNullOrEmpty (x.OrGroup)); sqlExps.AddRange (complexConditions.GroupBy (x =\u0026gt; x.OrGroup).ToList ().Select (x =\u0026gt; \u0026#34;( \u0026#34; + x.BuildSqlWhere (ref sqlParam, \u0026#34; OR \u0026#34;) + \u0026#34; )\u0026#34;)); var sqlWhwere = sqlExps.Count \u0026gt; 1 ? string.Join (\u0026#34; AND \u0026#34;, sqlExps) : sqlExps[0]; return ($\u0026#34; WHERE {sqlWhwere} \u0026#34;, sqlParam); } public static string BuildSqlWhere (this IEnumerable\u0026lt;Condition\u0026gt; conditions, ref Dictionary\u0026lt;string, object\u0026gt; sqlParams, string keywords = \u0026#34; AND \u0026#34;) { if (conditions == null || !conditions.Any ()) return string.Empty; var sqlParamIndex = 1; var sqlExps = new List\u0026lt;string\u0026gt; (); foreach (var condition in conditions) { var index = sqlParams.Count + sqlParamIndex; switch (condition.Op) { case Operation.Equals: sqlExps.Add ($\u0026#34;{condition.Field} = @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = condition.Value; break; case Operation.NotEquals: sqlExps.Add ($\u0026#34;{condition.Field} \u0026lt;\u0026gt; @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = condition.Value; break; case Operation.Contains: sqlExps.Add ($\u0026#34;{condition.Field} LIKE @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;%{condition.Value}%\u0026#34;; break; case Operation.NotContains: sqlExps.Add ($\u0026#34;{condition.Field} NOT LIKE @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;%{condition.Value}%\u0026#34;; break; case Operation.StartsWith: sqlExps.Add ($\u0026#34;{condition.Field} LIKE @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;%{condition.Value}\u0026#34;; break; case Operation.EndsWith: sqlExps.Add ($\u0026#34;{condition.Field} LIKE @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;{condition.Value}%\u0026#34;; break; case Operation.GreaterThen: sqlExps.Add ($\u0026#34;{condition.Field} \u0026gt; @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;{condition.Value}\u0026#34;; break; case Operation.GreaterThenOrEquals: sqlExps.Add ($\u0026#34;{condition.Field} \u0026gt;= @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;{condition.Value}\u0026#34;; break; case Operation.LessThan: sqlExps.Add ($\u0026#34;{condition.Field} \u0026lt; @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;{condition.Value}\u0026#34;; break; case Operation.LessThanOrEquals: sqlExps.Add ($\u0026#34;{condition.Field} \u0026lt;= @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;{condition.Value}\u0026#34;; break; case Operation.StdIn: sqlExps.Add ($\u0026#34;{condition.Field} IN @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;{condition.Value}\u0026#34;; break; case Operation.StdNotIn: sqlExps.Add ($\u0026#34;{condition.Field} NOT IN @Param{index}\u0026#34;); sqlParams[$\u0026#34;Param{index}\u0026#34;] = $\u0026#34;{condition.Value}\u0026#34;; break; } sqlParamIndex += 1; } return sqlExps.Count \u0026gt; 1 ? string.Join (keywords, sqlExps) : sqlExps[0]; } } 现在，我们可以换一种方式来查 Dapper，果然是因为手写 SQL 没有安全感的缘故啊！\nvar searchParameters = new SearchParameters(); searchParameters.Page = new PageModel() { PageSize = 10, CurrentPage = 1 }; searchParameters.Query = new QueryModel(); searchParameters.Query.Add(new Condition() { Field = \u0026#34;OrgCode\u0026#34;, Op = Operation.Contains, Value = \u0026#34;飞天御剑流\u0026#34;, OrGroup = \u0026#34;OrgCode\u0026#34; }); searchParameters.Query.Add(new Condition() { Field = \u0026#34;OrgCode\u0026#34;, Op = Operation.Equals, Value = \u0026#34;新选组\u0026#34;, OrGroup = \u0026#34;OrgCode\u0026#34; }); searchParameters.Query.Add(new Condition() { Field = \u0026#34;CreatedAt\u0026#34;, Op = Operation.GreaterThenOrEquals, Value = new DateTime(2020, 1, 1)}); _repository.GetByQuery\u0026lt;BusinessUnit\u0026gt;(searchParameters); 对于定义Condition时，Field属性安全感缺失的问题，我们可以这样来解决：\npublic class Condition\u0026lt;T\u0026gt; : Condition public new Expression\u0026lt;Func\u0026lt;T, dynamic\u0026gt;\u0026gt; Field { get; set; } public Operation Op { get; set; } public object Value { get; set; } public string OrGroup { get; set; } } public class QueryModel : List\u0026lt;Condition\u0026gt; { public void Add\u0026lt;T\u0026gt;(Condition\u0026lt;T\u0026gt; condition) where T : class { var filedName = string.Empty; var memberExp = condition.Field.Body as MemberExpression; if (memberExp == null) { var ubody = (UnaryExpression)condition.Field.Body; memberExp = ubody.Operand as MemberExpression; } filedName = memberExp.Member.Name; Add(new Condition() { Field = filedName, Op = condition.Op, Value = condition.Value, OrGroup = condition.OrGroup }); } } 其实，这还是表达式树的内容，在上面的代码片段中，早已出现过它的身影，回想起多年前用这个东西改造 INotifyPropertyChanged 的时候，总觉得一切似曾相识：\nsearchParameters.Query.Add(new Condition\u0026lt;Foo\u0026gt;() { Field = x =\u0026gt; x.StringValue, Op = Operation.Contains, Value = \u0026#34;有朋\u0026#34;, OrGroup = \u0026#34;StringValue\u0026#34; }); 本文小结 和博主的大多数博客一样，这篇博客是一个“醉翁之意不在酒”的博客。听起来在说如何动态创建 Lambda 表达式，实际上讲的还是表达式树，至于原因，则还是博客开篇所提到的“一以贯之”。博主想写这篇博客，是源于实际工作中遇到的“查询”问题，而最后解决的还真就是查询的问题。不管是 Dynamic Linq 中的 ParseLambda()还是表达式树中的 LambdaExpression，本质上都是同一个东西，最终的命运都是 Predict这个委托。SearchParameters 则是对前者的一种简化，通过控制 Lambda 表达式的复杂度来简化问题，相比起直接传一个字符串过来，这种在风险的控制上要更高一点，之所以要传字符串，则是又一个非关技术的无聊的问题了，用 Jira 里的概念说应该叫做设计如此(By Design)。好了，以上就是这篇博客的内容啦，谢谢大家！\n",
            "tags": ["Linq","Lambda","表达式树"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "使用 jsDelivr 为 Hexo 博客提供高效免费的CDN加速",
            "date": "2020-02-05",
            "permalink": "http://localhost:1313/posts/1417719502/",
            "summary": "最近给博客做了升级，从 3.x 升级到了 4.x，主要是在官网看到了关于静态页面生成效率提升的内容。众所周知，Hexo 在文章数目增加以后会越来越慢。博主大概是从 14 年年底开始使用 Hexo 这个静态博客的，截止到目前一共有 176 篇博客，其中的“慢”可想而知，中间甚至动过使用 Hugo 和 VuePress 的念头，所以，听说有性能方面的提升，还是决定第一时间来试试。整个升级过程挺顺利的，唯一遇到的问题是关于外部链接检测方面的，具体可以参考这里。今天，博主主要想和大家分享下关于如何使用jsDelivr来为博客提供免费、高效的 CDN 服务，希望对大家有所帮助。\njsDelivr是一个免费、快速和可信赖的 CDN 加速服务，官网上声称它每个月可以支撑680亿次的请求。博主是在去年年底的时候，偶然了解到这个服务的存在，这次趁着疫情肆虐的间隙，终于把这个服务集成到了博客中。更重要的是，这个服务在 Github 上是开源的。目前，它提供了针对npm、Github和WordPress的加速服务，只需要一行代码就可以获得加速效果，以常用的jQuery和Bootstrap为例：\n// load jQuery v3.2.1 https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js // load bootstrap v4.4.1 https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.js 这意味着我们只需要发布一个 npm 的包，就可以使用它提供的加速服务。CDN 加速的好处我这里就不再多说了，只要我们的项目中用到了第三方的静态资源，譬如 JavaScript/CSS 等等都应该考虑接入到 CDN 中。有人常常担心 CDN 挂掉或者是私有化部署无法接入外网环境。我想说，我们目光应该长远一点，现在早已不是早年那种单打独斗式的开发模式了，我们不可能把所有资源都放到本地来。随着云计算的概念越发地深入人心，越来越多的基础服务都运行在一台又一台虚拟化的“云服务器”上，这种情况下，搞这种集中化配置的做法，是完全违背分布式的发展趋势的。\n如果说，针对 npm 包的 CDN 加速服务离我们还有点遥远，因为我们大多数情况下都是在使用别人写好的库。那么，接下来，针对 Github 的 CDN 加速服务应该会让我们无比兴奋吧，毕竟 Github Pages 的“慢”大家是可以感受得到的。不然，为什么大家要用 Coding Pages 做国内/国外的双线部署呢？首先，我们在浏览器里输入下面这个地址：https://cdn.jsdelivr.net/gh/qinyuanpei/qinyuanpei.github.io@latest/\njsDelivr提供的CDN加速资源\r此时，可以注意到，jsDelivr可以把我们 Github 上的资源呈现出来，只要我们在 Github 上发布过相应的版本即可。这里的版本，可以理解为一次 Release，对应 Git 中 tag 的概念，虽然 Github 现在引入了包管理器的概念，试图统一像 npm、nuget、pip 等等这样的包管理器。它提供的 CDN 服务有一个基本的格式：",
            "content": "最近给博客做了升级，从 3.x 升级到了 4.x，主要是在官网看到了关于静态页面生成效率提升的内容。众所周知，Hexo 在文章数目增加以后会越来越慢。博主大概是从 14 年年底开始使用 Hexo 这个静态博客的，截止到目前一共有 176 篇博客，其中的“慢”可想而知，中间甚至动过使用 Hugo 和 VuePress 的念头，所以，听说有性能方面的提升，还是决定第一时间来试试。整个升级过程挺顺利的，唯一遇到的问题是关于外部链接检测方面的，具体可以参考这里。今天，博主主要想和大家分享下关于如何使用jsDelivr来为博客提供免费、高效的 CDN 服务，希望对大家有所帮助。\njsDelivr是一个免费、快速和可信赖的 CDN 加速服务，官网上声称它每个月可以支撑680亿次的请求。博主是在去年年底的时候，偶然了解到这个服务的存在，这次趁着疫情肆虐的间隙，终于把这个服务集成到了博客中。更重要的是，这个服务在 Github 上是开源的。目前，它提供了针对npm、Github和WordPress的加速服务，只需要一行代码就可以获得加速效果，以常用的jQuery和Bootstrap为例：\n// load jQuery v3.2.1 https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js // load bootstrap v4.4.1 https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.js 这意味着我们只需要发布一个 npm 的包，就可以使用它提供的加速服务。CDN 加速的好处我这里就不再多说了，只要我们的项目中用到了第三方的静态资源，譬如 JavaScript/CSS 等等都应该考虑接入到 CDN 中。有人常常担心 CDN 挂掉或者是私有化部署无法接入外网环境。我想说，我们目光应该长远一点，现在早已不是早年那种单打独斗式的开发模式了，我们不可能把所有资源都放到本地来。随着云计算的概念越发地深入人心，越来越多的基础服务都运行在一台又一台虚拟化的“云服务器”上，这种情况下，搞这种集中化配置的做法，是完全违背分布式的发展趋势的。\n如果说，针对 npm 包的 CDN 加速服务离我们还有点遥远，因为我们大多数情况下都是在使用别人写好的库。那么，接下来，针对 Github 的 CDN 加速服务应该会让我们无比兴奋吧，毕竟 Github Pages 的“慢”大家是可以感受得到的。不然，为什么大家要用 Coding Pages 做国内/国外的双线部署呢？首先，我们在浏览器里输入下面这个地址：https://cdn.jsdelivr.net/gh/qinyuanpei/qinyuanpei.github.io@latest/\njsDelivr提供的CDN加速资源\r此时，可以注意到，jsDelivr可以把我们 Github 上的资源呈现出来，只要我们在 Github 上发布过相应的版本即可。这里的版本，可以理解为一次 Release，对应 Git 中 tag 的概念，虽然 Github 现在引入了包管理器的概念，试图统一像 npm、nuget、pip 等等这样的包管理器。它提供的 CDN 服务有一个基本的格式：\nhttps://cdn.jsdelivr.net/gh/user/repo@version/file\n如果大家感兴趣，可以把这里的 user 和 repo 改成自己的来体验一番。需要注意的是，这里的版本号同样可以换成 Commit ID 或者是分支的名称。我个人建议用 tag，因为它通常携带了版本号信息，语义上要更好一点。那么，顺着这个思路，我们只要把 Hexo 中的资源的相对路径改为 jsDelivr 的 CDN 加速路径就好啦！为了让切换更加自如，这里我们为 Hexo 写一个 Helper，它可以理解为 Hexo 中的辅助代码片段。我们在 \u0026lt;YouTheme\u0026gt;/scripts/ 目录下新建一个plugins.js 文件，这样 Hexo 会在渲染时自动加载这个脚本文件：\nconst source = (path, cache, ext) =\u0026gt; { if (cache) { const minFile = `${path}${ext === \u0026#39;.js\u0026#39; ? \u0026#39;.min\u0026#39; : \u0026#39;\u0026#39;}${ext}`; const jsdelivrCDN = hexo.config.jsdelivr; return jsdelivrCDN.enable ? `//${jsdelivrCDN.baseUrl}/gh/${jsdelivrCDN.gh_user}/${jsdelivrCDN.gh_repo}@latest/${minFile}` : `${minFile}?v=${version}` } else { return path + ext } } hexo.extend.helper.register(\u0026#39;theme_js\u0026#39;, (path, cache) =\u0026gt; source(path, cache, \u0026#39;.js\u0026#39;)) hexo.extend.helper.register(\u0026#39;theme_css\u0026#39;, (path, cache) =\u0026gt; source(path, cache, \u0026#39;.css\u0026#39;)) 接下来，修改布局文件，项目中的 JavaScript 和 CSS 文件，均通过 theme_js() 和 thems_css() 两个函数引入：\n//加载JavaScript \u0026lt;script src=\u0026#34;\u0026lt;%- url_for(theme_js(\u0026#39;assets/scripts/search\u0026#39;, cache)) %\u0026gt;\u0026#34; async\u0026gt;\u0026lt;/script\u0026gt; //加载CSS \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;\u0026lt;%- url_for(theme_css(\u0026#39;/assets/styles/style\u0026#39;, cache)) %\u0026gt;\u0026#34;\u0026gt; 既然是否使用 CDN 加速是可配置的，我们要在 _config.yml 文件中添加相应的配置项：\n# jsdelivr CDN jsdelivr: enable: true gh_user: qinyuanpei gh_repo: qinyuanpei.github.io baseUrl: cdn.jsdelivr.net 除此以外，我们还需要在部署博客的时候，生成一个名为 latest 的 tag。虽然官网上说，在引用 CDN 的时候版本号可以省略，不过经过博主反复尝试，不带版本号并不会指向正确的版本，有些资源文件会报 404，因为这部分资源文件回滚以后发现还是没有。所以，最后博主只好把这个版本号给固定下来了，这样又引入一个新问题，即：每次部署的时候都要先删除远程的 latest。所以，这块儿的Travis CI脚本看起来会有点讨厌，如果大家有更好的方案，欢迎大家在博客中留言：\ngit tag latest git push --force --quiet \u0026#34;https://${CI_TOKEN}@${GH_REF}\u0026#34; master:master --tags 好了，现在重新生成、部署，来看看效果吧： Coding Pages速度\rGithub Pages速度\r感觉效果还不错，Github Pages 比平时要快很多，博主顺便就给 Coding Pages 启用了 CDN 加速。话说，看到这张图的时候总是感慨，如果肺炎疫情地图能像这两张图一样就好啦！面对这场无声的战役，有很多人一直在一线抗击病魔，还有很多人默默无闻地在支援武汉。或许，宅在家里的你我，什么都做不了，可即便如此，还是让我们一起来祈祷疫情快点结束吧，因为春天都要来了呢……好了，这就是这篇博客的全部内容啦，谢谢大家！\n2020/02/13 更新 在此之前，博主提到版本号的问题，即每一次在 CDN 上生成的版本，怎样体现到 Hexo 中引用的资源上面。当时采用了一个取巧的方法，Hexo 中固定版本号为 latest，然后每次都推送这个 tag。这样引发一个问题，每次都先去远程删除这个 tag，显然这不是我期望的解决方案。最终，我采用的方案是，通过 Travis CI 编译部署的时候，首先导出变量 $TRAVIS_BUILD_NUMBER 到一个文本文件中，然后Hexo在生成静态页面的时侯，从这个文本文件中读取该变量的值作为版本号，这样每次编译部署的时候，我们总能获得一个新的 tag，而这个 tag 和 Hexo 中引用的资源版本一致，这样就彻底解决了这个遗留问题。修改后的 plugins.js 文件内容如下：\nvar fs = require(\u0026#39;fs\u0026#39;); var version = \u0026#39;latest\u0026#39; fs.readFile(\u0026#39;./BUILD_NUMBER.txt\u0026#39;, function (error, data) { if (error) { console.log(\u0026#39;load BUILD_NUMBER.txt fails, \u0026#39; + error) } else { version = data.toString().trim(); } }); const source = (path, cache, ext) =\u0026gt; { if (cache) { const minFile = `${path}${ext === \u0026#39;.js\u0026#39; ? \u0026#39;.min\u0026#39; : \u0026#39;\u0026#39;}${ext}`; const jsdelivrCDN = hexo.config.jsdelivr; return jsdelivrCDN.enable ? `//${jsdelivrCDN.baseUrl}/gh/${jsdelivrCDN.gh_user}/${jsdelivrCDN.gh_repo}@${version}/${minFile}` : `${minFile}?v=${version}` } else { return path + ext } } hexo.extend.helper.register(\u0026#39;theme_js\u0026#39;, (path, cache) =\u0026gt; source(path, cache, \u0026#39;.js\u0026#39;)) hexo.extend.helper.register(\u0026#39;theme_css\u0026#39;, (path, cache) =\u0026gt; source(path, cache, \u0026#39;.css\u0026#39;)) 修改后的 .travis.yml 文件可以在 这里 获取。\n",
            "tags": ["Hexo","CDN","jsDelivr"],
            "categories": ["独立博客"],
            "thumbnail": ""
        },
        {
            "title": "从 .NET Core 2.2 升级到 3.1 的踩坑之旅",
            "date": "2020-01-22",
            "permalink": "http://localhost:1313/posts/3099575458/",
            "summary": "有时候，版本更新太快并不是一件好事。虽然，两周一个迭代的“敏捷”开发依然被客户嫌弃交付缓慢，可一边是前端领域“求不要再更新了，学不动了”的声音，一边则是.NET Core从1.x到2.x再到3.x的高歌猛进。版本更新太快，带来的是API的频繁变动，无法形成有效的知识沉淀，就像转眼到了2020年，Python 2.x和Windows 7都引来了“寿终正寝”，可能你都还没有认真地学习过这些知识，突然就被告知这些知识要过期了，想想还是觉得挺疯狂啊。最近一直在捣鼓，如何让.NET Core应用跑在Heroku平台上，因为Docker镜像里使用最新的.NET Core 3.1运行时，所以，痛定思痛之余，决定把手头项目升级到3.1。上一次痛苦还是在2.1升级2.2，这还真没过多长时间。所以呢，这篇博客主要梳理下从2.2升级到3.1过程中遇到的问题。\n更新项目文件 调整目标框架为netcoreapp3.1 删除引用项：Microsoft.AspNetCore.App、Microsoft.AspNetCore.Razor.Design 删除AspNetCoreHostingModel，如果项目文件中的值为InProcess(因为ASP.NET Core 3.0 或更高版本项目默认为进程内承载模型） 更新程序入口 CreateWebHostBuilder()方法的返回值类型由IWebHostBuilder调整为IHostBuilder 增加引用项：Microsoft.Extensions.Hosting Kestrel配置变更至ConfigureWebHostDefaults()方法 public static IHostBuilder CreateWebHostBuilder(string[] args) =\u0026gt;\rHost.CreateDefaultBuilder(args)\r.ConfigureWebHostDefaults(webBuilder =\u0026gt;\r{\rwebBuilder.ConfigureKestrel(serverOptions =\u0026gt;\r{\r// Set properties and call methods on options\r})\r.UseStartup\u0026lt;Startup\u0026gt;();\r}); 如果通过 HostBuilder手动创建宿主，则需要在 ConfigureWebHostDefaults()方法中显式调用·UseKestrel()：\npublic static void Main (string[] args) {\rvar host = new HostBuilder ()\r.UseContentRoot (Directory.GetCurrentDirectory ())\r.ConfigureWebHostDefaults (webBuilder =\u0026gt; {\rwebBuilder.UseKestrel (serverOptions =\u0026gt; {\r// Set properties and call methods on options\r})\r.",
            "content": "有时候，版本更新太快并不是一件好事。虽然，两周一个迭代的“敏捷”开发依然被客户嫌弃交付缓慢，可一边是前端领域“求不要再更新了，学不动了”的声音，一边则是.NET Core从1.x到2.x再到3.x的高歌猛进。版本更新太快，带来的是API的频繁变动，无法形成有效的知识沉淀，就像转眼到了2020年，Python 2.x和Windows 7都引来了“寿终正寝”，可能你都还没有认真地学习过这些知识，突然就被告知这些知识要过期了，想想还是觉得挺疯狂啊。最近一直在捣鼓，如何让.NET Core应用跑在Heroku平台上，因为Docker镜像里使用最新的.NET Core 3.1运行时，所以，痛定思痛之余，决定把手头项目升级到3.1。上一次痛苦还是在2.1升级2.2，这还真没过多长时间。所以呢，这篇博客主要梳理下从2.2升级到3.1过程中遇到的问题。\n更新项目文件 调整目标框架为netcoreapp3.1 删除引用项：Microsoft.AspNetCore.App、Microsoft.AspNetCore.Razor.Design 删除AspNetCoreHostingModel，如果项目文件中的值为InProcess(因为ASP.NET Core 3.0 或更高版本项目默认为进程内承载模型） 更新程序入口 CreateWebHostBuilder()方法的返回值类型由IWebHostBuilder调整为IHostBuilder 增加引用项：Microsoft.Extensions.Hosting Kestrel配置变更至ConfigureWebHostDefaults()方法 public static IHostBuilder CreateWebHostBuilder(string[] args) =\u0026gt;\rHost.CreateDefaultBuilder(args)\r.ConfigureWebHostDefaults(webBuilder =\u0026gt;\r{\rwebBuilder.ConfigureKestrel(serverOptions =\u0026gt;\r{\r// Set properties and call methods on options\r})\r.UseStartup\u0026lt;Startup\u0026gt;();\r}); 如果通过 HostBuilder手动创建宿主，则需要在 ConfigureWebHostDefaults()方法中显式调用·UseKestrel()：\npublic static void Main (string[] args) {\rvar host = new HostBuilder ()\r.UseContentRoot (Directory.GetCurrentDirectory ())\r.ConfigureWebHostDefaults (webBuilder =\u0026gt; {\rwebBuilder.UseKestrel (serverOptions =\u0026gt; {\r// Set properties and call methods on options\r})\r.UseIISIntegration ()\r.UseStartup\u0026lt;Startup\u0026gt; ();\r})\r.Build ();\rhost.Run ();\r} 更新Startup Configure()方法第二个参数由``IHostingEnvironment调整为IWebHostEnvironment(需要引用Microsoft.Extensions.Hosting`) 从管道中删除UseMvc()扩展方法，相应地，删除AddMvc()及其链式调用相关方法 AddMvc()等价于AddRazorPages() + AddControllersWithViews() AddControllers()对应WebApi模板，AddControllersWithViews()对应MVC模板， AddRazorPages()对应SPA模板 路由注册由传统路由调整为终结点路由： public void Configure(IApplicationBuilder app, IWebHostEnvironment env)\r{\rapp.UseStaticFiles();\rapp.UseRouting();\rapp.UseCors();\rapp.UseAuthentication();\rapp.UseAuthorization();\rapp.UseEndpoints(endpoints =\u0026gt;\r{\r//SignalR路由 endpoints.MapHub\u0026lt;ChatHub\u0026gt;(\u0026#34;/chat\u0026#34;);\r//RazorPages路由\rendpoints.MapRazorPages()\r//特性路由(WebApi)\rendpoints.MapControllers();\r//控制器路由(MVC)\rendpoints.MapControllerRoute(\u0026#34;default\u0026#34;, \u0026#34;{controller=Home}/{action=Index}/{id?}\u0026#34;);\r});\r} 如果希望继续使用传统路由，则可以使用下列方法任一：\nservices.AddMvc(options =\u0026gt; options.EnableEndpointRouting = false);\rservices.AddControllers(options =\u0026gt; options.EnableEndpointRouting = false);\rservices.AddControllersWithViews(options =\u0026gt; options.EnableEndpointRouting = false);\rservices.AddRazorPages().AddMvcOptions(options =\u0026gt; options.EnableEndpointRouting = false); 序列化/反序列化 从.NET Core 3.0 开始，System.Text.Json默认作为替代Newtonsoft.json的新一代JSON API 直接从.NET Core 3.0 创建的SignalR项目，服务端返回的JSON数据存在大小写的问题，这是由System.Text.Json引起的。解决方案是： services.AddSignalR()\r.AddJsonProtocol(options =\u0026gt; options.PayloadSerializerOptions.PropertyNamingPolicy = null); 同理，对于该方案对于services.AddControllers()一样有效，前提是项目中使用了System.Text.Json。同理，对于SignalR的客户端项目，我们有：\nnew HubConnectionBuilder()\r.WithUrl(\u0026#34;/chatHub\u0026#34;)\r.AddJsonProtocol(options =\u0026gt;\r{ //TODO\r})\r.Build(); SignalR的JavaScript客户端由@aspnet/signalr 调整为为 @microsoft/signalr： const signalR = require(\u0026#34;@microsoft/signalr\u0026#34;);\rlet connection = new signalR.HubConnectionBuilder().withUrl(url).build(); 如果希望继续使用Newtonsoft.json，则需要安装AspNetCore NewtonsoftJson。相应地，需要显式调用AddNewtonsoftJson()扩展方法： services.AddControllers()\r.AddNewtonsoftJson(options =\u0026gt; {\roptions.SerializerSettings.ContractResolver = new CamelCasePropertyNamesContractResolver();\r}); 同样地，AddNewtonsoftJson()支持AddControllers()， AddControllersWithViews()， AddRazorPages()所有方法\n疑难杂症 升级后提示无法加载类型：Microsoft.AspNetCore.Mvc.MvcJsonOptions，解决方案是： 升级Swashbuckle.AspNetCore至最新版本(5.0+)，调整Swagger中间件配置代码： services.AddSwaggerGen(swagger =\u0026gt;\r{\r//这里发生了变化，需要引用：Microsoft.OpenApi.Models\rswagger.SwaggerDoc(\u0026#34;v1\u0026#34;, new OpenApiInfo { Title = \u0026#34;ynamic WebApi\u0026#34;, Version = \u0026#34;v1.0\u0026#34; });\r}); 安装完 .NET Core 3.x，使用dotnet build编译项目提示找不到Microsoft.NET.Sdk.Web。解决方案是： 升级2.2的时候，调整项目文件中的Microsoft.NET.Sdk.Web为Microsoft.NET.Sdk可以解决，而这个方法在3.x以后失效。 此时，可以检查环境变量MSBuildSDKsPath中的SDK版本和实际版本是否一致，尤其是像博主这样从2.0一路升级到3.x的朋友，应该都会遇到这个问题。 参考链接 从 ASP.NET Core 2.2 迁移到3.0 升级 ASP.NET Core 3.0 设置 JSON 返回 PascalCase 格式与 SignalR 问题 Incompatibility with ASP.NET Core 3.0 ",
            "tags": [".NET Core","SignalR","迁移"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "Vue 快速实现通用表单验证",
            "date": "2019-09-06",
            "permalink": "http://localhost:1313/posts/169430744/",
            "summary": "本文开篇第一句话，想引用鲁迅先生《祝福》里的一句话，那便是：“我真傻，真的，我单单知道后端整天都是 CRUD，我没想到前端整天都是 Form 表单”。这句话要从哪里说起呢？大概要从最近半个月的“全栈工程师”说起。项目上需要做一个城市配载的功能，顾名思义，就是通过框选和拖拽的方式在地图上完成配载。博主选择了前后端分离的方式，在这个过程中发现：首先，只要有依赖 jQuery 的组件，譬如 Kendoui，即使使用了 Vue，依然需要通过 jQuery 去操作 DOM。其次，只有有通过 Rozar 生成的 DOM，譬如 HtmlHelper，Vue 的双向绑定就突然变得尴尬起来，更不用说，Rozar 中的@语法和 Vue 中的@指令相互冲突的问题，原本可以直接用 v-for 生成列表，因为使用了 HtmlHelper，突然一下子变得厌恶起来，虽然 Rozar 语法非常强大，可我依然没有在 JavaScript 里写 C#的热情，因为实在太痛苦啦 Orz……\n所以，想做好前后端分离，首先需要分离出一套前端组件库，做不到这一点，前后端分离就无从谈起，就像我们公司的项目，即使框架切换到.NET Core，可是在很长的一段时间里，我们其实还是再写 MVC，因为所有的组件都是后端提供的 HtmlHelper/TagHelper 这种形式。我这次做项目的过程中，其实是通过 jQuery 实现了一部分组件，正因为如此，一个在前后端不分离时非常容易实现的功能，在前后端分离以后发现缺好多东西，就比如最简单的表单验证功能，即便你是在做一个新项目，为了保证产品在外观上的一致性，你还是得依赖老项目的东西，所以，这篇博客主要想说说前后端分离以后，Vue 的时代怎么去做表单的验证。因为我不想测试同事再给我提 Bug，问我为什么只有来自后端接口的验证，而没有来自前端页面的验证。我希望，在写下这篇博客之前，我可以实现和老项目一模一样的表单验证。如同 CRUD 之于后端，80%的前端都是在写 Form 表单，所以，这个事情还是挺有意思的。\n最简单的表单验证 OK，作为国内最接“地气”的前端框架，Vue 的文档可以说是相当地“亲民”啦！为什么这样说呢，因为其实在官方文档中，尤大已经提供了一个表单验证的示例，这个示例让我想起给某银行做自动化工具时的情景，因为这两者都是采用 MVVM 的思想，所以，理解起来是非常容易的，即：通过一个列表来存储错误信息，而这个错误信息会绑定到视图层，所以，验证的过程其实就是向这个列表里添加错误信息的过程。我们一起来看这个例子：\n\u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;你好，请登录\u0026lt;/h2\u0026gt; \u0026lt;div\u0026gt; \u0026lt;form id=\u0026#34;loginFrom\u0026#34;\u0026gt; \u0026lt;div\u0026gt; \u0026lt;label\u0026gt;邮箱\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; class=\u0026#34;form-control\u0026#34; id=\u0026#34;inputEmail3\u0026#34; placeholder=\u0026#34;Email\u0026#34; v-model=\u0026#34;email\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;label\u0026gt;密码\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; class=\u0026#34;form-control\u0026#34; id=\u0026#34;inputPassword3\u0026#34; placeholder=\u0026#34;Password\u0026#34; v-model=\u0026#34;password\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-default login\u0026#34; v-on:click=\u0026#34;login()\u0026#34;\u0026gt;登录\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div v-if=\u0026#34;errorList.",
            "content": "本文开篇第一句话，想引用鲁迅先生《祝福》里的一句话，那便是：“我真傻，真的，我单单知道后端整天都是 CRUD，我没想到前端整天都是 Form 表单”。这句话要从哪里说起呢？大概要从最近半个月的“全栈工程师”说起。项目上需要做一个城市配载的功能，顾名思义，就是通过框选和拖拽的方式在地图上完成配载。博主选择了前后端分离的方式，在这个过程中发现：首先，只要有依赖 jQuery 的组件，譬如 Kendoui，即使使用了 Vue，依然需要通过 jQuery 去操作 DOM。其次，只有有通过 Rozar 生成的 DOM，譬如 HtmlHelper，Vue 的双向绑定就突然变得尴尬起来，更不用说，Rozar 中的@语法和 Vue 中的@指令相互冲突的问题，原本可以直接用 v-for 生成列表，因为使用了 HtmlHelper，突然一下子变得厌恶起来，虽然 Rozar 语法非常强大，可我依然没有在 JavaScript 里写 C#的热情，因为实在太痛苦啦 Orz……\n所以，想做好前后端分离，首先需要分离出一套前端组件库，做不到这一点，前后端分离就无从谈起，就像我们公司的项目，即使框架切换到.NET Core，可是在很长的一段时间里，我们其实还是再写 MVC，因为所有的组件都是后端提供的 HtmlHelper/TagHelper 这种形式。我这次做项目的过程中，其实是通过 jQuery 实现了一部分组件，正因为如此，一个在前后端不分离时非常容易实现的功能，在前后端分离以后发现缺好多东西，就比如最简单的表单验证功能，即便你是在做一个新项目，为了保证产品在外观上的一致性，你还是得依赖老项目的东西，所以，这篇博客主要想说说前后端分离以后，Vue 的时代怎么去做表单的验证。因为我不想测试同事再给我提 Bug，问我为什么只有来自后端接口的验证，而没有来自前端页面的验证。我希望，在写下这篇博客之前，我可以实现和老项目一模一样的表单验证。如同 CRUD 之于后端，80%的前端都是在写 Form 表单，所以，这个事情还是挺有意思的。\n最简单的表单验证 OK，作为国内最接“地气”的前端框架，Vue 的文档可以说是相当地“亲民”啦！为什么这样说呢，因为其实在官方文档中，尤大已经提供了一个表单验证的示例，这个示例让我想起给某银行做自动化工具时的情景，因为这两者都是采用 MVVM 的思想，所以，理解起来是非常容易的，即：通过一个列表来存储错误信息，而这个错误信息会绑定到视图层，所以，验证的过程其实就是向这个列表里添加错误信息的过程。我们一起来看这个例子：\n\u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;你好，请登录\u0026lt;/h2\u0026gt; \u0026lt;div\u0026gt; \u0026lt;form id=\u0026#34;loginFrom\u0026#34;\u0026gt; \u0026lt;div\u0026gt; \u0026lt;label\u0026gt;邮箱\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; class=\u0026#34;form-control\u0026#34; id=\u0026#34;inputEmail3\u0026#34; placeholder=\u0026#34;Email\u0026#34; v-model=\u0026#34;email\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;label\u0026gt;密码\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; class=\u0026#34;form-control\u0026#34; id=\u0026#34;inputPassword3\u0026#34; placeholder=\u0026#34;Password\u0026#34; v-model=\u0026#34;password\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-default login\u0026#34; v-on:click=\u0026#34;login()\u0026#34;\u0026gt;登录\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div v-if=\u0026#34;errorList.length \u0026gt; 0\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;alert alert-danger\u0026#34; role=\u0026#34;alert\u0026#34;\u0026gt;{{errorList.join(\u0026#39;;\u0026#39;)}}\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; var vm = new Vue({ el: \u0026#39;#loginFrom\u0026#39;, data: { email: \u0026#34;\u0026#34;, password: \u0026#34;\u0026#34;, errorList: [] }, methods: { validate: function () { this.errorList = [] if (this.email == \u0026#39;\u0026#39;) { this.errorList.push(\u0026#39;请输入邮箱\u0026#39;); } else { var reg = /^([a-zA-Z]|[0-9])(\\w|\\-)+@[a-zA-Z0-9]+\\.([a-zA-Z]{2,4})$/; if (!reg.test(this.email)) { this.errorList.push(\u0026#39;请输入有效的邮箱\u0026#39;); } } if (this.password == \u0026#39;\u0026#39;) { this.errorList.push(\u0026#39;请输入密码\u0026#39;); } else { if (this.password.length \u0026lt; 6) { this.errorList.push(\u0026#39;密码长度不得少于6位\u0026#39;); } } return this.errorList.length \u0026lt;= 0; }, login: function () { if (this.validate()) { alert(\u0026#39;登录成功\u0026#39;); } } } }); \u0026lt;/script\u0026gt; 为了排除无关内容对大家的影响，写这个例子的时候，博主排除了一切复杂的 HTML 结构和 CSS 样式，经过简单润色以后，这个例子的效果展示如下，果然 GUI 满足了人们颜控的一面，可让这个世界高速运行的是 CLI，Bootstrap 是博主这种“全栈工程师”的最爱之一。这种验证方式简直是人类本能的反应，可这恰好是最糟糕的一个例子，因为这个代码完全没法复用，可以想象得到，如果再继续增加针对密码强度，譬如大小写、数字等等的验证，这个代码会混乱成什么样子，所以，这是最简单的表单验证，同样是最糟糕的表单验证。\n第一个表单验证的例子\r基于 jQuery 的表单验证 其实，如果不是因为老项目依赖 jQuery，而新项目在某些地方又需要和老项目保持一致，有谁会喜欢在 Vue 的世界里使用 jQuery 呢？因为数据驱动和事件驱动，真的是两种不同的思想，我就见过因为监听不到某个事件而花费一整天时间的人……所以，这里使用 jQuery 的表单验证插件jQuery Validation，目的只有一个，即实现博主对自己的承诺，做一个和老项目一模一样的表单验证。官方这个示例最大的问题是，它的检验逻辑扩展性比较差，后端同学对这个应该有所体会啦，譬如实际业务中常常有邮箱、手机号、非空、数字、正则等等的验证规则，而后端常常采用基于 Attribute 的验证或者是 FluentValidation 这样的库，所以，核心问题是，能不能定义相应的验证规则。接下来，我们通过 jQuery 的表单验证插件来实现验证。\n通常情况下，jQuery Validation 支持面向控件和面向代码两种验证方式。所谓面向控件，就是指在控件里添加类似required、email、range等等的扩展属性，jQuery Validation 内置了十余种标准的验证规则，基本可以满足我们的日常使用。而面向代码，就是通过 JavaScript 来定义验证规则，这就非常符合 Vue 数据驱动的风格了，因为在 JavaScript 里一切皆是对象，而这些对象可以作为 Vue 中的数据来使用。自然而然地，在第一个示例的基础上，我们可以非常容易地扩展出基于 jQuery 的表单验证：\nvar vm = new Vue({ el:\u0026#39;#loginFrom\u0026#39;, data:{ email:\u0026#34;\u0026#34;, password:\u0026#34;\u0026#34;, validators:{ rules: { email: { required: true, email: true }, password: { required: true, minlength: 6, } }, messages:{ email:{ required:\u0026#34;请输入邮箱\u0026#34;, email:\u0026#34;请输入有效的邮箱\u0026#34; }, password:{ required:\u0026#34;请输入密码\u0026#34;, minlength:\u0026#34;密码长度不得少于6位\u0026#34; } } } }, mounted:function(){ $(\u0026#39;#loginFrom\u0026#39;).validate(this.validators); } }); 对于当前表单 loginFrom，其验证规则为 validators，它完全参照jQuery Validation的 API 文档而来，具体大家可以从jQuery Validation的文档来做进一步了解。这里唯一看起来不爽的就是#loginFrom，因为它和整个 Vue 看起来格格不入。不过，像博主目前项目的处境，如果老项目里使用jQuery来对表单进行验证，而使用 Vue 开发的新项目要兼容老项目的设计风格，使用 jQuery 有什么不可以呢？不得不说，Vue 作为一个渐进式的开发框架，真正照顾了各个\u0026quot;年龄\u0026quot;段的前端工程师。使用jQuery Validation以后的表单验证效果如下：\n基于jQuery的表单验证\r通过jQuery Validation，我们或许能感觉到一点不一样的地方，那就是表单验证其实还是蛮有意思的哈。也许是因为我原本是一个无聊的人，所以看到一点新的东西就觉得有趣。就像我虽然在提交数据时在后端做了校验，可牺牲的其实是整个前端的使用体验。而如果在前端对数据进行校验，是在输入过程中校验还是在输入完成校验，是通过表单自带的提交功能还是自己发起一个 AJAX 请求，这里面的确是有非常多的细节支撑的。第一种方案不支持远程校验，这更加能说明校验本身要考虑的不单单只有前端了，同理，有了前端的校验，不代表后端可以不做校验。前端时间有人在知乎上提问，大意是说前端该不该完全信任后端返回的数据，严格来说，我们不应该信任任何人提供的数据，而这就是校验这件事情本身的意义。\n基于 Vue 的表单验证 OK，如果说前面的两种校验是因为我们有一点历史包袱，那么，接下来，我们将尝试采用更“现代化”的表单验证方式。通过 Vue 文档中关于数据校验这一节的内容，我们了解到官方推荐的两个表单验证插件是vuelidate和VeeValidate，而实际上这篇博客中的第一个例子，就是由文档中的例子演化而来。我个人比较喜欢后者，所以，下面我们将使用这个插件来完成第三个例子。首先 ，我们通过Vue-Cli创建一个 Vue 项目，然后安装下面vee-validate和vue-i18n两个插件：\nnpm install vee-validate@2.0.0 --save npm install vue-i18n 注意到这里指定了版本号，这是因为最新的 3.x 超出了我这个新人的接受范围，一句话，太难了！接下来，我们在入口文件main.js中添加下面的代码，目的是启用这两个插件：\nimport VueI18n from \u0026#39;vue-i18n\u0026#39;; import VeeValidate from \u0026#39;vee-validate\u0026#39;; import zh_CN from \u0026#39;vee-validate/dist/locale/zh_CN\u0026#39; //启用Vue国际化插件 Vue.use(VueI18n) //配置VeeValidate const i18n = new VueI18n({ locale: \u0026#39;zh_CN\u0026#39;, }) Vue.use(VeeValidate, { i18n, i18nRootKey: \u0026#39;validation\u0026#39;, dictionary: { zh_CN } }); 接下来，编写一个单文件组件LoginForm.vue:\n\u0026lt;!-- template of LoginForm --\u0026gt; \u0026lt;template\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h2 class=\u0026#34;text-center\u0026#34;\u0026gt;你好，请登录\u0026lt;/h2\u0026gt; \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; \u0026lt;form class=\u0026#34;form-horizontal col-md-offset-4 col-md-4\u0026#34; id=\u0026#34;loginFrom\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;inputEmail3\u0026#34; class=\u0026#34;col-sm-2 control-label\u0026#34;\u0026gt;邮箱\u0026lt;/label\u0026gt; \u0026lt;div class=\u0026#34;col-sm-10\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; class=\u0026#34;form-control\u0026#34; id=\u0026#34;email\u0026#34; name=\u0026#34;email\u0026#34; placeholder=\u0026#34;Email\u0026#34; v-model=\u0026#34;email\u0026#34; v-validate=\u0026#34;\u0026#39;required|email\u0026#39;\u0026#34; data-vv-as=\u0026#34;邮箱\u0026#34;/\u0026gt; \u0026lt;p class=\u0026#34;alert alert-danger\u0026#34; role=\u0026#34;alert\u0026#34; v-show=\u0026#34;errors.has(\u0026#39;email\u0026#39;)\u0026#34;\u0026gt;{{ errors.first(\u0026#39;email\u0026#39;) }}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34; name=\u0026#34;password\u0026#34; rules=\u0026#34;required\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;inputPassword3\u0026#34; class=\u0026#34;col-sm-2 control-label\u0026#34;\u0026gt;密码\u0026lt;/label\u0026gt; \u0026lt;div class=\u0026#34;col-sm-10\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; class=\u0026#34;form-control\u0026#34; id=\u0026#34;password\u0026#34; name=\u0026#34;password\u0026#34; placeholder=\u0026#34;Password\u0026#34; v-model=\u0026#34;password\u0026#34; v-validate=\u0026#34;\u0026#39;required|min:6\u0026#39;\u0026#34; data-vv-as=\u0026#34;密码\u0026#34;/\u0026gt; \u0026lt;p class=\u0026#34;alert alert-danger\u0026#34; role=\u0026#34;alert\u0026#34; v-show=\u0026#34;errors.has(\u0026#39;password\u0026#39;)\u0026#34;\u0026gt;{{ errors.first(\u0026#39;password\u0026#39;) }}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;col-sm-offset-2 col-sm-10\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;checkbox\u0026#34;\u0026gt; \u0026lt;label\u0026gt; \u0026lt;input type=\u0026#34;checkbox\u0026#34; /\u0026gt;记住密码 \u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;col-sm-offset-2 col-sm-10\u0026#34;\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-default login\u0026#34; v-on:click=\u0026#34;login()\u0026#34;\u0026gt;登录\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;!-- script of LoginForm --\u0026gt; \u0026lt;script\u0026gt; export default { name: \u0026#34;LoginForm\u0026#34;, components: {}, data: () =\u0026gt; ({ email: \u0026#34;\u0026#34;, password: \u0026#34;\u0026#34; }), methods: { login: function() { } } }; \u0026lt;/script\u0026gt; \u0026lt;!-- style of LoginForm --\u0026gt; \u0026lt;style scoped\u0026gt; .login { color: white; height: 38px; width: 300px; background-color: #2b669a; } \u0026lt;/style\u0026gt; 可以看到，我们在关键的两个 input 控件上添加了v-validate和data-vv-as这两个属性。比如我们这里需要验证用户输入的邮箱是否合法、邮箱是否为空，那么我们就可以使用下面的语法：\n\u0026lt;input type=\u0026#34;text\u0026#34; class=\u0026#34;form-control\u0026#34; id=\u0026#34;email\u0026#34; name=\u0026#34;email\u0026#34; placeholder=\u0026#34;Email\u0026#34; v-model=\u0026#34;email\u0026#34; v-validate=\u0026#34;\u0026#39;required|email\u0026#39;\u0026#34; data-vv-as=\u0026#34;邮箱\u0026#34;/\u0026gt; \u0026lt;p class=\u0026#34;alert alert-danger\u0026#34; role=\u0026#34;alert\u0026#34; v-show=\u0026#34;errors.has(\u0026#39;email\u0026#39;)\u0026#34;\u0026gt;{{ errors.first(\u0026#39;email\u0026#39;) }}\u0026lt;/p\u0026gt; 这些语法在 Vue 中被称为指令，而data-vv-as则是 HTML5 中的一个特性，用来给提示信息中的字段起一个别名。实际上，这个插件里同样内置了一批常见的校验规则。当控件中的值不满足校验条件时，就会在errors中产生错误信息，所以，我们根据错误信息中是否包含指定字段来决定要不要展示错误信息，这就是这个插件的作用。运行这个例子，我们会得到下面的结果。\n基于Vue的表单校验\r既然提到这类表单验证最难的地方在于扩展性，那么下面我们再来看看如何扩展一个新的校验规则，这里以最常见的手机号校验为例, 个人以为这是这个插件最为强大的地方：\nValidator.extend(\u0026#39;isMobile\u0026#39;, { messages: { zh_CN: field =\u0026gt; field + \u0026#39;必须是11位手机号码\u0026#39; }, validate: value =\u0026gt; { return value.length === 11 \u0026amp;\u0026amp; /^((13|14|15|17|18)[0-9]{1}\\d{8})$/.test(value) } }) 相信通过今天这篇博客，大家应该对 Vue 里的表单验证有一点心得了。这类验证的库或者框架其实非常多，整合到 Vue 中要做的工作无外乎写一个插件，在控件触发相关事件或者表单提交的时候进行验证。作为一个 Vue 的新人，这个过程可谓是路漫漫其修远。你大概想不到，我是在凌晨加班加到凌晨两点半的情况下做完这几个示例的，最近这两三个月里加的班比我过去三年都多，这到底是好事还是坏事呢？有时候不知道自己还能不能坚持下去，往事已矣，人难免会感到迷茫的吧！\n本文小结 这篇博客主要通过三个示例分享了 Vue 下表单校验的实现，而促使博主对这一切进行研究的原始动力，则是源于一个实际工作中通过 Vue 开发的新项目。前后端要不要分离、项目里要不要继续使用 jQuery、该不该频繁地操作 DOM，这其实是毫无关联地三件事情，而这种事情 90%的人是完全不关心的，就像有一种看起来相当“成年人”的做法，出了事情第一时间不是去纠结谁的过错，而是问能不能马上解决以及解决问题需要多长时间。这看起来好像一点问题都没有，可不去在意事件本身对错的人，是因为这些问题不需要他去处理，利益相关和责任相关是完全不一样的，因为你不能一出问题全部都找到程序员这里，这项目又不是程序员一个人的。我关心这些无关紧要的问题，纯粹是因为我对自己做的东西有一种感情，我想做好它而已，我希望自己是个纯粹的人，而且可以一直纯粹下去，晚安！\n",
            "tags": ["Vue","表单","验证"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "使用 ASP.NET Core 和 Hangfire 实现 HTTP 异步化方案",
            "date": "2019-07-04",
            "permalink": "http://localhost:1313/posts/1071063696/",
            "summary": "Hi，大家好，我是 Payne，欢迎大家一如既往地关注我的博客。今天这篇博客里的故事背景，来自我工作中的一次业务对接，因为客户方提供的是长达上百行的 XML，所以一度让更喜欢使用 JSON 的博主感到沮丧，我这里不是想讨论 XML 和 JSON 彼此的优缺点，而是我不明白 AJAX 里的 X 现在基本都被 JSON 替代了，为什么还有这么多的人坚持使用并友好的 XML 作为数据的交换协议呢？也许你会说，因为有这样或者那样等等的理由，就像 SOA、ESB、SAP 等等类似的技术在企业级用户依然大量流行一样，而这些正是“消费”XML 的主力军。我真正想说的是，在对接这类接口时，我们会遇到一个异步化的 HTTP 协议场景，这里的异步和多线程、async/await 没有直接关系，因为它描述的实际上是业务流程上的一种“异步”。\n引子-想对 XML 说不 我们知道，HTTP 协议是一个典型的请求-响应模型，由调用方(Client)调用服务提供者(Server)提供的接口，在理想状态下，后者在处理完请求后会直接返回结果。可是当后者面对的是一个“耗时”任务时，这种方式的问题就立马凸显出来，此时调用者有两个选择：一直等对方返回直至超时(同步)、隔一会儿就看看对方是否处理完了(轮询)。这两种方式，相信大家都非常熟悉了，如果继续延伸下去，我们会联想到长/短轮询、SignalR、WebSocket。其实，更好的方式是，我们接收到一个“耗时”任务时，立即返回表明我们接收了任务，等任务执行完以后再通知调用者，这就是我们今天要说的 HTTP 异步化方案。因为对接过程中，客户采用的就是这种方案，ESB 这类消息总线本身就提供了这种功能，可作为调用方的博主就非常难受啦，因为明明能“同步”地处理完的事情，现在全部要变成“异步”处理，就像一个习惯了 async/await 语法糖的人，突然间就要重新开始写 APM 风格的代码，宝宝心里苦啊，“异步”处理就异步处理嘛，可要按人家要求去返回上百行的 XML，博主表示想死的心都有了好嘛……\n好了，吐槽归吐槽，吐槽完我们继续梳理下 HTTP 异步化的方案，这种方式在现实生活中还是相当普遍的，毕竟人类都是“异步”做事，譬如“等你哪天有空一起吃个饭”，测试同事对我说得最多的话就是，“等你这个 Bug 改完了同我说一声”，更不用说，JavaScript 里典型的异步单线程的应用等等……实现“异步”的思路其实是非常多的，比如同样在 JavaScript 里流行的回调函数，比如通过一张中间表存起来，比如推送消息到消息队列里。在面向数据库编程的时候，我听到最多的话就是，没有什么问题是不能用一张中间表来解决的，如果一张不行那就用两张。项目上我是用 Quartz+中间表的方式实现的，因为这是最为普通的方式。这里，我想和大家分享下，关于使用 Hangfire 来实现类似 Quartz 定时任务的相关内容，果然，我这次又做了一次标题党呢，希望大家会对今天的内容感兴趣。简单来说，我们会提供一个接口，调用方提供参数和回调地址，调用后通过 Hangfire 创建后台任务，等任务处理结束后，再通过回调地址返回结果给调用方，这就是所谓的 HTTP 异步化。\n开箱即用的 Hangfire 我们项目上是使用 Quartz 来实现后台任务的，因为它采用了反射的方式来调用具体的 Job，因此，它的任务调度和任务实现是耦合在同一个项目里的，常常出现单个 Job 引发整个系统卡顿的情况，尤其是是它的触发器，常常导致一个 Job 停都停不下来，直到后来才渐渐开始通过 Web API 来分离这两个部分。Quartz 几乎没有一个自己的可视化界面，我们为此专门为它开发了一套 UI。我这里要介绍的 Hangfire，可以说它刚好可以作为 Quartz 的替代品，它是一个开箱即用的、轻量级的、开源后台任务系统，想想以前为 Windows 开发定时任务，只能通过定时器(Timer)来实现，尚不知道 CRON 为何物，而且只能用命令行那种拙劣的方式来安装/卸载，我至今都记得，测试同事问我，能不能不要每次都弹个黑窗口出来，这一起想起来还真是让人感慨啊。好了，下面我们开始今天的实践吧！首先，第一步自然是安装 Hangfire 啦，这里我们新建一个 ASP.",
            "content": "Hi，大家好，我是 Payne，欢迎大家一如既往地关注我的博客。今天这篇博客里的故事背景，来自我工作中的一次业务对接，因为客户方提供的是长达上百行的 XML，所以一度让更喜欢使用 JSON 的博主感到沮丧，我这里不是想讨论 XML 和 JSON 彼此的优缺点，而是我不明白 AJAX 里的 X 现在基本都被 JSON 替代了，为什么还有这么多的人坚持使用并友好的 XML 作为数据的交换协议呢？也许你会说，因为有这样或者那样等等的理由，就像 SOA、ESB、SAP 等等类似的技术在企业级用户依然大量流行一样，而这些正是“消费”XML 的主力军。我真正想说的是，在对接这类接口时，我们会遇到一个异步化的 HTTP 协议场景，这里的异步和多线程、async/await 没有直接关系，因为它描述的实际上是业务流程上的一种“异步”。\n引子-想对 XML 说不 我们知道，HTTP 协议是一个典型的请求-响应模型，由调用方(Client)调用服务提供者(Server)提供的接口，在理想状态下，后者在处理完请求后会直接返回结果。可是当后者面对的是一个“耗时”任务时，这种方式的问题就立马凸显出来，此时调用者有两个选择：一直等对方返回直至超时(同步)、隔一会儿就看看对方是否处理完了(轮询)。这两种方式，相信大家都非常熟悉了，如果继续延伸下去，我们会联想到长/短轮询、SignalR、WebSocket。其实，更好的方式是，我们接收到一个“耗时”任务时，立即返回表明我们接收了任务，等任务执行完以后再通知调用者，这就是我们今天要说的 HTTP 异步化方案。因为对接过程中，客户采用的就是这种方案，ESB 这类消息总线本身就提供了这种功能，可作为调用方的博主就非常难受啦，因为明明能“同步”地处理完的事情，现在全部要变成“异步”处理，就像一个习惯了 async/await 语法糖的人，突然间就要重新开始写 APM 风格的代码，宝宝心里苦啊，“异步”处理就异步处理嘛，可要按人家要求去返回上百行的 XML，博主表示想死的心都有了好嘛……\n好了，吐槽归吐槽，吐槽完我们继续梳理下 HTTP 异步化的方案，这种方式在现实生活中还是相当普遍的，毕竟人类都是“异步”做事，譬如“等你哪天有空一起吃个饭”，测试同事对我说得最多的话就是，“等你这个 Bug 改完了同我说一声”，更不用说，JavaScript 里典型的异步单线程的应用等等……实现“异步”的思路其实是非常多的，比如同样在 JavaScript 里流行的回调函数，比如通过一张中间表存起来，比如推送消息到消息队列里。在面向数据库编程的时候，我听到最多的话就是，没有什么问题是不能用一张中间表来解决的，如果一张不行那就用两张。项目上我是用 Quartz+中间表的方式实现的，因为这是最为普通的方式。这里，我想和大家分享下，关于使用 Hangfire 来实现类似 Quartz 定时任务的相关内容，果然，我这次又做了一次标题党呢，希望大家会对今天的内容感兴趣。简单来说，我们会提供一个接口，调用方提供参数和回调地址，调用后通过 Hangfire 创建后台任务，等任务处理结束后，再通过回调地址返回结果给调用方，这就是所谓的 HTTP 异步化。\n开箱即用的 Hangfire 我们项目上是使用 Quartz 来实现后台任务的，因为它采用了反射的方式来调用具体的 Job，因此，它的任务调度和任务实现是耦合在同一个项目里的，常常出现单个 Job 引发整个系统卡顿的情况，尤其是是它的触发器，常常导致一个 Job 停都停不下来，直到后来才渐渐开始通过 Web API 来分离这两个部分。Quartz 几乎没有一个自己的可视化界面，我们为此专门为它开发了一套 UI。我这里要介绍的 Hangfire，可以说它刚好可以作为 Quartz 的替代品，它是一个开箱即用的、轻量级的、开源后台任务系统，想想以前为 Windows 开发定时任务，只能通过定时器(Timer)来实现，尚不知道 CRON 为何物，而且只能用命令行那种拙劣的方式来安装/卸载，我至今都记得，测试同事问我，能不能不要每次都弹个黑窗口出来，这一起想起来还真是让人感慨啊。好了，下面我们开始今天的实践吧！首先，第一步自然是安装 Hangfire 啦，这里我们新建一个 ASP.NET Core 的 Web API 项目就好，然后通过 NuGet 依次安装以下库：\nInstall-Package HangFire Install-Package Hangfire.MySql.Core 这里我们选择了 MySQL 来实现任务的持久化，从官方的流程图中可以了解到，Hangfire 有服务端、持久化存储和客户端三大核心部件组成，而持久化存储这块儿，除了官方默认的 SQLServer(可以集成 MSMQ)以外，还支持 Redis、MongoDB 等，Hangfire 使用起来是非常简单哒，首先在 Startup 类的 ConfigureServices()方法中注入 Hangfire 相关的服务，然后在 Configure()方法中使用 HangfireServer 和 UseHangfireDashboard 即可：\npublic void ConfigureServices (IServiceCollection services) { //为了简化说明，已忽略该方法中无关的代码 services.AddHangfire (x =\u0026gt; x.UseStorage (new MySqlStorage (Configuration.GetConnectionString (\u0026#34;Hangfire\u0026#34;))) .UseFilter (new HttpJobFilter ()) .UseSerilogLogProvider () ); } public void Configure (IApplicationBuilder app, IHostingEnvironment env) { //为了简化说明，已忽略该方法中无关的代码 app.UseHangfireServer (new BackgroundJobServerOptions () { Queues = new string[] { \u0026#34;default\u0026#34; }, WorkerCount = 5, ServerName = \u0026#34;default\u0026#34;, }); app.UseHangfireDashboard (); app.ApplicationServices.GetService\u0026lt;ILoggerFactory\u0026gt; ().AddSerilog (); } 注意到在配置持久化的部分，我们使用了一个数据库连接字符串 Hangfire，它需要我们在 appsettings.json 中配置 ConnectionStrings 部分。这里我们为 Hangfire 设置了默认队列 default、默认服务器 default、并发数目为 5。与此同时，我们开启了 Hangfire 中自带的 Dashboard，可以通过这个界面来监控后台任务的执行情况。此时运行项目，输入以下地址：http://locahost:/hangfire，就会看到下面的画面，这说明 Hangfire 配置成功：\nHangfire Dashboard\rHangfire 中默认支持四种类型的后台任务，他们分别是Fire-and-forget jobs、Delayed jobs、Recurring jobs和Continuations。严格来说，Fire-and-forget jobs和Delayed jobs并不能算后台任务，因为它们在执行一次后就会从队列中移除，属于一次性“消费”的任务，这两者的不同在于Delayed jobs可以在设定的时间上延迟执行。而Recurring jobs和Continuations则是周期性任务，任务在入队后可以按照固定的时间间隔去执行，周期性任务都是支持 CRON 表达式的，Continuations类似于 Task 中的 ContinueWith()方法，可以对多个任务进行组合，我们现在的项目中开发了大量基于 Quartz 的 Job，可当你试图把这些 Job 相互组合起来的时候，你就会觉得相当尴尬，因为后台任务做所的事情往往都是大同小异的。从官方文档中 ，我们会发现 Hangfire 的关键代码只有下面这四行代码，可以说是相当简洁啦！\n//Fire-and-forget jobs var jobId = BackgroundJob.Enqueue( () =\u0026gt; Console.WriteLine(\u0026#34;Fire-and-forget!\u0026#34;)); //Delayed jobs var jobId = BackgroundJob.Schedule( () =\u0026gt; Console.WriteLine(\u0026#34;Delayed!\u0026#34;), TimeSpan.FromDays(7)); //Recurring jobs RecurringJob.AddOrUpdate( () =\u0026gt; Console.WriteLine(\u0026#34;Recurring!\u0026#34;), Cron.Daily); //Continuations BackgroundJob.ContinueWith( jobId, () =\u0026gt; Console.WriteLine(\u0026#34;Continuation!\u0026#34;)); Hangfire 除了这种偏函数式风格的用法以外，同样提供了泛型版本的用法，简而言之，泛型版本是自带依赖注入的版本。众所周知，稍微复杂点的功能，常常会依赖多个服务，比如后台任务常常需要给相关人员发邮件或者是消息，此时，Job 的实现就会依赖 MailService 和 MessageService。Hangfire 内置了基于 Autofac 的 IoC 容器，因此，当我们使用泛型版本时，它可以自动地从容器中 Resolve 相应的类型出来。事实上，我们可以通过重写 JobActivator 来实现自己的依赖注入，譬如博主就喜欢 Castle。下面是一个简单的例子：\n//Define a class depends on IDbContext \u0026amp; IEmailService public class EmailSender { private IDbContext _dbContext; private IEmailService _emailService; public EmailSender() { _dbContext = new DbContext(); _emailService = new EmailService(); } // ... } //When it is registered in Ioc Container BackgroundJob.Enqueue\u0026lt;EmailSender\u0026gt;(x =\u0026gt; x.Send(\u0026#34;Joe\u0026#34;, \u0026#34;Hello!\u0026#34;)); 可扩展的 Hangfire OK，在对 Hangfire 有了一个初步的了解以后，我们再回到本文的题目，我们希望实现一个基于 HTTP 方式调用的 HttpJob。因为我们不希望任务调度和具体任务放在一起，我们项目上采用 Quartz 来开发后台任务，它要求我们实现一个特定接口 IbaseJob，最终任务调度时会通过反射来创建 Job，就在刚刚过去的这周里，测试同事向我反馈了一个 Bug，而罪魁祸首居然是因为某个 DLL 没有分发，所以，我希望实现一个基于 HTTP 方式调用的 HttpJob，这既是为了将任务调度和具体任务分离，同时为了满足这篇文章开头描述的场景，得益于 Hnagfire 良好的扩展性，我们提供了一组 Web API，代码如下：\n/// \u0026lt;summary\u0026gt; /// 添加一个任务到队列并立即执行 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;jobDescriptor\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; [HttpPost (\u0026#34;AddEnqueue\u0026#34;)] public JsonResult Enqueue (HttpJobDescriptor jobDescriptor) { try { var jobId = string.Empty; jobId = BackgroundJob.Enqueue (() =\u0026gt; HttpJobExecutor.DoRequest (jobDescriptor)); return new JsonResult (new { Flag = true, Message = $\u0026#34;Job:#{jobId}-{jobDescriptor.JobName}已加入队列\u0026#34; }); } catch (Exception ex) { return new JsonResult (new { Flag = false, Message = ex.Message }); } } /// \u0026lt;summary\u0026gt; /// 添加一个延迟任务到队列 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;jobDescriptor\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; [HttpPost (\u0026#34;AddSchedule\u0026#34;)] public JsonResult Schedule ([FromBody] HttpJobDescriptor jobDescriptor) { try { var jobId = string.Empty; jobId = BackgroundJob.Schedule (() =\u0026gt; HttpJobExecutor.DoRequest (jobDescriptor), TimeSpan.FromMinutes ((double) jobDescriptor.DelayInMinute)); return new JsonResult (new { Flag = true, Message = $\u0026#34;Job:#{jobId}-{jobDescriptor.JobName}已加入队列\u0026#34; }); } catch (Exception ex) { return new JsonResult (new { Flag = false, Message = ex.Message }); } } /// \u0026lt;summary\u0026gt; /// 添加一个定时任务 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;jobDestriptor\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; [HttpPost (\u0026#34;AddRecurring\u0026#34;)] public JsonResult Recurring ([FromBody] HttpJobDescriptor jobDescriptor) { try { var jobId = string.Empty; RecurringJob.AddOrUpdate (jobDescriptor.JobName, () =\u0026gt; HttpJobExecutor.DoRequest (jobDescriptor), jobDescriptor.Corn, TimeZoneInfo.Local); return new JsonResult (new { Flag = true, Message = $\u0026#34;Job:{jobDescriptor.JobName}已加入队列\u0026#34; }); } catch (Exception ex) { return new JsonResult (new { Flag = false, Message = ex.Message }); } } /// \u0026lt;summary\u0026gt; /// 删除一个定时任务 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;jobName\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; [HttpDelete (\u0026#34;DeleteRecurring\u0026#34;)] public JsonResult Delete (string jobName) { try { RecurringJob.RemoveIfExists (jobName); return new JsonResult (new { Flag = true, Message = $\u0026#34;Job:{jobName}已删除\u0026#34; }); } catch (Exception ex) { return new JsonResult (new { Flag = false, Message = ex.Message }); } } /// \u0026lt;summary\u0026gt; /// 触发一个定时任务 /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;jobName\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; [HttpGet (\u0026#34;TriggerRecurring\u0026#34;)] public JsonResult Trigger (string jobName) { try { RecurringJob.Trigger (jobName); return new JsonResult (new { Flag = true, Message = $\u0026#34;Job:{jobName}已触发执行\u0026#34; }); } catch (Exception ex) { return new JsonResult (new { Flag = false, Message = ex.Message }); } } /// \u0026lt;summary\u0026gt; /// 健康检查 /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; [HttpGet (\u0026#34;HealthCheck\u0026#34;)] public IActionResult HealthCheck () { var serviceUrl = Request.Host; return new JsonResult (new { Flag = true, Message = \u0026#34;All is Well!\u0026#34;, ServiceUrl = serviceUrl, CurrentTime = DateTime.Now }); } 你可以注意到，这里用到其实还是四种后台任务，在此基础上增加了删除 Job 和触发 Job 的接口，尤其是触发 Job 执行的接口，可以弥补 Quartz 的不足，很多时候，我们希望别人调了接口后触发后台任务，甚至希望在编写 Job 的过程中使用依赖注入，因为种种原因，实施起来总感觉有点碍手碍脚。这里我们定义了一个 HttpJobExecutor 的类，顾名思义，它是执行 Http 请求的一个类，说来惭愧，我写作这篇博客时，是一边看文档一边写代码的，所以，等我实现了这里的 HttpJobExecutor 的时候，我忽然发现文档中关于依赖注入的内容，简直相见恨晚啊。这里直接给出它的实现，我要再一次安利 RestSharp 这个库，比 HttpWebRequest、HttpClient 这两套官方的 API 要好用许多，可还是有人喜欢一遍又一遍地封装啊，话说自从我们把 WCF 换成 Web API 后，看着相关同事在 Git 上的折腾历史，果然还是回到了写 Http Client 的老路上来，话说在纠结是手写代理还是动态代理的时候，Retrofit 了解下啊！\n[HttpJobFilter] public static void DoRequest (HttpJobDescriptor jobDestriptor) { var client = new RestClient (jobDestriptor.HttpUrl); var httpMethod = (object) Method.POST; if (!Enum.TryParse (typeof (Method), jobDestriptor.HttpMethod.ToUpper (), out httpMethod)) throw new Exception ($\u0026#34;不支持的HTTP动词：{jobDestriptor.HttpMethod}\u0026#34;); var request = new RestRequest ((Method) httpMethod); if (jobDestriptor.JobParameter != null) { var json = JsonConvert.SerializeObject (jobDestriptor.JobParameter); request.AddParameter (\u0026#34;application/json\u0026#34;, json, ParameterType.RequestBody); } var response = client.Execute (request); if (response.StatusCode != HttpStatusCode.OK) throw new Exception ($\u0026#34;调用接口{jobDestriptor.HttpUrl}失败，接口返回：{response.Content}\u0026#34;); } 在这里，我们以 HealthCheck 这个接口为例，来展示 HttpJob 是如何工作的。顾名思义，这是一个负责健康检查的接口。我们现在通过 Postman 来触发健康检查这个后台任务。在这里，该接口是一个 GET 请求：\n通过Postman创建后台任务\r接下来，我们我们就会在 Hangfire 的 Dashborad 中找到对应的记录，因为这是一个Fire \u0026amp; Forget类型的任务，因此我们几乎看不到中间的过程，它就已经执行结束啦。我们可以在 Dashboard 中找到对应的任务，然后了解它的具体执行情况。值得一提的是，Hangfire 自带了重试机制，对于执行失败的任务，我们可以重试栏目下看到，这里是其中一条任务的执行记录。可以注意到，Hangfire 会把每个 Job 的参数序列化为 JSON 并持久化起来，仔细对照的话，你会发现，它和我们在 Postman 中传入的参数是完全一样的！\nHangfire中Job执行详情查看\r在执行 Job 的过程中，我们可能会希望记录 Job 执行过程中的日志。这个时候，Hangfire 强大的扩展性再次我们提供了这种可能性。注意到在 HttpJobExecutor 类上有一个 [HttpJobFilter]的标记，显然这是由 Hangfire 提供的一个过滤器，博主在这个过滤器中对 Job 的 ID、状态等做了记录，因为在整个项目中博主已经配置了 Serilog 作为 Hangfire 的 LogProvider，所以，我们可以在过滤器中使用 Serilog 来记录日志，不过博主个人感觉这个 Filtre 稍显鸡肋，这里还是给出代码片段吧！\npublic class HttpJobFilter : JobFilterAttribute, IApplyStateFilter { private static readonly ILog Logger = LogProvider.GetCurrentClassLogger (); public void OnStateApplied (ApplyStateContext context, IWriteOnlyTransaction transaction) { if (context.NewState is FailedState) { var failedState = context.NewState as FailedState; if (failedState != null) { Logger.ErrorException ( String.Format (\u0026#34;Background Job #{0} 执行失败。\u0026#34;, context.BackgroundJob.Id), failedState.Exception); } } else { Logger.InfoFormat ( \u0026#34;当前执行的Job为：#{0}, 状态为：{1}。\u0026#34;, context.BackgroundJob.Id, context.NewState.Name ); } } public void OnStateUnapplied (ApplyStateContext context, IWriteOnlyTransaction transaction) { } } 为什么我说这个 Filter 有点鸡肋呢？因为你看下面的图就会明白了啊！\n使用Serilog记录日志\r本文小结 果然，我还是不得不承认，这又是一篇彻彻底底的\u0026quot;水文\u0026quot;啊,因为写着写着就发现自己变成了标题党。这篇文章总结下来其实只有两句话，一个不喜欢写 XML 报文的博主，如何与 ERP、SAP、ESB 里的 XML 报文斗智斗勇的故事，在这样一个背景下，为了满足对方的\u0026quot;异步\u0026quot;场景, 不得不引入一个后台任务系统来处理这些事情，其实，这个事情用消息队列、用 Redis、甚至普通的中间表都能解决，可惜我写这篇文章的时候，是有一点个人化的情绪在里面的，这种情绪化导致的后果就是，可能我越来越难以控制一篇文章的写作走向啦，大概是写东西越来越困难，而又没有时间取吸收新的知识进来，这让我觉得自己的进步越来越少，Hangfire 的有点说起来就是挺好用的，以上！\n",
            "tags": [".NET Core","Hangfire","HTTP"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "使用 .NET Core 和 Vue 搭建 WebSocket 聊天室",
            "date": "2018-08-01",
            "permalink": "http://localhost:1313/posts/1989654282/",
            "summary": "Hi，大家好，我是Payne，欢迎大家关注我的博客，我的博客地址是：https://qinyuanpei.github.io。今天这篇博客，我们来说说WebSocket。各位可能会疑惑，为什么我会突然间对WebSocket感兴趣，这是因为最近接触到了部分“实时”的业务场景，譬如：用户希望在远程视频通话过程中，实时地监控接入方的通话状态，实时地将接入方的响应时间、通话时长以及接通率等信息推送到后台。与此同时，用户可以通过监控平台看到实时变化着的图表。坦白地讲，这种业务场景陌生吗？不，每一年的双11，都能见到小伙伴们实时地“剁手”。所以，在今天这篇文章中，我们会以WebSocket聊天室为例，来讲解如何基于WebSocket构建实时应用。\nWebSocket概述 WebSocket是HTML5标准中的一部分，从Socket这个字眼我们就可以知道，这是一种网络通信协议。WebSocket是为了弥补HTTP协议的不足而产生的，我们知道，HTTP协议有一个重要的缺陷，即：请求只能由客户端发起。这是因为HTTP协议采用了经典的请求-响应模型，这就限制了服务端主动向客户端推送消息的可能。与此同时，HTTP协议是无状态的，这意味着连接在请求得到响应以后就关闭了，所以，每次请求都是独立的、上下文无关的请求。这种单向请求的特点，注定了客户端无法实时地获取服务端的状态变化，如果服务端的状态发生连续地变化，客户端就不得不通过“轮询”的方式来获知这种变化。毫无疑问，轮询的方式不仅效率低下，而且浪费网络资源，在这种背景下，WebSocket应运而生。\nWebSocket协议最早于2008年被提出，并于2011年成为国际标准。目前，主流的浏览器都已经提供了对WebSocket的支持。在WebSocket协议中，客户端和服务器之间只需要做一次握手操作，就可以在客户端和服务器之间实现双向通信，所以，WebSocket可以作为**服务器推送**的实现技术之一。因为它本身以HTTP协议为基础，所以对HTTP协议有着更好的兼容性，无论是通信效率还是传输的安全性都能得到保证。WebSocket没有同源限制，客户端可以和任意服务器端进行通信，因此具备通过一个单一连接来支持上下游通信的能力。从本质上来讲，WebSocket是一个在握手阶段使用HTTP协议的TCP/IP协议，换句话说，一旦握手成功，WebSocket就和HTTP协议再无瓜葛，下图展示了它与HTTP协议的区别：\nHTTP与WebSocket的区别\r构建一个聊天室 OK，在对WebSocket有了一个基本的认识以后，接下来，我们以一个最简单的场景来体验下WebSocket。这个场景是什么呢？你已经知道了，答案就是网络聊天室。这是一个非常典型的实时场景。这里我们分为服务端实现和客户端实现，其中：服务端实现自豪地采用.NET Core，而客户端实现采用Vue的双向绑定特性。现在是公元2018年了，当jQuery已成往事，操作DOM这种事情交给框架去做就好，而且我本人很喜欢MVVM这种模式，Vue的渐进式框架，非常适合我这种不会写ES6的伪前端。\n.NET Core与中间件 关于.NET Core中对WebSocket的支持，这里主要参考了官方文档，在这篇文档中，演示了一个最基本的Echo示例，即服务端如何接收客户端消息并返回消息给客户端。这里，我们首先需要安装Microsoft.AspNetCore.WebSockets这个库，直接通过Visual Studio Code内置的终端安装即可。接下来，我们需要在Startup类的Configure方法中添加WebSocket中间件：\napp.UseWebSockets() 更一般地，我们可以配置以下两个配置，其中，KeepAliveInterval表示向客户端发送Ping帧的时间间隔；ReceiveBufferSize表示接收数据的缓冲区大小：\nvar webSocketOptions = new WebSocketOptions() { KeepAliveInterval = TimeSpan.FromSeconds(120), ReceiveBufferSize = 4 * 1024 }; app.UseWebSockets(webSocketOptions); 好了，那么怎么接收一个来自客户端的请求呢？这里以官方文档中的示例代码为例来说明。首先，我们需要判断下请求的地址，这是客户端和服务端约定好的地址，默认为**/，这里我们以/ws为例；接下来，我们需要判断当前的请求上下文是否为WebSocket请求，通过context.WebSockets.IsWebSocketRequest来判断。当这两个条件同时满足时，我们就可以通过context.WebSockets.AcceptWebSocketAsync()**方法来得到WebSocket对象，这样就表示“握手”完成，这样我们就可以开始接收或者发送消息啦。\nif (context.Request.Path == \u0026#34;/ws\u0026#34;) { if (context.WebSockets.IsWebSocketRequest) { WebSocket webSocket = await context.WebSockets.AcceptWebSocketAsync(); //TODO } }); 一旦建立了Socket连接，客户端和服务端之间就可以开始通信，这是我们从Socket中收获的经验，这个经验同样适用于WebSocket。这里分别给出WebSocket发送和接收消息的实现，并针对代码做简单的分析。\nprivate async Task SendMessage\u0026lt;TEntity\u0026gt;(WebSocket webSocket, TEntity entity) { var Json = JsonConvert.SerializeObject(entity); var bytes = Encoding.UTF8.GetBytes(Json); await webSocket.SendAsync( new ArraySegment\u0026lt;byte\u0026gt;(bytes), WebSocketMessageType.",
            "content": " Hi，大家好，我是Payne，欢迎大家关注我的博客，我的博客地址是：https://qinyuanpei.github.io。今天这篇博客，我们来说说WebSocket。各位可能会疑惑，为什么我会突然间对WebSocket感兴趣，这是因为最近接触到了部分“实时”的业务场景，譬如：用户希望在远程视频通话过程中，实时地监控接入方的通话状态，实时地将接入方的响应时间、通话时长以及接通率等信息推送到后台。与此同时，用户可以通过监控平台看到实时变化着的图表。坦白地讲，这种业务场景陌生吗？不，每一年的双11，都能见到小伙伴们实时地“剁手”。所以，在今天这篇文章中，我们会以WebSocket聊天室为例，来讲解如何基于WebSocket构建实时应用。\nWebSocket概述 WebSocket是HTML5标准中的一部分，从Socket这个字眼我们就可以知道，这是一种网络通信协议。WebSocket是为了弥补HTTP协议的不足而产生的，我们知道，HTTP协议有一个重要的缺陷，即：请求只能由客户端发起。这是因为HTTP协议采用了经典的请求-响应模型，这就限制了服务端主动向客户端推送消息的可能。与此同时，HTTP协议是无状态的，这意味着连接在请求得到响应以后就关闭了，所以，每次请求都是独立的、上下文无关的请求。这种单向请求的特点，注定了客户端无法实时地获取服务端的状态变化，如果服务端的状态发生连续地变化，客户端就不得不通过“轮询”的方式来获知这种变化。毫无疑问，轮询的方式不仅效率低下，而且浪费网络资源，在这种背景下，WebSocket应运而生。\nWebSocket协议最早于2008年被提出，并于2011年成为国际标准。目前，主流的浏览器都已经提供了对WebSocket的支持。在WebSocket协议中，客户端和服务器之间只需要做一次握手操作，就可以在客户端和服务器之间实现双向通信，所以，WebSocket可以作为**服务器推送**的实现技术之一。因为它本身以HTTP协议为基础，所以对HTTP协议有着更好的兼容性，无论是通信效率还是传输的安全性都能得到保证。WebSocket没有同源限制，客户端可以和任意服务器端进行通信，因此具备通过一个单一连接来支持上下游通信的能力。从本质上来讲，WebSocket是一个在握手阶段使用HTTP协议的TCP/IP协议，换句话说，一旦握手成功，WebSocket就和HTTP协议再无瓜葛，下图展示了它与HTTP协议的区别：\nHTTP与WebSocket的区别\r构建一个聊天室 OK，在对WebSocket有了一个基本的认识以后，接下来，我们以一个最简单的场景来体验下WebSocket。这个场景是什么呢？你已经知道了，答案就是网络聊天室。这是一个非常典型的实时场景。这里我们分为服务端实现和客户端实现，其中：服务端实现自豪地采用.NET Core，而客户端实现采用Vue的双向绑定特性。现在是公元2018年了，当jQuery已成往事，操作DOM这种事情交给框架去做就好，而且我本人很喜欢MVVM这种模式，Vue的渐进式框架，非常适合我这种不会写ES6的伪前端。\n.NET Core与中间件 关于.NET Core中对WebSocket的支持，这里主要参考了官方文档，在这篇文档中，演示了一个最基本的Echo示例，即服务端如何接收客户端消息并返回消息给客户端。这里，我们首先需要安装Microsoft.AspNetCore.WebSockets这个库，直接通过Visual Studio Code内置的终端安装即可。接下来，我们需要在Startup类的Configure方法中添加WebSocket中间件：\napp.UseWebSockets() 更一般地，我们可以配置以下两个配置，其中，KeepAliveInterval表示向客户端发送Ping帧的时间间隔；ReceiveBufferSize表示接收数据的缓冲区大小：\nvar webSocketOptions = new WebSocketOptions() { KeepAliveInterval = TimeSpan.FromSeconds(120), ReceiveBufferSize = 4 * 1024 }; app.UseWebSockets(webSocketOptions); 好了，那么怎么接收一个来自客户端的请求呢？这里以官方文档中的示例代码为例来说明。首先，我们需要判断下请求的地址，这是客户端和服务端约定好的地址，默认为**/，这里我们以/ws为例；接下来，我们需要判断当前的请求上下文是否为WebSocket请求，通过context.WebSockets.IsWebSocketRequest来判断。当这两个条件同时满足时，我们就可以通过context.WebSockets.AcceptWebSocketAsync()**方法来得到WebSocket对象，这样就表示“握手”完成，这样我们就可以开始接收或者发送消息啦。\nif (context.Request.Path == \u0026#34;/ws\u0026#34;) { if (context.WebSockets.IsWebSocketRequest) { WebSocket webSocket = await context.WebSockets.AcceptWebSocketAsync(); //TODO } }); 一旦建立了Socket连接，客户端和服务端之间就可以开始通信，这是我们从Socket中收获的经验，这个经验同样适用于WebSocket。这里分别给出WebSocket发送和接收消息的实现，并针对代码做简单的分析。\nprivate async Task SendMessage\u0026lt;TEntity\u0026gt;(WebSocket webSocket, TEntity entity) { var Json = JsonConvert.SerializeObject(entity); var bytes = Encoding.UTF8.GetBytes(Json); await webSocket.SendAsync( new ArraySegment\u0026lt;byte\u0026gt;(bytes), WebSocketMessageType.Text, true, CancellationToken.None ); } 这里我们提供一个泛型方法，它负责对消息进行序列化并转化为byte[]，最终调用**SendAsync()方法发送消息。与之相对应地，客户端会在onmessage()**回调中就会接受到消息，这一点我们放在后面再说。WebSocket接收消息的方式，和传统的Socket非常相似，我们需要将字节流循环读取到一个缓存区里，直至所有数据都被接收完。下面给出基本的代码示例：\nvar buffer = new ArraySegment\u0026lt;byte\u0026gt;(new byte[bufferSize]); var result = await webSocket.ReceiveAsync(buffer, CancellationToken.None); while (!result.EndOfMessage) { result = await webSocket.ReceiveAsync(buffer, default(CancellationToken)); } var json = Encoding.UTF8.GetString(buffer.Array); json = json.Replace(\u0026#34;\\0\u0026#34;, \u0026#34;\u0026#34;).Trim(); return JsonConvert.DeserializeObject\u0026lt;TEntity\u0026gt;(json, new JsonSerializerSettings() { DateTimeZoneHandling = DateTimeZoneHandling.Local }); 虽然不大清楚，为什么这里反序列化后的内容中会有大量的**\\0**，以及这个全新的类型ArraySegment到底是个什么鬼，不过程序员的一生无非都在纠结这样两个问题，“it works” 和 “it doesn\u0026rsquo;t works\u0026quot;，就像人生里会让你纠结的无非是”她喜欢你“和”她不喜欢我“这样的问题。有时候，这样的问题简直就是玄学，五柳先生好读书而不求甚解，我想这个道理在这里同样适用，截止到我写这篇博客前，这个代码一直工作得很好，所以，这两个问题我们可以暂时先放在一边，因为眼下还有比这更为重要的事情。\n通过这篇文档，我们可以非常容易地构建出一个”实时应用“，可是它离我们这篇文章中的目标依然有点距离，如果各位足够细心的话，就会发现这样一个问题，即示例中的代码都是写在app.Use()方法中的，这样会使我们的Startup类显得臃肿，而熟悉OWIN或者ASP.NET Core的朋友，就会知道Startup类是一个非常重要的东西，我们通常会在这里配置相关的组件。在ASP.NET Core中，我们可以通过Configure()方法来为IApplicationBuilder增加相关组件，这种组件通常被称为中间件。那么，什么是中间件呢？\n中间件示意图\r从这张图中可以看出，中间件实际上是指在HTTP请求管道中处理请求和响应的组件，每个组件都可以决定是否要将请求传递给下一个组件，比如身份认证、日志记录就是最为常见的中间件。在ASP.NET Core中，我们通过app.Use()方法来定义一个Func\u0026lt;RequestDelegate,RequestDelegate\u0026gt;类型的参数，所以，我们可以简单地认为，在ASP.NET Core中，Func\u0026lt;RequestDelegate,RequestDelegate\u0026gt;就是一个中间件，而通过app.Use()方法，这些中间件会根据注册的先后顺序组成一个链表，每一个中间件的输入是上一个中间件的输出，每一个中间件的输出则会成为下一个中间件的输入。简而言之，每一个RequestDelegate对象不仅包含了自身对请求的处理，而且包含了后续中间件对请求的处理，我们来看一个简单的例子：\napp.Use(async (context,next)=\u0026gt; { await context.Response.WriteAsync(\u0026#34;这是第一个中间件\\r\\n\u0026#34;); await next(); }); app.Use(async (context,next)=\u0026gt; { await context.Response.WriteAsync(\u0026#34;这是第二个中间件\\r\\n\u0026#34;); await next(); }); app.Use(async (context,next)=\u0026gt; { await context.Response.WriteAsync(\u0026#34;这是第三个中间件\\r\\n\u0026#34;); await next(); }); 通过Postman或者任意客户端发起请求，我们就可以得到下面的结果，现在想象一下，如果我们在第一种中间件中不调用next()会怎么样呢？答案是中间件之间的链路会被打断，这意味着后续的第二个、第三个中间件都不会被执行。什么时候我们会遇到这种场景呢？当我们的认证中间件认为一个请求非法的时候，此时我们不应该让用户访问后续的资源，所以直接返回403对该请求进行拦截。在大多数情况下，我们需要让请求随着中间件的链路传播下去，所以，对于每一个中间件来说，除了完成自身的处理逻辑以外，还至少需要调用一次next()，以保证下一个中间件会被调用，这其实和职责链模式非常相近，可以让数据在不同的处理管道中进行传播。\nASP.NET Core中间件示例\rOK，这里我们继续遵从这个约定，将整个聊天室相关的逻辑写到一个中间件里，这样做的好处是，我们可以将不同的WebSocket互相隔离开，同时可以为我们的Startup类”减负“。事实证明，这是一个正确的决定，在开发基于WebSocket的弹幕功能时，我们就是用这种方式开发了新的中间件。这里，我们给出的是WebSocketChat中间件中最为关键的部分，详细的代码我已经放在Github上啦，大家可以参考WebSocketChat类，其基本原理是：使用一个字典来存储每一个聊天室中的会话(Socket)，当用户打开或者关闭一个WebSocket连接时，会向服务器端发送一个事件(Event)，这样客户端中持有的用户列表将被更新，而根据发送的消息，可以决定这条消息是被发给指定联系人还是群发：\npublic async Task Invoke(HttpContext context) { if (!IsWebSocket(context)) { await _next.Invoke(context); return; } var userName = context.Request.Query[\u0026#34;username\u0026#34;].ToArray()[0]; var webSocket = await context.WebSockets.AcceptWebSocketAsync(); while (webSocket.State == WebSocketState.Open) { var entity = await Receiveentity\u0026lt;MessageEntity\u0026gt;(webSocket); switch (entity.Type) { case MessageType.Chat: await HandleChat(webSocket, entity); break; case MessageType.Event: await HandleEvent(webSocket, entity); break; } } await webSocket.CloseAsync(WebSocketCloseStatus.NormalClosure, \u0026#34;Close\u0026#34;, default(CancellationToken)); } 其中，HandleEvent负责对事件进行处理，HandleChat负责对消息进行处理。当有用户加入聊天室的时候，首先会向所有客户端广播一条消息，告诉大家有新用户加入了聊天室，与此同时，为了让大家可以和新用户进行通信，必须将新的用户列表推送到客户端。同理，当有用户离开聊天室的时候，服务器端会有类似的事件推送到客户端。事件同样是基于消息来实现的，不过这两种采用的数据结构不同，具体大家可以通过源代码来了解。发送消息就非常简单啦，给指定用户发送消息是通过用户名来找WebSocket对象，而群发消息就是遍历字典中的所有WebSocket对象，这一点我们不再详细说啦！\nVue驱动的客户端 在实现服务端的WebSocket以后，我们就可以着手客户端的开发啦！这里我们采用原生的WebSocket API来开发相关功能。具体来讲，我们只需要实例化一个WebSocket类，并设置相应地回调函数就可以了，我们一起来看下面的例子：\nvar username = \u0026#34;PayneQin\u0026#34; var websocket = new WebSocket(\u0026#34;ws://localhost:8002/ws?username=\u0026#34; + username); 这里我们使用**/s这个路由来访问WebSocket，相应地，在服务端代码中我们需要判断context.Request.Path**，WebSocket在握手阶段是基于HTTP协议的，所以我们可以以QueryString的形式给后端传递一个参数，这里我们需要一个用户名，它将作为服务端存储WebSocket时的一个键。一旦建立了WebSocket，我们就可以通过回调函数来监听服务器端的响应，或者是发送消息给服务器端。主要的回调函数有onopen、onmessage、onerror和onclose四个，基本使用方法如下：\nwebsocket.onopen = function () { console.log(\u0026#34;WebSocket连接成功\u0026#34;); }; websocket.onmessage = function (event) { console.log(\u0026#34;接收到服务端消息：\u0026#34; + event.data) }; websocket.onerror = function () { console.log(\u0026#34;WebSocket连接发生错误\u0026#34;); }; websocket.onclose = function () { console.log(\u0026#34;WebSocket连接关闭\u0026#34;); }; 原生的WebSocket API只有两个方法，即send()和close()，这两个方法非常的简单，我们这里不再说明。需要说明的是，客户端使用了Vue来做界面相关的绑定，作为一个不会写CSS、不会写ES6的伪前端，我做了一个相当简洁(简陋)的前端页面，下面给出主要的页面结构，ViewModel层的代码比较多，大家可以参考这里：\n\u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt; Hi，{{ username }}。欢迎来到WebSocket聊天室！ \u0026lt;hr/\u0026gt; 发送给： \u0026lt;select v-model=\u0026#34;sendTo\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;All\u0026#34;\u0026gt;全部\u0026lt;/option\u0026gt; \u0026lt;option v-for=\u0026#34;user in userList\u0026#34; :value=\u0026#34;user\u0026#34;\u0026gt;{{user}}\u0026lt;/option\u0026gt; \u0026lt;/select\u0026gt; \u0026lt;hr/\u0026gt; \u0026lt;input id=\u0026#34;text\u0026#34; type=\u0026#34;text\u0026#34; v-model=\u0026#34;message\u0026#34; /\u0026gt; \u0026lt;button v-on:click=\u0026#34;sendMessage\u0026#34;\u0026gt;发送消息\u0026lt;/button\u0026gt; \u0026lt;hr/\u0026gt; \u0026lt;button v-on:click=\u0026#34;openWebSocket\u0026#34;\u0026gt;打开WebSocket连接\u0026lt;/button\u0026gt; \u0026lt;button v-on:click=\u0026#34;closeWebSocket\u0026#34;\u0026gt;关闭WebSocket连接\u0026lt;/button\u0026gt; \u0026lt;button v-on:click=\u0026#34;clearMessageList\u0026#34;\u0026gt;清空聊天记录\u0026lt;/button\u0026gt; \u0026lt;hr/\u0026gt; \u0026lt;div id=\u0026#34;messageList\u0026#34; v-html=\u0026#34;messageList\u0026#34;\u0026gt; {{ messageList }} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 下面是实际的运行效果，果然是非常简洁呢，哈哈:laughing:\nWebSocket聊天室展示\r再看Websocket 好了，我们花了如此大的篇幅来讲WebSocket，那么你对WebSocket了解了多少呢？或许通过这个聊天室的实例，我们对WebSocket有了一个相对直观的认识，可你是否想过换一个角度来认识它呢？我们说过，WebSocket是以HTTP协议为基础的，那么至少可以在握手阶段捕获到相关请求吧！果断在Chrome中打开”开发者工具“，在面板上选择监听”WebSocket\u0026quot;，然后我们就会得到下面的内容。\nWebSocket的秘密-请求\r相比HTTP协议，WebSocket在握手阶段的请求有所变化，主要体现在Upgrade、Connection这两个字段，以及Sec-WebSocket系列的这些字段。下面来分别解释下这些字段的含义，Upgrade和Connection这两个字段，是最为关键的两个字段，它的目的是告诉Apache、Nginx这些服务器，这是一个WebSocket请求。接下来，是Sec-WebSocket-Key、Sec-WebSocket-Protocol和Sec-WebSocket-Version这三个字段，其中Sec-WebSocket-Key是一个由浏览器采用Base64算法随机生成的字符串，目的是验证服务器是否真的支持WebSocket；Sec-WebSocket-Protocol则是一个由用户指定的字符串，目的是区分同一URL下，不同服务所需要的协议；Sec-WebSocket-Version是告诉服务器浏览器支持的WebSocket版本，标准规定9-12的版本号是保留字段，所以在这里我们看到的版本号是13.\nWebSocket的秘密-响应\r那么，对于这个浏览器发起的这个请求，服务端是如何做出响应的呢？这就要来看看服务端返回的内容。 和客户端发起的请求类似，服务端返回的内容中依然会有Upgrade和Connection这两个字段，它们和请求中的含义是完全一致的。这里需要说明的是Sec-WebSocket-Accept这个字段，我们前面提到，浏览器会通过WebSocket-Key检验服务器是否真的支持WebSocket，具体怎么检验呢？是通过下面的算法。除此之外，一个特殊的地方是这个Response的状态码是101，这表示服务端说：下面我们就按照WebSocket协议来通信吧！当然，一个更为残酷的现实是，从这里开始，就不再是HTTP协议的势力范围了啊：\nsec-websocket-accept = base64(hsa1(sec-websocket-key + 258EAFA5-E914-47DA-95CA-C5AB0DC85B11)) 本文小结 这篇文章选取了“实时应用”这样一个业务场景作为切入点，引出了本文的主题——WebSocket。WebSocket是一种建立在HTTP协议基础上的双向通信协议，它弥补了以“请求-响应”模型为基础的HTTP协议先天上的不足，客户端无需再通过“轮询”这种方式来获取服务端的状态变化。WebSocket在完成“握手”后，即可以长连接的方式在客户端和服务端间构建双向通道，因而WebSocket可以在实时应用场景下，作为服务器推送技术的一种方案选择。本文以一个WebSocket聊天室的案例，来讲解WebSocket在实际项目中的应用，在这里我们使用ASP.NET Core来完成服务端WebSocket的实现，而客户端选用原生WebSocket API和Vue来实现，在此基础上，我们讲解了ASP.NET Core下中间件的概念，并将服务器端WebSocket以中间件的形式实现。在下一篇文章中，我们将偏重于服务器端的数据推送，客户端将作为数据展现层而存在。好了，以上就是这篇文章的全部内容啦，谢谢大家，让我们一起期待下一篇文章吧！\n",
            "tags": ["WebSocket",".NET Core","Vue"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "草食系程序员的穿搭指南",
            "date": "2018-07-25",
            "permalink": "http://localhost:1313/posts/94443781/",
            "summary": "最近一直在看 《逃避虽可耻但有用》(逃げるは恥だが役に立つ) 这部日剧，当我们感慨各种脑洞都满足不了人类的好奇心时，日剧依然在老老实实地讲述着故事，即使这个故事离普通人依旧很遥远。可我认为，这是一部以轻喜剧为载体的温情剧，不管你是单身、恋爱中还是已婚，你都能从这部剧中找到自己对应的部分。所以，对于这部日剧而言，我个人是推荐大家去看一看的。原谅我不肯用我贫乏的语言去评价这部电视剧，因为我相信“此中不足为外人道也”。所谓“如人饮水，冷暖自知”，感情这件事情，懂的人自然会懂，不懂的人假装懂。\n剧中男主津崎平匡是一个“典型”的程序员，因为外表无攻击性，一脸的人畜无害，而被女主森山实栗称为“草食系”男人。男主的长相在主流审美中或许谈不上帅，因为这个世界更欣赏的，是风见君这样帅气的男人。程序员群体木讷而内向的性格，其实都是大众给贴上去的标签。人们不喜欢被贴上各种标签，可人们喜欢给别人贴各种标签，因为这样子区分不同的人最省事儿。我们无法指责这个世界用五官和三观来割裂地看待一个人，我们唯一能做的，就是去改变留在人们心中的刻板印象。剧中男主在很多方面是比我们优秀的，向他学习不能保证我们会娶到 Gakki，可能让我们变得更优秀。\n好了，下面就由我带大家一起来盘点男主在剧中的穿搭，所以，这是一篇总结向的草食系程序员穿搭指南。考虑到这部剧中室内场景比室外场景更多，季节主要集中在秋冬季，所以，我们将从环境、季节、种类等多个维度，对男主在剧中的穿搭进行盘点。活在一个看脸的时代最大的悲哀就是，那些长得比你好看，明明可以靠颜值，非要靠才华的人，永远都比你更努力。虽然津崎先生经常被人说“低情商”和“屌丝”，可我相信他比我们大多数“屌丝”要优秀得多。当然，这些优点需要大家在剧中去发掘。我只是希望，通过这种方式来提升自我。面对来自这个世界的恶意，争辩是没有意义的，你只能努力去纠正这种偏见。\n室内篇 20180724012456548-101-2018725\r1、深蓝色衬衣 + 深绿色休闲裤。作为职场日常穿搭，在第一集中出现，中年已婚男士池日在男主津崎面前炫耀“爱妻便当”，高情商的田沼先生替男主解围，安慰男主要好好吃饭。建议搭配：休闲皮鞋 + 一条优质皮带。同样地，我想说的是，一个人更要好好吃饭。\n20180724012544572102-2018725\r2、蓝色衬衣 + 西裤，俨然是雇主与雇员的上下级关系。女主森山实栗通过试用期考核，指令清晰、有条不紊给女主留下良好印象。作为职场常规搭配，搭配黑框眼睛，给人一种斯文儒雅的感觉，建议根据个人肤色，选择合适的颜色，具体来讲，如果你皮肤较白，建议选择明亮的色彩；如果你皮肤较黑，建议选择中性的色彩。\n20180724012633880103-2018725\r3、因为业务需求发生变更，男主被公司安排加班，在大家的共同努力下，项目终于按时完成，男主小心翼翼地在同事面前测试程序，衬衣领口的双色纹路，避免了视觉上的枯燥感，同事们在身后欢呼，男主深藏功与名，穿一件白衬衣，幻想自己是阿泰尔，千军万马避白袍，写程序没有 Bug。\n20180724012656554104-2018725\r4、每个程序员都会有一件格子衬衫，仙剑之父姚壮宪更是穿了一辈子格子衬衫。讲道理，男主穿格子衬衫难看吗？为什么程序员穿格子衬衫和特步鞋就要被黑到异次元呢？其实，只要不是浮夸的大格子衬衫，穿起来一样萌萌哒，关键是合体！当然，只要一胖就完啦。所以，穿搭是技巧，健身是根本啊。\n20180724012839470105-2018725\r5、女人变美只需要一只口红，而男人变帅只需要一条领带。男女主决定协议结婚后，召集双方父母商议结婚事宜。一套贴合肩线的西装，搭配一件白色衬衫，视觉上给人成熟稳重的感觉，男主虽然在剧中表现得很“怂”，可这并不影响他的“帅”啊，这套衣服最多算彩排，真正的新郎礼服请关注第 11 集……(嗯，这是最后一集，日剧追起来很快呦)\n20180724012746230106-2018725\r6、简洁到不能再简洁的短袖衬衣 + 牛仔裤。前一秒的踌躇满志，同下一秒的惊慌失措，莫名地戳中萌点，明明同事就在眼前，非要学人家卷福发短信。请女生们不要再吐槽男生穿衣服“土”，你告诉我，除了长裤和短裤我们还有什么？对了，短裤是不能穿的哦……，尤其是花花绿绿的那种🙃\n20180724012906742107-2018725\r7、蓝白相间的衬衣，相比普通蓝色衬衣，平添了一种活泼的感觉，就连工牌卡的绳子都来凑热闹。你知道怎么快速从人群中识别一名程序员吗？牛仔裤 + 双肩包 + 工牌卡。不，我拒绝这种符号化的穿搭，大隐隐于市，忘了这套新手村装备吧……当然，如果你包里还是各种数据线……好像换汤不换药啊(逃\n20180724013156508108-2018725\r8、任何领域都会鄙视链的存在，像津崎先生这样优秀的工程师，自然远非某某培训班的学生们。如何做一名优雅的学院派呢？你需要一件毛衣或者是一件马甲，而且一定要套在衬衣上。你问我为什么这么穿，因为通常教授们都这样穿，请参考卷福主演的电影《模仿游戏》，负责破译德军恩尼格码密码机的专家们，都是这样的穿着，同样的，还有《万物李军》里剑桥的教授们……\n20180724013332938109-2018725\r9、同样是毛衣和衬衣的搭配，圆领和 V 领是一种风格，是否翻出衬衣领又是一种风格。而我们的男主，显然可以同时驾驭这两种风格，再搭配一件休闲外套，试问还有谁？风见君帅是帅了，不过他的衣服好像永远都是针织衫啊，难道说有钱人都喜欢买一堆一样的衣服？恩，我说的就是老乔和小扎这种有钱人……\n20180724013332940110-2018725\r10、果然，有圆领就会有 V 领，强迫症对工牌卡挂绳莫名地充满好感，这个“V”字完美地贴合衣领。针织衫和衬衣，需要有一定的层次感，比如备受我们嫌弃的格子衬衫，如果搭配针织衫效果还是非常不错的，唯一的要求或许是肩膀不能过宽，因为这样会显得整体线条僵硬。我有一个问题，像女主这样宽肩膀的女生，穿一字肩真的不怕滑下来吗？😂\n20180724013332943111-2018725\r11、这种“假领”的毛衣，穿出来同样好看，我严重怀疑，这个创意是来自上海静安区同福里的老马。如果你的脖子比较长，可以考虑尝试下高领毛衣，请注意，我不是在教你，去做一名女装大佬。话说回来，衬衣上套毛衣最大的缺点是，需要挤上衬衣最上面的扣子，所以买衬衣时，请确保可以放入两根手指，这样子不会像《杀破狼 2》里的张晋一样被“帅”死。\n20180724013332944112-1-2018725\r20180724013332946112-2-2018725\r12、这里分别是针织衫和毛背心搭配格子衬衫的正确示例，简而言之，衣服的搭配上需要体现出层次感，切忌选择色调过于接近的颜色，衬衣一定要修身，否则搭配毛衣会让你显得臃肿不堪。我要立一个 flag，等我瘦到 120 斤，我就奖励自己一件针织衫。\n20180724013332949113-1-2018725\r20180724013332951113-2-2018725\r13、毛衣和针织衫真的是搭配率超级高的优质单品，穿出来真的非常好看。我知圆领 T 恤是夏天最常见的穿搭，可如果你想尝试下不同的风格，我建议你买一件衬衣或者是 Polo 衫或者是针织衫，这些都能带给你不一样的感觉。我一直想尝试皮夹克或者是牛仔外套，可我自我感觉不适合这样硬朗的风格，谁让我是一个温柔的蓝孩纸呢……\n室外篇 20180724013619267201-2018725\r1、这种材质的衣服应该很容易脏，而且大概率会让你显得臃肿(胖)，可不得不说，这一身和女主站一起挺搭的，我们学习穿搭只有两个目的，找到女朋友和不给女朋友丢脸(🙃)。作为围巾控，这身搭配我觉得可以尝试一下。\n20180724013619269202-2018725\r2、一个男人，只要有一件合身的西装，就已经在变帅的路上迈出一大步。这一款的话，毛衣黑白两种颜色，和衬衣蓝灰白的色调蛮接近的，所以基本上看不出层次感来。其实一直不明白男主为什么如此沉闷的颜色，难道是因为向女主表白以后变成熟了吗？😂\n20180724013619273203-2018725\r3、你看，这件衣服再次发挥了格子衬衫的伟大魅力，而在这件蓝色的针织衫的衬托下，可以明显地感觉到男主变“白”了，当 90 后们开始步入中年职场，不妨尝试穿一点靓丽的颜色，因为我们还可以再年轻一下。池日先生又讲了一句“名言”，你看津崎先生这震惊的小眼神。",
            "content": "最近一直在看 《逃避虽可耻但有用》(逃げるは恥だが役に立つ) 这部日剧，当我们感慨各种脑洞都满足不了人类的好奇心时，日剧依然在老老实实地讲述着故事，即使这个故事离普通人依旧很遥远。可我认为，这是一部以轻喜剧为载体的温情剧，不管你是单身、恋爱中还是已婚，你都能从这部剧中找到自己对应的部分。所以，对于这部日剧而言，我个人是推荐大家去看一看的。原谅我不肯用我贫乏的语言去评价这部电视剧，因为我相信“此中不足为外人道也”。所谓“如人饮水，冷暖自知”，感情这件事情，懂的人自然会懂，不懂的人假装懂。\n剧中男主津崎平匡是一个“典型”的程序员，因为外表无攻击性，一脸的人畜无害，而被女主森山实栗称为“草食系”男人。男主的长相在主流审美中或许谈不上帅，因为这个世界更欣赏的，是风见君这样帅气的男人。程序员群体木讷而内向的性格，其实都是大众给贴上去的标签。人们不喜欢被贴上各种标签，可人们喜欢给别人贴各种标签，因为这样子区分不同的人最省事儿。我们无法指责这个世界用五官和三观来割裂地看待一个人，我们唯一能做的，就是去改变留在人们心中的刻板印象。剧中男主在很多方面是比我们优秀的，向他学习不能保证我们会娶到 Gakki，可能让我们变得更优秀。\n好了，下面就由我带大家一起来盘点男主在剧中的穿搭，所以，这是一篇总结向的草食系程序员穿搭指南。考虑到这部剧中室内场景比室外场景更多，季节主要集中在秋冬季，所以，我们将从环境、季节、种类等多个维度，对男主在剧中的穿搭进行盘点。活在一个看脸的时代最大的悲哀就是，那些长得比你好看，明明可以靠颜值，非要靠才华的人，永远都比你更努力。虽然津崎先生经常被人说“低情商”和“屌丝”，可我相信他比我们大多数“屌丝”要优秀得多。当然，这些优点需要大家在剧中去发掘。我只是希望，通过这种方式来提升自我。面对来自这个世界的恶意，争辩是没有意义的，你只能努力去纠正这种偏见。\n室内篇 20180724012456548-101-2018725\r1、深蓝色衬衣 + 深绿色休闲裤。作为职场日常穿搭，在第一集中出现，中年已婚男士池日在男主津崎面前炫耀“爱妻便当”，高情商的田沼先生替男主解围，安慰男主要好好吃饭。建议搭配：休闲皮鞋 + 一条优质皮带。同样地，我想说的是，一个人更要好好吃饭。\n20180724012544572102-2018725\r2、蓝色衬衣 + 西裤，俨然是雇主与雇员的上下级关系。女主森山实栗通过试用期考核，指令清晰、有条不紊给女主留下良好印象。作为职场常规搭配，搭配黑框眼睛，给人一种斯文儒雅的感觉，建议根据个人肤色，选择合适的颜色，具体来讲，如果你皮肤较白，建议选择明亮的色彩；如果你皮肤较黑，建议选择中性的色彩。\n20180724012633880103-2018725\r3、因为业务需求发生变更，男主被公司安排加班，在大家的共同努力下，项目终于按时完成，男主小心翼翼地在同事面前测试程序，衬衣领口的双色纹路，避免了视觉上的枯燥感，同事们在身后欢呼，男主深藏功与名，穿一件白衬衣，幻想自己是阿泰尔，千军万马避白袍，写程序没有 Bug。\n20180724012656554104-2018725\r4、每个程序员都会有一件格子衬衫，仙剑之父姚壮宪更是穿了一辈子格子衬衫。讲道理，男主穿格子衬衫难看吗？为什么程序员穿格子衬衫和特步鞋就要被黑到异次元呢？其实，只要不是浮夸的大格子衬衫，穿起来一样萌萌哒，关键是合体！当然，只要一胖就完啦。所以，穿搭是技巧，健身是根本啊。\n20180724012839470105-2018725\r5、女人变美只需要一只口红，而男人变帅只需要一条领带。男女主决定协议结婚后，召集双方父母商议结婚事宜。一套贴合肩线的西装，搭配一件白色衬衫，视觉上给人成熟稳重的感觉，男主虽然在剧中表现得很“怂”，可这并不影响他的“帅”啊，这套衣服最多算彩排，真正的新郎礼服请关注第 11 集……(嗯，这是最后一集，日剧追起来很快呦)\n20180724012746230106-2018725\r6、简洁到不能再简洁的短袖衬衣 + 牛仔裤。前一秒的踌躇满志，同下一秒的惊慌失措，莫名地戳中萌点，明明同事就在眼前，非要学人家卷福发短信。请女生们不要再吐槽男生穿衣服“土”，你告诉我，除了长裤和短裤我们还有什么？对了，短裤是不能穿的哦……，尤其是花花绿绿的那种🙃\n20180724012906742107-2018725\r7、蓝白相间的衬衣，相比普通蓝色衬衣，平添了一种活泼的感觉，就连工牌卡的绳子都来凑热闹。你知道怎么快速从人群中识别一名程序员吗？牛仔裤 + 双肩包 + 工牌卡。不，我拒绝这种符号化的穿搭，大隐隐于市，忘了这套新手村装备吧……当然，如果你包里还是各种数据线……好像换汤不换药啊(逃\n20180724013156508108-2018725\r8、任何领域都会鄙视链的存在，像津崎先生这样优秀的工程师，自然远非某某培训班的学生们。如何做一名优雅的学院派呢？你需要一件毛衣或者是一件马甲，而且一定要套在衬衣上。你问我为什么这么穿，因为通常教授们都这样穿，请参考卷福主演的电影《模仿游戏》，负责破译德军恩尼格码密码机的专家们，都是这样的穿着，同样的，还有《万物李军》里剑桥的教授们……\n20180724013332938109-2018725\r9、同样是毛衣和衬衣的搭配，圆领和 V 领是一种风格，是否翻出衬衣领又是一种风格。而我们的男主，显然可以同时驾驭这两种风格，再搭配一件休闲外套，试问还有谁？风见君帅是帅了，不过他的衣服好像永远都是针织衫啊，难道说有钱人都喜欢买一堆一样的衣服？恩，我说的就是老乔和小扎这种有钱人……\n20180724013332940110-2018725\r10、果然，有圆领就会有 V 领，强迫症对工牌卡挂绳莫名地充满好感，这个“V”字完美地贴合衣领。针织衫和衬衣，需要有一定的层次感，比如备受我们嫌弃的格子衬衫，如果搭配针织衫效果还是非常不错的，唯一的要求或许是肩膀不能过宽，因为这样会显得整体线条僵硬。我有一个问题，像女主这样宽肩膀的女生，穿一字肩真的不怕滑下来吗？😂\n20180724013332943111-2018725\r11、这种“假领”的毛衣，穿出来同样好看，我严重怀疑，这个创意是来自上海静安区同福里的老马。如果你的脖子比较长，可以考虑尝试下高领毛衣，请注意，我不是在教你，去做一名女装大佬。话说回来，衬衣上套毛衣最大的缺点是，需要挤上衬衣最上面的扣子，所以买衬衣时，请确保可以放入两根手指，这样子不会像《杀破狼 2》里的张晋一样被“帅”死。\n20180724013332944112-1-2018725\r20180724013332946112-2-2018725\r12、这里分别是针织衫和毛背心搭配格子衬衫的正确示例，简而言之，衣服的搭配上需要体现出层次感，切忌选择色调过于接近的颜色，衬衣一定要修身，否则搭配毛衣会让你显得臃肿不堪。我要立一个 flag，等我瘦到 120 斤，我就奖励自己一件针织衫。\n20180724013332949113-1-2018725\r20180724013332951113-2-2018725\r13、毛衣和针织衫真的是搭配率超级高的优质单品，穿出来真的非常好看。我知圆领 T 恤是夏天最常见的穿搭，可如果你想尝试下不同的风格，我建议你买一件衬衣或者是 Polo 衫或者是针织衫，这些都能带给你不一样的感觉。我一直想尝试皮夹克或者是牛仔外套，可我自我感觉不适合这样硬朗的风格，谁让我是一个温柔的蓝孩纸呢……\n室外篇 20180724013619267201-2018725\r1、这种材质的衣服应该很容易脏，而且大概率会让你显得臃肿(胖)，可不得不说，这一身和女主站一起挺搭的，我们学习穿搭只有两个目的，找到女朋友和不给女朋友丢脸(🙃)。作为围巾控，这身搭配我觉得可以尝试一下。\n20180724013619269202-2018725\r2、一个男人，只要有一件合身的西装，就已经在变帅的路上迈出一大步。这一款的话，毛衣黑白两种颜色，和衬衣蓝灰白的色调蛮接近的，所以基本上看不出层次感来。其实一直不明白男主为什么如此沉闷的颜色，难道是因为向女主表白以后变成熟了吗？😂\n20180724013619273203-2018725\r3、你看，这件衣服再次发挥了格子衬衫的伟大魅力，而在这件蓝色的针织衫的衬托下，可以明显地感觉到男主变“白”了，当 90 后们开始步入中年职场，不妨尝试穿一点靓丽的颜色，因为我们还可以再年轻一下。池日先生又讲了一句“名言”，你看津崎先生这震惊的小眼神。\n20180724013619275204-1-2018725\r20180724013619277204-2-2018725\r4、这一次，男女主在众人“陪伴”下开展了一次小旅行，男主所穿的这件短袖衬衣真的是最普通的衣服，目测在某澜之家就可以找到同款，搭配这斜挎的帆布包简直是减龄神器，我真心羡慕那些三十多岁还会被认成学生的“大叔”们，在下高中刚毕业就被叫叔叔到现在，人家明明想被叫做“哥哥”😓\n20180724013619279205-1-2018725\r20180724013619281205-2-2018725\r20180724013619283205-3-2018725\r5、为什么这三件毛衣给人越来越帅的感觉？因为你发现它的颜色越来越纯粹，纯粹到最后就剩下一种颜色，所以，人家建议衣服上不要有 Logo 不无道理啊，在下有一位朋友，喜欢穿各种印有二次元图案的 T 恤，30 多岁了永远都给人萌萌哒的感觉，你说到底听谁的好呢？总之，我计划今天买一件纯色毛衣，你呢？\n20180724013619285206-2018725\r6、呃……这件应该被称为棒球衫还是夹克呢？我个人不太喜欢这种拼接的样式，我更喜欢那种纯色的简洁的夹克。说起这一集，男主因为错过女主的生日而自责，独自到商城里为女主挑选礼物，面对琳琅满目的商品，男主一脸茫然……有时，女生会嫌弃男生分不清口红色号什么的，并送给男生一个“直男”称号，其实，面对不熟悉的领域，谦虚而大方的承认就好了，我们当然是直的，难道你们喜欢弯的吗？\n20180724013619286207-2018725\r7、你一定觉得像男主这样西装革履的高薪人士，每天都是坐在电脑前喝喝咖啡写写代码。其实，我们是一群连星巴克都不舍得去的人，每一次紧急加班，都是咖啡因转换为二进制代码的过程。我们并不是不会花钱，我们在数码产品、电子设备等方面的投入，完全不亚于你们买衣服、做美甲等等。有判词云：钱多、话少、死得快\n20180724013619288208-2018725\r8、嗯，这件怎么评价呢？中规中矩的秋冬款外套。我一直有一个愿望，等瘦下来以后买件卫衣穿，因为我实在怀疑自己，穿任何套头的衣服都会显得胖。不过好在这是秋冬季节，大家一起胖呀，这种衣服应该会比较容易脏，因为在下就有一件差不多的，果然直男审美啊，呵呵🙃\n20180724013619290209-2018725\r9、这个世界对长得高的人相当宽容，即使他们长得并不好看，可他们长得高穿衣服好看啊。从此刻开始，我希望你打破这种认知，谁说矮个子男生不能穿长款的衣服，男主这就是活生生的例子啊，我知道女生都喜欢 175+的男生，可我希望你能找到我除了不能举高高以外的优点，女主到菜场就买了棵葱回来，男主赶紧接过来拎在手里，真是适合过日子的人啊\n20180724233648400210-2018725\r10、这是整部剧出镜率最高的一套衣服。什么？你问我这是哪一集？话说，你们都不看片尾曲的吗？由男主演唱的单曲《恋》，着实为抖音贡献了大量流量，这舞蹈难道不可爱吗？这衣服难道不好看吗？我说过了，衬衣 + 背心是学院派的典型穿法，男主果然是个文艺的男孩子，他的帅你 Get 到了吗？\n本文小结 这或许是我写过的最“八卦”的一篇博客啦，有时候，越是轻松的东西越容易被人接受。程序员，他们并不是情商低，并不是内向，并不是不会撩妹，仅仅是因为这个世界不单单需要娱乐精神，同样需要严谨和专注。搅动一个人的情绪，无非是分泌出某种荷尔蒙；而真正驱动这个世界的，是严格甚至苛刻的规则。面对不熟悉的领域，应该保持敬畏心，而非以标签化的定义以讹传讹；如果靠贴标签就可以给人分类，那么谁是好人谁又是坏人？谁代表了正义谁又代表了邪恶？哦哦，对了，我们不会修电脑以及做任何你认为简单的事情……\n",
            "tags": ["日剧","程序员","穿搭","八卦"],
            "categories": ["生活感悟"],
            "thumbnail": ""
        },
        {
            "title": "我是猫，一只特立独行的猫",
            "date": "2018-03-06",
            "permalink": "http://localhost:1313/posts/352037321/",
            "summary": "终于在除夕夜到来前，在 Kindle 上读完了 2017 年的最后一本书，来自夏目漱石先生的《我是猫》。起初买这本书的动机说起来非常滑稽，一来以为这会是一本诙谐幽默的书，二来对夏目这个名字莫名地充满好感。我读的是曹曼翻译的中文译本，读时觉得这位作者的文字清新素雅，即使全书行文节奏堪称缓慢到极点，想来应该是我们这个时代的人物。及至翻阅作者生平，始知这位被誉为“国民大作家”的日本作家，早在 100 年前就在日本文学史上享有盛名。这种感觉如何去形容呢？大概就是杨过从剑冢石刻的寥寥数语中，遥想独孤求败“生平求一敌手而不可得”的寂寥难堪。这位老先生的文字可以说非常”摩登“了，因为在 100 年后的今天再次读来，竟完全读不出违和感来，所谓”嬉笑怒骂皆成文章“，讽刺与幽默杂然相陈，这是我喜欢这本书的理由。\n对于《我是猫》这本书，按照作者的话说，它是一部没有什么情节的小说，因为它完全是以一只猫的视角来行文，这只生活在一个教师家庭里的猫，每天都会接触到形形色色的文人，譬如：不食人间烟火，空有一番理论而不去实践的独仙；整天磨玻璃球，做事一丝不苟甚至古板木呐的寒月；表面上每天都很乐观，实则唯恐天下不乱的米亭；做事三分钟热情，自命清高的苦沙弥……等等。在猫的眼睛这里，这些人整天聚在一起讨论没有意义的事情，对现实世界心怀不满，不思进取就会怨天尤人，甚至金田及其夫人的”拜金主义“，为金钱而陷害苦沙弥的邻居，唯利是图、虚伪圆滑的铃木，这些人在猫的眼睛里都是丑陋而黑暗的。这只猫平静地叙述着它的见闻，仿佛它早已经整个人类和社会看穿看透，或许带着些嘲讽，或许带着些同情。\n每年的 2 月 22 日是日本的猫节，这是我在读完这本书以后知道的。而猫在日本的文化形象中是非常神圣的，据说这是因为猫最早由遣唐使带来日本，首先作为宫廷宠物出现，直至江户时代进入”寻常百姓家“。除此之外，日本作为重度渔业国度，对稻米的珍惜使其在捕鼠护粮方面极为重视，猫作为老鼠的天敌自然而然地受到喜爱。相传招财猫起源于东京世田谷的豪德寺，因此猫在日本被人们当作神明供奉。再比如日本动漫中的机器猫、龙猫和 Hello Kitty 都是猫在日本文化中的经典形象，日本的文学作品比如《草枕子》、《源氏物语》等里面都有关于猫的故事。时至今日，依然有大量德川家族与猫的故事流传。因此，猫在日本人眼中有一种浓厚的贵族气息。陈凯歌导演的《妖猫传》，改编自日本作家梦枕貘的小说《沙门空海》，猫在其中的重要性不言自明。\n这是一本“猫眼看世界”的书，这是一个怎样的世界呢？1871 年，日本历史上最为大刀阔斧的一次改革——明治维新，开始在全国范围内推行。改革带来经济飞速发展的同时，带来了各种矛盾日益突出的社会问题。36 年的 1905 年，时年 38 岁的夏目漱石，以猫的视角，如初入人类社会一般，探讨当时知识分子的心理状态和对社会变迁的感慨，并因此一举成名，获得社会广泛关注，被认为是日本批判现实主义文学的丰碑。每一个时代都有它的无奈，或许我们今天难以想象老先生当时的心境，不过从这些猫的口吻里，从这些辛辣的讽刺和戏谑中，我们总能读出作者当时内心的苦闷。猫眼里那些荒诞不经的行为，恰恰就是你我每天的生活，我们总说人类和猫是好朋友，可那仅仅是我们以为的，在猫的眼睛里，我们就像一群神经病。\n猫是如何看待人类的呢？猫说：世间的奢侈往往是无能的表现。猫一年到头都穿着同一件衣服，而人类好像不把尽可能多的东西往身上照顾就难受，人类给羊添麻烦，受蚕照顾，承蒙棉花的恩泽，你看吧，我们的所作所为连只猫都看不下去。人类羡慕猫的悠闲，故而感慨道：什么时候能像猫一样轻松就好了。可明明是人类自己制造出一堆乱七八糟的事情给自己，到头来还抱怨真痛苦真痛苦，就像自己生起一堆火，到头来嚷着热死了热死了。这一切在猫看来都是庸庸碌碌的。猫甚至断言道：人类不可能永远繁荣昌盛下去。嗯，我愿静候属于猫族时代的到来。从前是“人类一思考，上帝就发笑”，而现在是“人类一思考，猫君就发笑”。猫觉得人类模仿它们的声音时是愚蠢的，尤其是在抚摸它们的时候，因为根本不存在撒娇声，只有被撒娇声，因为我们期待的是，猫向我们撒娇，可难道不是我们在向猫撒娇？\n个体的荒谬，在人类的个性面前根本不值一提，就如同人类的个性得到完全解放以后，永远像一锅众口难调的羹汤。小说中苦沙弥、迷亭、寒月、东风和独仙时常在一起聊天，话题涉及哲学、艺术，爱情、生活等多个方面，这只“毒舌”的猫，就在无意识地引导和放大这些观点，“我认为这个世界上，没有比爱和美更受人尊重的了”，所以这本书里的观点，其实并不是完全的消极的，就像这只猫平静地看着这个世界，它对人类有过嘲讽，有过同情，它甚至没有自己的名字，当它失足淹死在水缸里的时候，对这个世界更多的是种悲天悯人吧！我们这个世界上有五种毒药，佛家所谓的“贪嗔痴慢疑”，作者提到“可没有任何一个人，能够全然抛开自己去研究外界，如果人类能够把自己疏离出来，那么疏离的瞬间，也就没有了自己”，人类常常不愿放过自己，更不愿放过别人，因为所有无解的问题，都可以制造一个意义出来，而我们早已习惯这一切。\n曾经有朋友问我，为什么喜欢猫这种动物，我回答说，因为我就像一只猫，一只特立独行的猫，对所有人都很友善和蔼，却喜欢独来独往。因为维护这种若即若离的关系，对我来说比任何事情都要困难。人类以为猫都是傲娇的动物，其实这是人类的一厢情愿，因为从智力上来说，猫的智力是不及狗的。猫自然对人是有感情的，不过在人类驯养动物的历程中，狗更聪明、更懂得如何向人类索取，我们所认定的感情，在狗的世界里或许并不是。人类难以理解的事物，所谓阳春白雪，所谓曲高和寡，不自然地背负上高冷的名声，对一只猫而已，到底是我们不了解猫，还是不了解我们自己。人总在试图驯化猫这种动物，可猫无非是人类的一种折射而已，它就像那些独立潇洒的人一样，不藉由粘人和撒娇来获取安全感，在这个世上没有谁会离不开谁。你走，我不必送你；你来，不管多大风多大雨，我都去接你。这是我——一只猫的自白。\n有时候，难免会觉得人类自作聪明，喜欢给世间的事物贴上不同的标签，譬如二哈、喵星人、汪星人、猫主子……可你知道猫如何评价人类的吗？作者说，“这些人虽然看起来快活，但是如果叩问他们的心底，却可以听见悲凉的回响”。为什么会听见悲凉的回响呢？大概是人类丰富而有趣的个性，不断地尝试挑战世俗的眼光，结果被世俗打败而变得世俗，这听起来简直就像是，英雄杀死魔王又变成魔王的故事的翻版。“每个人地位都提高，等同于每个人的地位都下降。人类不再做让自己委屈的事情，正是个人力量变强的证明；几乎不再插手别人的事情，反而是群体力量变弱的证明”。一点亏都不愿意吃，一点小便宜就要占，无一不是为了证明个人意志的强化，可人与人间的空间越来越狭窄，日益窘迫，为了扩充自己膨胀到近乎爆炸。人与人之间那点空间，是一切痛苦的根源，你说这还不算作悲凉吗？没有谁可以完全了解一个人，不完全认知是人类关系的反应剂，痛苦、误会、偏见……等等纷至沓来，你以为你模仿猫叫，猫就真的听懂了吗？\n夏目先生在后记里写道，“世事变迁就像猫的眼珠一样变幻莫测，短短几个月世间，就可以去那极乐世界，或者可以把薪水花光光。年底过去了，正月过去了，花朵凋谢，新叶又生。以后世界将如何变化，我不了解，只不过水缸中猫的瞳孔，应该可以凝成永恒”。我想，世界会一如既往地这样无奈下去，时间会一如既往地这样消逝下去，而你和我会一如既往地平庸且烦恼下去。假如有这样一只猫，通过瞳孔记下了我的生平，不知道它会如何评价我呢？就像在某一个下雨天，突然想到某一个人，单单是因为怕这世界里，从此再没了对方的音讯。可单单是想到又有什么意义呢？《笑傲江湖》里令狐冲率领江湖群雄，前往少林寺解救被困的圣姑，这个将来会成为他妻子的人，而眼下更是生死未知、前途未明，可在一片寂静中听到雪花簌簌落下时，他想到的却是：小师妹不知这时候不知在干甚么。及至岳林珊为林平之所杀，临死托付令狐冲替她照顾小林子，内心却又不知做何感想了吧……",
            "content": "终于在除夕夜到来前，在 Kindle 上读完了 2017 年的最后一本书，来自夏目漱石先生的《我是猫》。起初买这本书的动机说起来非常滑稽，一来以为这会是一本诙谐幽默的书，二来对夏目这个名字莫名地充满好感。我读的是曹曼翻译的中文译本，读时觉得这位作者的文字清新素雅，即使全书行文节奏堪称缓慢到极点，想来应该是我们这个时代的人物。及至翻阅作者生平，始知这位被誉为“国民大作家”的日本作家，早在 100 年前就在日本文学史上享有盛名。这种感觉如何去形容呢？大概就是杨过从剑冢石刻的寥寥数语中，遥想独孤求败“生平求一敌手而不可得”的寂寥难堪。这位老先生的文字可以说非常”摩登“了，因为在 100 年后的今天再次读来，竟完全读不出违和感来，所谓”嬉笑怒骂皆成文章“，讽刺与幽默杂然相陈，这是我喜欢这本书的理由。\n对于《我是猫》这本书，按照作者的话说，它是一部没有什么情节的小说，因为它完全是以一只猫的视角来行文，这只生活在一个教师家庭里的猫，每天都会接触到形形色色的文人，譬如：不食人间烟火，空有一番理论而不去实践的独仙；整天磨玻璃球，做事一丝不苟甚至古板木呐的寒月；表面上每天都很乐观，实则唯恐天下不乱的米亭；做事三分钟热情，自命清高的苦沙弥……等等。在猫的眼睛这里，这些人整天聚在一起讨论没有意义的事情，对现实世界心怀不满，不思进取就会怨天尤人，甚至金田及其夫人的”拜金主义“，为金钱而陷害苦沙弥的邻居，唯利是图、虚伪圆滑的铃木，这些人在猫的眼睛里都是丑陋而黑暗的。这只猫平静地叙述着它的见闻，仿佛它早已经整个人类和社会看穿看透，或许带着些嘲讽，或许带着些同情。\n每年的 2 月 22 日是日本的猫节，这是我在读完这本书以后知道的。而猫在日本的文化形象中是非常神圣的，据说这是因为猫最早由遣唐使带来日本，首先作为宫廷宠物出现，直至江户时代进入”寻常百姓家“。除此之外，日本作为重度渔业国度，对稻米的珍惜使其在捕鼠护粮方面极为重视，猫作为老鼠的天敌自然而然地受到喜爱。相传招财猫起源于东京世田谷的豪德寺，因此猫在日本被人们当作神明供奉。再比如日本动漫中的机器猫、龙猫和 Hello Kitty 都是猫在日本文化中的经典形象，日本的文学作品比如《草枕子》、《源氏物语》等里面都有关于猫的故事。时至今日，依然有大量德川家族与猫的故事流传。因此，猫在日本人眼中有一种浓厚的贵族气息。陈凯歌导演的《妖猫传》，改编自日本作家梦枕貘的小说《沙门空海》，猫在其中的重要性不言自明。\n这是一本“猫眼看世界”的书，这是一个怎样的世界呢？1871 年，日本历史上最为大刀阔斧的一次改革——明治维新，开始在全国范围内推行。改革带来经济飞速发展的同时，带来了各种矛盾日益突出的社会问题。36 年的 1905 年，时年 38 岁的夏目漱石，以猫的视角，如初入人类社会一般，探讨当时知识分子的心理状态和对社会变迁的感慨，并因此一举成名，获得社会广泛关注，被认为是日本批判现实主义文学的丰碑。每一个时代都有它的无奈，或许我们今天难以想象老先生当时的心境，不过从这些猫的口吻里，从这些辛辣的讽刺和戏谑中，我们总能读出作者当时内心的苦闷。猫眼里那些荒诞不经的行为，恰恰就是你我每天的生活，我们总说人类和猫是好朋友，可那仅仅是我们以为的，在猫的眼睛里，我们就像一群神经病。\n猫是如何看待人类的呢？猫说：世间的奢侈往往是无能的表现。猫一年到头都穿着同一件衣服，而人类好像不把尽可能多的东西往身上照顾就难受，人类给羊添麻烦，受蚕照顾，承蒙棉花的恩泽，你看吧，我们的所作所为连只猫都看不下去。人类羡慕猫的悠闲，故而感慨道：什么时候能像猫一样轻松就好了。可明明是人类自己制造出一堆乱七八糟的事情给自己，到头来还抱怨真痛苦真痛苦，就像自己生起一堆火，到头来嚷着热死了热死了。这一切在猫看来都是庸庸碌碌的。猫甚至断言道：人类不可能永远繁荣昌盛下去。嗯，我愿静候属于猫族时代的到来。从前是“人类一思考，上帝就发笑”，而现在是“人类一思考，猫君就发笑”。猫觉得人类模仿它们的声音时是愚蠢的，尤其是在抚摸它们的时候，因为根本不存在撒娇声，只有被撒娇声，因为我们期待的是，猫向我们撒娇，可难道不是我们在向猫撒娇？\n个体的荒谬，在人类的个性面前根本不值一提，就如同人类的个性得到完全解放以后，永远像一锅众口难调的羹汤。小说中苦沙弥、迷亭、寒月、东风和独仙时常在一起聊天，话题涉及哲学、艺术，爱情、生活等多个方面，这只“毒舌”的猫，就在无意识地引导和放大这些观点，“我认为这个世界上，没有比爱和美更受人尊重的了”，所以这本书里的观点，其实并不是完全的消极的，就像这只猫平静地看着这个世界，它对人类有过嘲讽，有过同情，它甚至没有自己的名字，当它失足淹死在水缸里的时候，对这个世界更多的是种悲天悯人吧！我们这个世界上有五种毒药，佛家所谓的“贪嗔痴慢疑”，作者提到“可没有任何一个人，能够全然抛开自己去研究外界，如果人类能够把自己疏离出来，那么疏离的瞬间，也就没有了自己”，人类常常不愿放过自己，更不愿放过别人，因为所有无解的问题，都可以制造一个意义出来，而我们早已习惯这一切。\n曾经有朋友问我，为什么喜欢猫这种动物，我回答说，因为我就像一只猫，一只特立独行的猫，对所有人都很友善和蔼，却喜欢独来独往。因为维护这种若即若离的关系，对我来说比任何事情都要困难。人类以为猫都是傲娇的动物，其实这是人类的一厢情愿，因为从智力上来说，猫的智力是不及狗的。猫自然对人是有感情的，不过在人类驯养动物的历程中，狗更聪明、更懂得如何向人类索取，我们所认定的感情，在狗的世界里或许并不是。人类难以理解的事物，所谓阳春白雪，所谓曲高和寡，不自然地背负上高冷的名声，对一只猫而已，到底是我们不了解猫，还是不了解我们自己。人总在试图驯化猫这种动物，可猫无非是人类的一种折射而已，它就像那些独立潇洒的人一样，不藉由粘人和撒娇来获取安全感，在这个世上没有谁会离不开谁。你走，我不必送你；你来，不管多大风多大雨，我都去接你。这是我——一只猫的自白。\n有时候，难免会觉得人类自作聪明，喜欢给世间的事物贴上不同的标签，譬如二哈、喵星人、汪星人、猫主子……可你知道猫如何评价人类的吗？作者说，“这些人虽然看起来快活，但是如果叩问他们的心底，却可以听见悲凉的回响”。为什么会听见悲凉的回响呢？大概是人类丰富而有趣的个性，不断地尝试挑战世俗的眼光，结果被世俗打败而变得世俗，这听起来简直就像是，英雄杀死魔王又变成魔王的故事的翻版。“每个人地位都提高，等同于每个人的地位都下降。人类不再做让自己委屈的事情，正是个人力量变强的证明；几乎不再插手别人的事情，反而是群体力量变弱的证明”。一点亏都不愿意吃，一点小便宜就要占，无一不是为了证明个人意志的强化，可人与人间的空间越来越狭窄，日益窘迫，为了扩充自己膨胀到近乎爆炸。人与人之间那点空间，是一切痛苦的根源，你说这还不算作悲凉吗？没有谁可以完全了解一个人，不完全认知是人类关系的反应剂，痛苦、误会、偏见……等等纷至沓来，你以为你模仿猫叫，猫就真的听懂了吗？\n夏目先生在后记里写道，“世事变迁就像猫的眼珠一样变幻莫测，短短几个月世间，就可以去那极乐世界，或者可以把薪水花光光。年底过去了，正月过去了，花朵凋谢，新叶又生。以后世界将如何变化，我不了解，只不过水缸中猫的瞳孔，应该可以凝成永恒”。我想，世界会一如既往地这样无奈下去，时间会一如既往地这样消逝下去，而你和我会一如既往地平庸且烦恼下去。假如有这样一只猫，通过瞳孔记下了我的生平，不知道它会如何评价我呢？就像在某一个下雨天，突然想到某一个人，单单是因为怕这世界里，从此再没了对方的音讯。可单单是想到又有什么意义呢？《笑傲江湖》里令狐冲率领江湖群雄，前往少林寺解救被困的圣姑，这个将来会成为他妻子的人，而眼下更是生死未知、前途未明，可在一片寂静中听到雪花簌簌落下时，他想到的却是：小师妹不知这时候不知在干甚么。及至岳林珊为林平之所杀，临死托付令狐冲替她照顾小林子，内心却又不知做何感想了吧……\n",
            "tags": ["夏目漱石","日本文学","我是猫"],
            "categories": ["读书笔记"],
            "thumbnail": ""
        },
        {
            "title": "你了解爱的艺术吗？",
            "date": "2016-09-24",
            "permalink": "http://localhost:1313/posts/2275646954/",
            "summary": "或许我不是一个懂得如何去爱人的人，我时常陷入一种自我否定的焦虑当中，当我发觉自己喜欢上一个人的时候，从某种意义上它会让我身上的缺点被无情地放大，我并不畏惧在喜欢的人面前暴露这些缺点，因为这就是真实的我，因此我从来不喜欢去塑造别人，让别人成为我心目中期待的样子，可是我会忍不住去塑造我自己，尤其是在和别人相处的过程中，发现我身上的缺点或者问题的时候，我习惯了对自我严格，虽然我知道这个过程注定痛苦，可是你能告诉我，爱到底是什么吗？如果爱不足以让我们改变，我们喜欢的究竟是一个怎样的自己、怎样的别人？\n",
            "content": "或许我不是一个懂得如何去爱人的人，我时常陷入一种自我否定的焦虑当中，当我发觉自己喜欢上一个人的时候，从某种意义上它会让我身上的缺点被无情地放大，我并不畏惧在喜欢的人面前暴露这些缺点，因为这就是真实的我，因此我从来不喜欢去塑造别人，让别人成为我心目中期待的样子，可是我会忍不住去塑造我自己，尤其是在和别人相处的过程中，发现我身上的缺点或者问题的时候，我习惯了对自我严格，虽然我知道这个过程注定痛苦，可是你能告诉我，爱到底是什么吗？如果爱不足以让我们改变，我们喜欢的究竟是一个怎样的自己、怎样的别人？\n弗洛姆这本《爱的艺术》是我自己为自己挑选的一本书，在我买了 Kindle 以后，我将我的时间安排在看书和学习上，因为我的确很喜欢读书，而我这种理性的性格有时候难免让人讨厌，所以读书特别是选择去读人文类书籍，从某种意义上来说是我在刻意地稀释这种理性思维造成的影响，《黑客与画家》里告诉我们一件事情，聪明人不被周围人喜欢是因为他们比周围人聪明，当谈恋爱越发地被人们改造成一种套路，我们对爱的定义或许会越来越模糊，可是爱作为一种大自然间普遍存在的情感，我坚信它是一种相当原始而简单的事情。\n弗洛姆认为，爱情是对人类生存问题的回答，人们在这个世界上逐渐意识到生不由己、意识到死的必然、意识到孤独和与世隔绝、意识到面对社会和自然的威力时的无能为力，所以，所有生活在不同文化和时代里的人，都面临着一个同一个问题，即：如何克服这种孤独感。古人说“古来圣贤皆寂寞，唯有饮者留其名”，其实何止是古来圣贤，我们生活在这个地球上的所有人，从出生到死亡都不可避免地被一种孤独感包围者，曾经和别人 讨论过这个问题，我对人情颇为淡漠，因为我觉得除了能够真正将彼此联系起来的两个人，在这个世界上你永远无法找到真正能让你灵魂皈依的地方，这种感觉并非是由血缘或者金钱这样的关系来维系，一个人的孤独与一群人的孤独，在整个宇宙间看起来，其实没有什么不同。\n爱的确是一门艺术，可对我们每个人而言，它像是某种缥缈甚至是难以揣测的情绪，你不能用一种非常理性的眼光来审视和定义它的存在，弗洛姆说：“不是拥有财物的人是富裕的，而是给予他人东西的人才是富裕者”，可现实是并非你不顾一切地对一个人好，就能赢得一份让你感动的爱情，所以在经历过挫折以后，我不再考虑一味地索取或者付出，我喜欢将这个过程叫做分享，人们在分享的过程中认识彼此、丰富彼此、提高彼此的生命感，这是我认为在爱情中我们需要去挖掘的一种潜质，如果一个人没有生命力，就不会有创造爱情的能力，所以当我们试图爱一个人之前，我们首先要学会爱我们自己，而弗洛伊德将这种人的自我欣赏叫做“自恋”，除了爱情自身积极性的因素以外，爱情具有所有爱的形式所共有的因素，如：关心、责任心、尊重和理解。\n爱情不是自私地占有对方或者是放弃除对方以外人的更为广泛意义上的博爱，“爱情是自由之子，永远不会是控制的产物”。或许我们穷极一生来认识自己、认识别人，可我们最终还是不认识自己、不认识别人，可我们无法阻止这种深入了解人的灵魂的秘密、了解人的核心，即“自我”的愿望将继续存在。毫无疑问，德尔斐的人箴言“认识你自己”表达了我们认识自己和他人的愿望。白昼和黑夜表面看起来是敌人，但它们却都是为了一个目标，因为相爱就是为了完成共同的事情。而从广义的爱的定义来看，中国古代先哲孟子的“老吾老以及人之老，幼吾幼以及人之幼”就很好的表达了这种观点，弗洛姆说：“一切爱的形式都以博爱为基础，我指的博爱就是对所有的人都有一种责任感，关心、尊重和了解他人，即愿意提高其他人的生活情趣”。以前以为爱是自私地占有一个人，是对除对方以外的人表现得漠不关心，可后来逐渐发现，我们对自己的爱远远超过别人，这或许不能叫做爱吧！\n我们来到这个世界上，更本质的意义在于我们希望认识些有趣的人，做些有趣的事情而已，这意味着我们常常试图在这个世界上留下自己的印记，我们习惯了在朋友圈里晒美食、晒旅游、晒自拍等等约定俗成的处事方法，亦如我们生来就渴望被人理解、被人喜欢一样，而这一切更本质的原因，我们现在称为“刷存在感”，恰恰迎合了这个观点，我们想要在这个世界上留下我们的印记，即使这些方式方法看起来并不是我们最初喜欢的样子，弗洛姆将在这个观点理解为“超越自己”的追求，这一追求属于人的最基本要求，即“人对自己的纯生物作用不满，他不能忍受自己仅仅是被扔进这一世界的小卒。他一定要感到自己是创造者，是能超越处于被创造者消极地位的生命。满足这一要求有许多可能性，最自然和最基本的途径就是母亲对自己创造物的关怀和爱”。所以，我们渴望被人喜欢、被人理解都是因为我们希望生来独特、生来不同，而这一切都源于父母对爱的一种创造力。\n我们常常表现出，一种试图要要证明比别人过得更好的心态，仿佛在朋友圈或者微博这种社交平台上，我们能找到更多的自豪感，可我同样知道，这个世界上最大的社交网站 Facebook，背后却是由一个有社交障碍的人创造出来的，我们都渴望让别人了解自己、认识自己，这不同于社交场合里那种客套的场面话，对此，弗洛姆认为，人与人之间可以通过讲述这种方式来打破人与生俱来的这种孤独感，“讲述自己的生活，叙述自己的希望和恐惧，谈出自己幼稚的或者不成熟的梦想，以及找到面对世界的共同利益——所有这一切都是克服人与人之间隔离的途径，甚至表露自己的愤怒和仇恨，毫无顾忌地交心也都被看作是亲密的表现”，我们对爱的终极理解其实应该是，我们通过爱一个人，进而爱全人类，爱一切生命，我们从自我的生命的本质出发去爱对方，并且去体验对方的本质，爱情是意志的行为，是人作的一项把全部生命交付对方的决定。\n圣经中“爱他人如同爱己”的说法，说明了对自己的完整性和独特性的尊重，爱自己，理解自己同尊重、爱和谅解别人是不可分割的，爱我同爱他人是紧密相连的。中世纪德意志神秘主义哲学家和神学家，爱克哈特有一句关于自爱的格言：“你若爱己，那就会爱所有的人如爱己。你若对一个人的爱少于爱己，如果你不是爱所有的人如同爱己，如果你不是在一个人身上爱所有的人——因为这个人就是上帝和人。一个既爱自己又爱他人如同爱己的人就是这样的人，一个值得这样评价的人”。我们来到世界上学会与人相处、学会如何去爱一个人，其实是在寻找一种探索生活所需要的信仰，因为生存或许会非常容易，可是学会生活或许会非常困难，我们努力提高生活质量，源于我们对自己和伴侣的一种爱，希望让彼此变得更好。\n或许对弗洛姆本人而言，这本《爱的艺术》更像是 他对自我的一种内省，因为他的爱情基本上在持续地遭遇着失败，他曾经和四个不同的女人结过婚，所以他在这本书里提出的大量观点都来源自他自己的感情经历，他早期研究过弗洛伊德的相关理论，而事实上，因为对爱的无能为力让他真正找到了爱的能力，我们在年轻的时候总会遇到一个非常喜欢，可我们却无法给她想要的生活的女孩子，或许我们都需要用一生去领悟爱的真正含义吧，就像弗洛姆这本书是建立在将理论和实践结合起来的基础上的，我不认为我此刻已经读懂了这本书，可是它对我的确非常重要，我总要学着去爱别人，让自己变得更好！\n",
            "tags": ["爱情","读书","Kindle"],
            "categories": ["读书笔记"],
            "thumbnail": ""
        },
        {
            "title": "Unity3D 游戏开发之在 uGUI 中使用不规则精灵制作按钮",
            "date": "2016-07-08",
            "permalink": "http://localhost:1313/posts/1190622881/",
            "summary": "各位朋友大家好，欢迎关注我的博客，我的博客地址是：https://blog.yuanpei.me。最近因为受到工作上业务因素影响，所以博主在 Unity 引擎上的研究有所停滞。虽然目前的工作内容和 Unity3D 没有直接的关联，可是我觉得工程师应该有这样一种情怀，即工作和兴趣是完全不同的两个概念。编程对我而言，首先是一种兴趣，其次是一份工作。所以我宁愿在每天下班以后继续研究自己感兴趣的东西，而非为了取悦这个世界、为了加班而加班。最近广电总局让整个游戏行业都坐立不安了，因为其新发布的一系列规定，让中国的独立游戏开发者怨声载道。可是我们更应该看到积极的一面是，无数的小游戏公司会在最近数月内大量消失，或许对中国野蛮生长的游戏行业这是一次“形式”上的整顿，可对我们开发者来说，在这个过程中努力提升自我、巩固基础永远比追求时髦、流行的技术或者框架有意义的多，因为热闹的从来都是昙花一现般的璀璨，而永恒的永远都是历久弥新的真理。好了，闲言少叙，今天我们的话题是在 uGUI 中使用不规则精灵制作按钮。\n",
            "content": "各位朋友大家好，欢迎关注我的博客，我的博客地址是：https://blog.yuanpei.me。最近因为受到工作上业务因素影响，所以博主在 Unity 引擎上的研究有所停滞。虽然目前的工作内容和 Unity3D 没有直接的关联，可是我觉得工程师应该有这样一种情怀，即工作和兴趣是完全不同的两个概念。编程对我而言，首先是一种兴趣，其次是一份工作。所以我宁愿在每天下班以后继续研究自己感兴趣的东西，而非为了取悦这个世界、为了加班而加班。最近广电总局让整个游戏行业都坐立不安了，因为其新发布的一系列规定，让中国的独立游戏开发者怨声载道。可是我们更应该看到积极的一面是，无数的小游戏公司会在最近数月内大量消失，或许对中国野蛮生长的游戏行业这是一次“形式”上的整顿，可对我们开发者来说，在这个过程中努力提升自我、巩固基础永远比追求时髦、流行的技术或者框架有意义的多，因为热闹的从来都是昙花一现般的璀璨，而永恒的永远都是历久弥新的真理。好了，闲言少叙，今天我们的话题是在 uGUI 中使用不规则精灵制作按钮。\n从用户体验说起 我们都知道在现代应用程序设计中，用户体验(UX)和用户界面(UI)是两个非常重要的内容。为什么用户体验(UX)和用户界面(UI)会显得如此重要呢？这是因为从普通用户的角度来讲，用户界面(UI)是其接触到一个产品时最先看到的最直观的东西，而在这个过程中产生的直观感受就是用户体验(UX)，所以说到底这是一个产品给用户的“第一印象”。\nUX和UI\r最近百度 UE 总监刘超在 IXDC 峰会上的演讲引起了大家的关注，抛开百度在人才选拔机制中存在的问题以及刘超本人在设计领域是否具备专业能力这两个问题，这件事情真正让大家吐槽的是什么呢？答案是用户体验。虽然 IXDC 并非国际级别的大型会议，但是我相信大家组织这样的活动，其本意是为了探讨交互、设计领域内的新方法和新思维，因为随着互联网行业的发展，交互和设计这个领域越来越被人们所关注，所以在这样一个场合下，当与会嘉宾都在试图向人们输出干货的时候，刘超以一个非常糟糕的“用户体验”来给大家讲什么是用户体验，这件事情起源自刘超的一个个人行为，结果牵一发而动全身，最终升级为百度继“魏则西事件”以后的又一次公关危机。\n什么叫设计\r我到底想说什么呢？我说的本质上就是用户体验的问题，在这个事件中，刘超穿着上的不得体(短裤搭配拖鞋?)、PPT 制作的粗制滥造(校招时所用修改)、演讲过程的敷衍糊弄(说相声、猜谜语)等因素，让刘超在与会者心目中的地位瞬间滑落到冰点，进而引发人们对百度在交互设计领域内的能力的怀疑，联想到百度最近这些年内出现的问题，这件事情难免会被人作为指责百度这家企业价值观问题，我想这是这个事情为什么会让大家如此关注的一个原因吧。\nWTF!\r那么，我们说这些到底和今天的主题有什么关系呢？我想说这当然有关系啊，因为我们提出的这个问题就是一个用户体验的问题。我们知道游戏行业对美术资源高度依赖，不管是 2D 游戏还是 3D 游戏，一个项目组中前期主要的工作量其实都在美术这边，虽然不同的游戏引擎、GUI 框架都为我们提供了标准的控件样式，然而在这样一个注重多样性的时代，默认样式、系统字体都会让人觉得这个产品缺乏新意，因此这种要求体现在游戏项目中就变成了，我们使用大量的图片资源来解决界面和字体的问题。\n例如，我们通常使用 BMFont 来制作位图字体，这是为了同时满足字体的多样性和资源的容量这两个要求。再比如我们在使用 cocos2d-x 和 Unity3D 引擎开发游戏的时候，我们将大量的时间花费在了 UI 的制作上，这一切的一切从本质上来讲都是为了提升产品的童虎体验。这样我们就会遇到一个问题，UI 中的按钮默认情况下都是规则的矩形，而实际上美术提供的素材常常是不规则的，因此如果继续使用以矩形为标准的这套机制，在实际使用中可能出现“用户点击在不该响应的区域结果程序响应了用户操作”这样的问题，为了解决这个问题，提升这一点点细微的用户体验，我们需要花费时间和精力来了解下面这些内容。\n两种不同的方案 目前，关于这个问题如何，解决通过搜索引擎我们能找到两种不同的方案：\n多边形碰撞器: 该方法是指给精灵(Sprite)添加一个多边形碰撞器(Rolygon Collider)组件，利用该组件来标记精灵的边界，这样通过比较鼠标位置和边界可以判断点击是否发生在精灵内部。这种方法的详细说明可以参考宣雨松的这篇文章：UGUI 研究院之不规则按钮的响应区域（十四） 精灵像素检测: 该方法是指通过读取精灵(Sprite)在某一点的像素值(RGBA)，如果该点的像素值中的 Alpha\u0026lt;0.5 则表示该点处是透明的，即用户点击的位置在精灵边界以外，否则用户点击的位置在精灵边界内部。这种方法的详细说明可以参考这里 多边形碰撞器 多边形碰撞器这种方案从本质上来讲，其核心思路是验证某一点是否在任意多边形内部，因为在这里 RolygonCollider2D 组件的作用体现在：第一，它可以在编辑器下进行可视化编辑对用户友好；第二，它可以在帮助我们标记精灵边界的同时保留顶点信息。所以在这里 RolygonCollider2D 组件相当于为我们提供任意多边形的顶点信息，而接下来我们要做是将鼠标位置转化为屏幕坐标，这样我们就获得了某一点的坐标。整体思路看起来是没有问题的，但我个人以及网友AwayMe都认为宣雨松这个算法存在问题，具体的理由如下：\n1、uGUI 中的元素采用的是以屏幕中心为原点(0,0)的平面直角坐标系，而普通屏幕坐标采用的是以左下角为原点(0,0)的平面直角坐标系，所以多边形顶点数组和鼠标位置不在一个坐标系内，使用 AABBB 这样的碰撞检测算法存在问题。\n2、RolygonCollider2D 中的 points 属性即多边形顶点数组存储的是相对于 UI 元素的相对坐标，在进行计算的时候应该统一转化为绝对坐标，这个过程在宣雨松的代码中有所涉及，但我认为对 UI 元素来讲，应该使用 transform.GetComponent().position 而非 transform.position，因为 transform.position 最初是给 3D 物体使用的，而实际上这里是存在误差的。\n3、我怀疑宣雨松提供的这个 ContainsPoint 方法的正确性，因为按照我的理解修改这个方法以后，发现界面响应的情况和实际情况是有所出入的，如下图所示，在整个区域内该方法都返回 false。为了排除因为我的方法而对结果产生的影响，我使用宣雨松的代码进行了测试，结论是这个方法不管进行坐标系的转换与否，它在整个区域内的返回值都是 false，因此我认为这个方法是错误的，虽然从理解算法的角度来看，它应该是根据线性差值来判断点在多边形中每条边的哪一侧的。\n响应区域说明\r在评论中网友AwayMe指出可以使用多边形碰撞器的 OverlapPoint 方法来判断一个点是否在多边形内部，可是经过我测试，这种方式和宣雨松提供的方法有着类似地问题，无论是否对坐标系进行转换，这个方法都返回 false，响应区域与上图完全一致。\n所以不管网络上有没有高质量的内容，一个核心的问题是你能否从中找到答案。如果你可以直接找到解决方案这可能是最好的结局；如果找不到直接的解决方案，却能够有所启发并独立解决问题，这是我们希望看到的结果。可是有时候人们并不这样想啊，人们想得到的是可以运行的代码而非解决问题的思路，因为可能人们并不想解决这个问题。\n好了，经过知乎上相关答案我找到了这篇文章，文章中提到了判断一个点是否在任意多边形内部的两种方法，分别为 Corssing Number 和 Winding Number。这两种方法在理论层面的相关细节请大家自行阅读这篇文章，我们这里选择的是前者，其基本思想是计算从该点引出的射线与多边形边界橡胶的次数，当其为奇数时表示该点在多边形内部，当其为偶数时表示在多边形外部。这里有一个有意思的事情是宣雨松选择的方法应该是著名的Ray-Crossing算法，可是为什么在这里会出现这样的问题呢？\n孰是孰非，一切都交给实践来证明吧！下面是我根据文章中提供的算法改写的一段 C#代码：\nbool ContainsPoint2(Vector2[] polyPoints,Vector2 p) { // 统计射线和多边形交叉次数 int cn = 0; // 遍历多边形顶点数组中的每条边 for(int i=0; i \u0026lt; polyPoints.Length-1; i++) { // 正常情况下这一步骤可以忽略这里是为了统一坐标系 polyPoints [i].x += transform.GetComponent\u0026lt;RectTransform\u0026gt; ().position.x; polyPoints [i].y += transform.GetComponent\u0026lt;RectTransform\u0026gt; ().position.y; // 从当前位置发射向上向下两条射线 if(((polyPoints [i].y \u0026lt;= p.y) \u0026amp;\u0026amp; (polyPoints [i + 1].y \u0026gt; p.y)) || ((polyPoints [i].y \u0026gt; p.y) \u0026amp;\u0026amp; (polyPoints [i + 1].y \u0026lt;= p.y))) { //compute the actual edge-ray intersect x-coordinate float vt = (float)(p.y - polyPoints [i].y) / (polyPoints [i + 1].y - polyPoints [i].y); //p.x \u0026lt; intersect if(p.x \u0026lt; polyPoints [i].x + vt * (polyPoints [i + 1].x - polyPoints [i].x)) ++cn; } } // 实际测试发现cn为0的情况即为宣雨松算法中存在的问题 // 所以在这里进行屏蔽直接返回false这样就可以让透明区域不再响应 if (cn == 0) return false; // 返回true表示在多边形外部否则表示在多边形内部 return cn % 2 == 0; } 这段代码说实话我理解的不是很透彻，而且令人费解的是实际结论和算法结论完全相反，因为按照我现在这样的设计，当 cn 为偶数时返回为 true，此时应该表示该点再多边形外部啊，可是事实上我测试这段代码的时候，它居然是可以正常工作的，即当该方法返回 true 的时候我的点击确实是在多边形内部，所以这是一段可以正常工作同时让我感到费解的代码，而且当我屏蔽了 cn 为 0 的这种情况以后，现在它已经可以完美的工作了\n正五边形精灵\r同样的，我们这里使用一张正五边形的精灵图片，然后编写下面的代码：\n/* * 基于多边形碰撞器实现的不规则按钮 * 作者：PayneQin * 日期：2016年7月9日 */ using UnityEngine; using System.Collections; using UnityEngine.UI; using UnityEngine.EventSystems; public class UnregularButtonWithCollider : MonoBehaviour,IPointerClickHandler { /// \u0026lt;summary\u0026gt; /// 多边形碰撞器 /// \u0026lt;/summary\u0026gt; PolygonCollider2D polygonCollider; void Start() { // 获取多边形碰撞器 polygonCollider = transform.GetComponent\u0026lt;PolygonCollider2D\u0026gt;(); } public void OnPointerClick(PointerEventData eventData) { // 对2D屏幕坐标系进行转换 Vector2 local; local.x = eventData.position.x - (float)Screen.width / 2.0f; local.y = eventData.position.y - (float)Screen.height / 2.0f; if(ContainsPoint(polygonCollider.points,local)) { Debug.Log (\u0026#34;这是一个正五边形!\u0026#34;); } } /// \u0026lt;summary\u0026gt; /// 判断指定点是否在给定的任意多边形内 /// \u0026lt;/summary\u0026gt; bool ContainsPoint(Vector2[] polyPoints,Vector2 p) { // 统计射线和多边形交叉次数 int cn = 0; // 遍历多边形顶点数组中的每条边 for(int i=0; i\u0026lt;polyPoints.Length-1; i++) { //正常情况下这一步骤可以忽略这里是为了统一坐标系 polyPoints [i].x += transform.GetComponent\u0026lt;RectTransform\u0026gt; ().position.x; polyPoints [i].y += transform.GetComponent\u0026lt;RectTransform\u0026gt; ().position.y; //从当前位置发射向上向下两条射线 if(((polyPoints [i].y \u0026lt;= p.y) \u0026amp;\u0026amp; (polyPoints [i + 1].y \u0026gt; p.y)) || ((polyPoints [i].y \u0026gt; p.y) \u0026amp;\u0026amp; (polyPoints [i + 1].y \u0026lt;= p.y))) { //compute the actual edge-ray intersect x-coordinate float vt = (float)(p.y - polyPoints [i].y) / (polyPoints [i + 1].y - polyPoints [i].y); //p.x \u0026lt; intersect if(p.x \u0026lt; polyPoints [i].x + vt * (polyPoints [i + 1].x - polyPoints [i].x)) ++cn; } } // 实际测试发现cn为0的情况即为宣雨松算法中存在的问题 // 所以在这里进行屏蔽直接返回false这样就可以让透明区域不再响应 if(cn == 0) return false; // 返回true表示在多边形外部否则表示在多边形内部 return cn % 2 == 0; } } 我们可以发现现在它可以正常工作啦！我们必须意识到的一点是，这个方法的空间复杂度为 O(n-1)，所以随着多边形顶点数目的增加，这个方法的执行效率会越来越低，如果对不规则精灵的边界没有十分苛刻的要求的话，我的建议是我们使用多边形碰撞器标记出一个相对模糊的边界即可，因为现在我们这个方法主要依靠数学计算，没有涉及到摄像机相关计算，所以宣雨松博客中有朋友指出他的方法仅仅适用于 Canvas 的模式为 Screen-Space Camera 这种情况，而我目前这个方法对除了 World Space 以外都是可以使用的，我最大的疑虑来自对鼠标位置进行转化的时候是否应该使用 Screen.width 和 Screen.height，因为我担心可能会出现屏幕适配这种需求。\n演示效果1\r精灵像素检测 精灵像素检测这个方案的灵感来自 Image 组件，我们在 MonoDevelop 或者 Visual Studio 中通过\u0026quot;转到定义\u0026quot;这个功能可以获得 Image 组件的内部细节。我们发现 uGUI 在处理控件是否被点击的时候，主要是根据 IsRaycastLocationValid 这个方法的返回值来进行判断的，而这个方法用到的基本原理则是判断指定点对应像素的 RGBA 数值中的 Alpha 是否大于某个指定临界值。例如，我们知道半透明通常是指 Alpha=0.5，而对一个.png 格式的图片来说半透明甚至完全透明的区域理论上不应该被响应的，所以根据这个原理我们只需要设定一个透明度的临界值然后对当前鼠标位置对应的像素进行判断就可以了，因此这种方法叫做精灵像素检测。\n下面我们来一起看这段 uGUI 的代码，这段代码通过 MonoDevelop 或者 Visual Studio 的\u0026quot;转到定义\u0026quot;功能可以找到，这里我做了简单的注释帮助大家理解代码：\npublic virtual bool IsRaycastLocationValid(Vector2 screenPoint, Camera eventCamera) { //当透明度\u0026gt;=1.0时，表示点击在可响应区域返回true if (this.m_EventAlphaThreshold \u0026gt;= 1f) { return true; } //当没有指定精灵时为什么要返回true? Sprite overrideSprite = this.overrideSprite; if (overrideSprite == null) { return true; } //坐标系转换\tVector2 local; RectTransformUtility.ScreenPointToLocalPointInRectangle(base.rectTransform, screenPoint, eventCamera, ref local); Rect pixelAdjustedRect = base.GetPixelAdjustedRect (); local.x += base.rectTransform.get_pivot ().x * pixelAdjustedRect.get_width (); local.y += base.rectTransform.get_pivot ().y * pixelAdjustedRect.get_height (); local = this.MapCoordinate(local, pixelAdjustedRect); Rect textureRect = overrideSprite.get_textureRect (); Vector2 vector = new Vector2(local.x / textureRect.get_width (), local.y / textureRect.get_height ()); //计算屏幕坐标对应的UV坐标 float num = Mathf.Lerp(textureRect.get_x (), textureRect.get_xMax (), vector.x) / (float)overrideSprite.get_texture().get_width(); float num2 = Mathf.Lerp(textureRect.get_y (), textureRect.get_yMax (), vector.y) / (float)overrideSprite.get_texture().get_height(); bool result; //核心方法：像素检测 try { result = (overrideSprite.get_texture().GetPixelBilinear(num, num2).a \u0026gt;= this.m_EventAlphaThreshold); } catch (UnityException ex) { Debug.LogError(\u0026#34;Using clickAlphaThreshold lower than 1 on Image whose sprite texture cannot be read. \u0026#34; + ex.Message + \u0026#34; Also make sure to disable sprite packing for this sprite.\u0026#34;, this); result = true; } //返回结果\treturn result; } 从这段代码中我们可以看出，这个方法核心在第 31 行代码，即传入一个 UV 坐标返回一个 RGBA 数值并将其和临界值相比较。可是在此之前，我们看到在引入 uGUI 及其专属组件 RectTransform 以后，现在 Unity 中的坐标系转换变得更加复杂了，我个人看到这部分代码是相当凌乱的，或许我应该找时间补习下矩阵变换了吧。所以现在我们就有思路啦，我们有两种方式，第一种基于这个思路重新定制一个 Image 组件;第二种直接修改 Image 组件的 eventAlphaThreshold 属性。考虑到坐标系转换这里非常复杂，显然第二种方式更容易接受，为什么这里可以直接修改 eventAlphaThreshold 属性呢，因为它在 Image 组件内部和代码中的 m_EventAlphaThreshold 相关联，这就是这篇文章的完整解释啦！\n圆形精灵图片\r好了，现在我们来一个简单的测试，我们这里准备一张圆形的精灵图片(如上图)，然后编写下面的代码：\n/* * 基于精灵像素检测实现的不规则按钮 * 作者：PayneQin * 日期：2016年7月9日 */ using UnityEngine; using System.Collections; using UnityEngine.UI; using UnityEngine.EventSystems; public class UnregularButtonWithPixel : MonoBehaviour,IPointerClickHandler { /// \u0026lt;summary\u0026gt; /// Image组件 /// \u0026lt;/summary\u0026gt; private Image image; /// \u0026lt;summary\u0026gt; /// 透明度临界值 /// \u0026lt;/summary\u0026gt; [Range(0.0f,0.5f)] public float Alpha; public void Start() { //获取Image组件 image = transform.GetComponent\u0026lt;Image\u0026gt;(); //设定透明度临界值 image.eventAlphaThreshold = Alpha; } public void OnPointerClick(PointerEventData eventData) { Debug.Log(\u0026#34;这是一个圆形!\u0026#34;); } } 这里我为了让大家在学(复)习(制)的时候更容易理解，我在 Click 事件的响应上，使用的是实现 IPointerClickHandler 接口这种方法，希望通过动态绑定这种方式添加事件响应的可以自己解决，我是不会为了满足你们的好(懒)奇(惰)而奉献出我的 EventTriggerListener 的代码的。好了，现在我们要做的就是为需要响应点击的不规则精灵附加该脚本，这样就可以解决不规则精灵响应的问题了。这种方法使用起来非常简单，需要注意的是：图片的类型必须是 Advance 且保证可读可写。因为我们在脚本中访问了像素，而简单伴随着的代价就是我们无法使用图集、该图片在内存中会复制一份，所以在项目性能上允许的情况下这种方法还是可以考虑使用的。\n演示效果2\r小结 本文通过对网络上两种比较通用的不规则按钮制作方案进行对比和研究，解决了基于多边形碰撞器实现不规则按钮这个过程中存在的问题，剖析了基于精灵像素检测实现不规则按钮 这个过程的内部原理，从易用性角度来讲，后者要优于前者，而这种方法的缺陷主要来自于它对图片类型的限制以及允许像素可读写这两个方面，它必须是 Advance 类型，所以普通的 Texture 或者 Sprite 拥有的特性在这里它都无法享受，比如我们无法为其做颜色渐变这类 Tween 动画、无法使用精灵特有的图集特性等等，于此同时它必须允许像素可读写，因此在实际使用中它会在内存中复制一份，在执行效率上可能会受到影响。而从技术性角度来讲，我个人更推推崇前者，因为在这个过程中我们学到了新的知识，明白了如何利用一个算法来解决实际的问题，而且它不会限制我们对精灵的使用，所有精灵拥有的特性在这里我们都可以使用，无非是在寻找算法、解决问题的过程中我们耗费了大量精力，可是这是值得的啊，不是吗？这就是我们做这件事情的意义所在。从昨天开始研究这两个问题到今天写完整篇文章，整个人是非常疲惫的，欢迎大家继续关注我的博客，今天的内容就是这样啦，谢谢大家！\n",
            "tags": ["Unity3D","uGUI","游戏开发"],
            "categories": ["Unity3D"],
            "thumbnail": ""
        },
        {
            "title": "使用 Mono 让.NET 程序跨平台运行",
            "date": "2016-03-06",
            "permalink": "http://localhost:1313/posts/1836680899/",
            "summary": "众所周知，Unity3D 引擎凭借着强大的跨平台能力而备受开发者的青睐，在跨平台应用开发渐渐成为主流的今天，具备跨平台开发能力对程序员来说就显得特别重要。传统的针对不同平台进行开发的方式常常让开发者顾此失彼，难以保证应用程序在不同的平台都有着相同的、出色的体验，这种情况下寻找到一种跨平台开发的方式将会为解决这个问题找到一种思路。从目前的开发环境来看，Web 应该是最有可能成为跨平台开发的神兵利器，可是长期以来 Web 开发中前端和后端都有各自不同的工作流，虽然现在出现了前端和后端逐渐融合的趋势，可在博主看来想让 Web 开发变得像传统开发这样简单还需要一定的过渡期。\n",
            "content": "众所周知，Unity3D 引擎凭借着强大的跨平台能力而备受开发者的青睐，在跨平台应用开发渐渐成为主流的今天，具备跨平台开发能力对程序员来说就显得特别重要。传统的针对不同平台进行开发的方式常常让开发者顾此失彼，难以保证应用程序在不同的平台都有着相同的、出色的体验，这种情况下寻找到一种跨平台开发的方式将会为解决这个问题找到一种思路。从目前的开发环境来看，Web 应该是最有可能成为跨平台开发的神兵利器，可是长期以来 Web 开发中前端和后端都有各自不同的工作流，虽然现在出现了前端和后端逐渐融合的趋势，可在博主看来想让 Web 开发变得像传统开发这样简单还需要一定的过渡期。\n从 Mono 到 Xamarin 对 Unity3D 来说，Mono 是实现它跨平台的核心技术。Mono 是一个旨在使得.NET 在 Linux 上运行的开源项目。它通过内置的 C#语言编译器、CLR 运行时和各种类库，可以使.NET 应用程序运行在 Windows、Linux、FreeBSD 等不同的平台上。而在商业领域，Xamarin 则实现了用 C#编写 Android 和 iOS 应用的伟大创举。Windows10 发布的时候，微软提出了通用应用 UWP 的设想，在这种设想下开发者可以直接在最新的 Visual Studio 中使用 C#编写跨平台应用。最近微软收购了 Xamarin，这一举措能够保证 Xamarin 这样的商业项目可以和微软的产品融合地更好。虽然在传统 Web 开发中 Java 和 PHP 目前占据主要优势，可是虽然云计算技术的流行，服务器成本的降低或许会让 C#这样优秀的语言更加成熟。我一直坚信技术没有好坏的区别，一切技术问题的核心是人，所以接下来，我们打算追随着跨平台开发的先驱——Java，最早提出的“一次编写、到处运行”的伟大思想来探索 C#程序跨平台的可能性。\nMono 跨平台的原理 在提到 Mono 跨平台的时候，我们首先需要引入公共语言基础(Common Language Infrastructure，CLI)这个概念，CLI 是一套 ECMA 定义的标准，它定义了一个和语言无关的跨体系结构的运行环境，这使得开发者可以用规范定义内各种高级语言来开发软件，并且无需修正即可让软件运行在不同的计算机体系结构上。因此我们可以说跨平台的原理是因为我们定义了这样一个和语言无关的跨体系结构的运行环境规范，只要符合这个规范的应用程序都可以运行在不同的计算机体系结构上，即实现了跨平台。针对这个标准，微软实现了公共语言运行时（Common Language Runtime，CLR)，因此 CLR 是 CLI 的一个实现。我们熟悉的.NET 框架就是一个在 CLR 基础上采用系统虚拟机的编程平台，它为我们提供了支持多种编程语言如 C#、VB.NET、C++、Python 等。我们编写的 C#程序首先会被 C#编译器编译为公共中间语言即 CIL 或者是 MSIL(微软中间语言)，然后再由 CLR 转换为操作系统的原生代码（Native Code）。\n好了，现在我们来回答最开始的问题：Mono 为什么能够跨平台。我们回顾.NET 程序运行机制可以发现实现.NET 跨平台其实需要这三个关键：编译器、CLR 和基础类库。在.NET 下我们编写一个最简单的“Hello World”都需要 mscorlib.dll 这个动态链接库，因为.NET 框架已经为我们提供了这些，因为在我们的计算机上安装着.NET 框架，这是我们编写的应用程序能够在 Windows 下运行的原因。再回头来看 Mono，首先 Mono 和 CLR 一样，都是 CLI 这一标准的实现，所以我们可以理解为 Mono 实现了和微软提供给我们的类似的东西，因为微软的.NET 框架属于商业化闭源产品，所以 Mono 除了在实现 CLR 和编译器的同时实现了大量的基础库，而且在某种程度上 Mono 实现的版本与相同时期.NET 的版本有一定的差距，这点使用 Unity3D 开发游戏的朋友应该深有感触吧！这就决定了我们在将应用程序移植到目标平台时能否实现在目标平台上和当前平台上是否能够具有相同的体验。因为公共中间语言即 CIL 能够运行在所有实现了 CLI 标准的环境中，而 CLI 标准则是和具体的平台或者说 CPU 无关的，因此只要 Mono 运行时能够保证 CIL 的运行，就可以实现应用程序的跨平台。我们可以通过下面这张图来总结下这部分内容：\n开发第一个跨平台程序 下面我们来尝试开发第一个跨平台程序，我们使用 Visual Studio 或者 MonoDevelop 编写一个简单的控制台应用程序，为了减少这个程序对平台特性的依赖，我们这里选择 System 这个命名空间来实现最为基础的 Hello World，这意味着我们的应用程序没有使用任何除 mscorlib.dll 以外的库：\nusing System; namespace MonoApplication { class MainClass { public static void Main(string[] args) { Console.WriteLine(\u0026#34;Hello World!\u0026#34;); } } } 因为我们的计算机安装了.NET 框架，所以我们编写的这个程序会被 C#编译器编译为公共中间语言 CIL,然后再由 CLR 转换为 Native Code。通常情况下公共中间语言(CIL)会被存储到.il 文件中，可是在这里我们在编译的时候好像并没有看到这个文件的生成啊，这是因为这里生成的可执行文件(.exe)本质上是公共中间语言(CIL)形态的可执行文件。这一点我们可以通过 ildasm 这个工具来验证，该工具可以帮助我们查看 IL 代码，通常它位于 C:\\Program Files\\Microsoft SDKs\\Windows\\v7.0A\\bin 这个位置。下面是通过这个工具获得的 IL 代码：\n.method public hidebysig static void Main(string[] args) cil managed { .entrypoint // 代码大小 13 (0xd) .maxstack 8 IL_0000: nop IL_0001: ldstr \u0026#34;Hello World!\u0026#34; IL_0006: call void [mscorlib]System.Console::WriteLine(string) IL_000b: nop IL_000c: ret } // end of method MainClass::Main 可以看到这段代码和我们编写的程序中的 Main 方法完全对应，关于这段代码的含义，大家可以通过搜索引擎来了解 IL 代码的语法。因为我们这里想要说明的是，这里生成的可执行文件(.exe)从本质上来讲并非是一个可执行文件。因为它能否执行完全是取决于 CPU 的，这和我们直接用 C++编写的应用程序不同，我们知道不同的编译器如 Windows 下的 VC++和 Linux 下的 GCC 都是和硬件紧密相连的，所以我们编译的程序能够在各自的平台直接运行，即 CPU 是认识这些程序的。可是在.NET 这里就不一样了，因为我们通过 C#编译器即 csc.exe 编译出来的文件，其实是一个看起来像可执行文件，实际上却是一个和平台无关、和 CPU 无关的 IL 文件。\n那么我们就会感到迷茫了啊，平时我们编译完 C#程序双击就可以打开啊，哈哈，现在隆重请出.NET 程序的家长公共语言运行时(CLR)。公共语言运行时实际上是程序运行的监管者，程序运行的情况完全由运行时来决定。我们双击这个文件的时候，公共语言运行时会将其加载到内存中，然后由即时编译器(JIT)来识别 IL 文件，然后由 CPU 去完成相应的操作。\n所以我们可以这样理解.NET 程序跨平台，因为 IL 文件是一个和平台无关、和 CPU 无关的、跨平台的文件结构，所以我们只需要在不同的平台上实现这样一个公共语言运行时(CLR)就可以实现在不同的平台上运行同一个程序。但这个过程中，需要有一个 C#编译器负责将 C#代码转换为 IL 代码，然后需要有一个公共语言运行时(CLR)来解析 IL 代码。与此同时，我们在.NET 框架下使用了大量的基础类库，这些类库在 Windows 以外的平台是没有的，所以除了 C#编译器和公共语言运行时以外，我们还需要基础类库。现在大家是不是对 Mono 有了更清楚的认识了呢？没错，Mono 所做的事情其实就是我们在讨论的这些事情。这里博主想说说即时编译(JIT)和静态编译(AOT)，这两种编译方式我们可以按照\u0026quot;解释型\u0026quot;和\u0026quot;编译型\u0026quot;来理解,为什么 Unity3D 在 iOS 平台上做热更新的时候会出现问题呢？这是因为 iOS 平台考虑到安全性禁止使用 JIT 即时编译，所以像 C#这种需要编译的语言在这里就无计可施了。\n好了，既然我们有 Mono 这样的工具能够帮助我们实现跨平台开发。那么我们现在就来考虑将这个程序移植到 Linux 平台，这里以 Linux Deepin 为例，我们按照 C#程序编译的过程来完成这个移植过程：\n1、将 C#程序编译为 IL 文件：在.NET 下我们使用 csc.exe 这个程序来完成编译，在 Mono 下我们使用 mcs.exe 这个程序来完成编译，这个程序在安装完 Mono 以后在其安装目录内可以找到。我们在命令行下输入命令： mcs D:\\项目管理\\CSharp\\MonoApplication\\MonoApplication\\Main.cs 2、这样将生成 Main.exe 这样一个 IL 文件，现在我们需要一个运行时来解析它，在.NET 下我们使用 CLR 来完成这个步骤，在 Mono 下我们使用 mono.exe 这个文件来完成这个步骤。我们在命令行下输入下列命令： mono D:\\项目管理\\CSharp\\MonoApplication\\MonoApplication\\Main.exe 在Mono中运行.NET程序\r我们可以看到命令行下输出了我们期望的 Hello World，这意味着我们编写的程序现在运行在 Mono 中了，实际上在 Windows 下由 Mono 提供的 C#编译器 mcs.exe 编译的 IL 文件双击是可以直接运行的，因为我们的计算机上安装了 CLR，它作为.NET 的一部分内置在我们的计算机中。由此我们会发现一个问题，我们这里的跨平台实际上是编译器、运行时和基础类库这三部分的跨平台，这意味着我们在 Linux 下运行.NET 程序是需要 Mono 提供支持的。因为在这里我无法在 Linux 离线安装 Mono，所以 Linux 下运行.NET 程序的验证需要等博主以后有时间再来更新啦！可是我们可以想象到，通过 C#编译器编译得到的可执行文件在 Linux 下是无法正常运行的，因为通常情况下 Windows 程序在 Linux 下运行是需要虚拟机环境或者 Wine 这样的软件来支持的，显然让这样一个 Windows 程序运行在 Linux 环境下是因为我们在 Linux 下安装了 Mono。\n谈谈 Mono 跨平台以后 好了，到现在为止我们基本理清了 Mono 跨平台的原理。我们知道微软的技术体系在发展过程中因为某些历史遗留问题，.NET 程序在不同的 Windows 版本中的兼容性有时候会出现问题，虽然微软宣布 Windows XP 停止维护，我们编写 Windows 应用程序的时候可以忽略对 Windows XP 版本的支持，可是因为国内用户不喜欢在线更新补丁的这种普遍现状，所以假如让用户在安装程序的时候先去安装.NET 框架一定会降低用户体验，其次.NET 框架会增加应用程序安装包的大小，所以我们需要一种能够让我们开发的.NET 应用程序在脱离微软的这套技术体系时，同时能够安全、稳定的运行，所以我们这里考虑借助 Mono 让.NET 程序脱离.NET 框架运行。\n首先，我们来说说.NET 程序为什么能够脱离.NET 框架运行，我们注意到 Mono 提供了一个 Mono 运行时，所以我们可以借助这样一个运行时来运行编译器生成的 IL 代码。我们继续以 Hello World 为例，我们在使用 Mono 编译出 IL 代码以后需要使用 Mono 运行时来解析 IL 代码，所以假如我们可以编写一个程序来调用 Mono 运行时就可以解决这个问题。在这个问题中，其实精简应用程序安装包的大小从本质上来讲就是解决基础类库的依赖问题，因为 Mono 实现了.NET 框架中大部分的基础类库，所以移植.NET 应用程序的关键是基础类库的移植，比如 WinForm 在 Linux 下的解决方案是 GTK，这些细节在考虑跨平台的时候都是非常重要的问题。\n小结 本文从 Mono 跨平台的原理说起，探讨了.NET 应用程序跨平台的可能性和具体实现。跨平台是一个涉及到非常多内容的话题，我个人理解的跨平台是要编写跨平台的代码，这意味着我们在编写程序的时候需要考虑减少对平台特性的移植，比如说 Linq 是一个非常棒的特性，可是这个特性离开了 Windows、离开了.NET 就没有办法得到保证，所以如果要让使用了 Linq 的应用程序跨平台就会是一件非常麻烦的事情！在不同的平台间保持相同的体验很难，就像我们编写的 Web 程序在不同的浏览器间都有着不一样的表现，所以跨平台这个问题我们就抱着学习的态度来研究吧！\n",
            "tags": ["跨平台","Mono",".NET","Linux"],
            "categories": ["编程语言"],
            "thumbnail": ""
        },
        {
            "title": "《仙剑奇侠传六》游戏感言",
            "date": "2015-07-24",
            "permalink": "http://localhost:1313/posts/1118169753/",
            "summary": " 目前游戏主线剧情进行到 50%左右，在游戏尚未通关前，我对于这一部游戏的感觉始终是一种说不清道不明的情感，作为仙剑系列中唯一一部，从项目立项到宣传曝光再到游戏上市整个过程中持续关注的游戏，它可以说是承载了无数玩家的期待和祝福。和大部分玩家一样，在游戏曝光的第一时刻我们曾经热火朝天地讨论过各种各样可能的设定、曾经为这部游戏的系统玩家想过各种各样的尝试，然而当我面对这款游戏的时候，我的内心平静得像一潭死水。我今天 23 岁，刚刚从大学毕业的我本应该还没有被这个社会完全改变，可我不知道是我变了还是仙剑变了，这一次打开仙剑的时候，我总有一种恍若隔世的恍惚感。\n",
            "content": " 目前游戏主线剧情进行到 50%左右，在游戏尚未通关前，我对于这一部游戏的感觉始终是一种说不清道不明的情感，作为仙剑系列中唯一一部，从项目立项到宣传曝光再到游戏上市整个过程中持续关注的游戏，它可以说是承载了无数玩家的期待和祝福。和大部分玩家一样，在游戏曝光的第一时刻我们曾经热火朝天地讨论过各种各样可能的设定、曾经为这部游戏的系统玩家想过各种各样的尝试，然而当我面对这款游戏的时候，我的内心平静得像一潭死水。我今天 23 岁，刚刚从大学毕业的我本应该还没有被这个社会完全改变，可我不知道是我变了还是仙剑变了，这一次打开仙剑的时候，我总有一种恍若隔世的恍惚感。\n引子 曾经，仙剑一的游戏开始界面是简单到不能再简单的竹简、酒葫芦、剑，这些元素组合起来就仗剑江湖的行侠仗义、白云苍狗的醉梦人生和徐徐道来的温暖故事；曾经，仙剑三的《御剑江湖》伴随着云山雾绕的蜀山像一幅遗留在历史深处的卷轴缓缓地打开让人不由得心头一阵惊艳；曾经，仙三外传开头蜀山掌门大战狼妖，无数道剑气凝成的剑柱从天而降可以让你感受到那种仙家道法的玄妙和奇幻；曾经，仙剑四的《回梦游仙》在耳畔响起的时候卷云台像朵清新脱俗的莲花静静地盛开却在最后一刻明白这朵莲花是一切悲剧的开始；曾经，仙剑五前传的明州码头在夕阳和晚霞的交相辉映中，瑾轩和瑕妹依偎在一起看着落霞与孤鹜齐飞是永远的温馨画面……\n可是仙剑六让我看到了什么呢？我看到了一片经过高斯模糊的绿油油的草地，没错！它真的是一片绿油油的草地，以至于当我打开这个游戏的时候我不得不在心里问自己：这真的是一个古风的仙侠/武侠游戏吗？在仙剑奇侠传六的宣传阶段，我在我的 QQ 群里、我的博客中不断向大家输送着这样一个概念：仙剑六是仙剑奇侠传系列二十年的突破之作，它一定不会让大家失望的，甚至我不遗余力地和游戏圈子里使用 Unity3D 引擎开发游戏的朋友们说，看，仙剑六是使用 Unity3D 引擎开发的，它的画面是历代游戏中最好的，这一次它终于要尝试即时战斗了。然而当我打开这个游戏的时候，我知道自己会被打脸，尤其是当我面对接受了我的这些观点的朋友的时候。\n对于此次仙剑六游戏优化的问题，我不想做详细的说明，我更不想为北软洗白，当我们觉得仙剑奇侠传六之泰坦陨落变成一个笑话的时候，我觉得我们可以说说 Unity3D 这个游戏引擎了。我承认，Unity3D 是个手机游戏引擎；我承认，Unity3D 在画面表现上无法和 UE、CE 这些顶级的游戏引擎相提并论；我承认，现在国内各种各样的 Unity3D 教程满天飞；我承认，Unity3D 入门快、成本低、跨平台性能强……可是这些都不是你们不用心做仙剑六的理由好吗？在我看来，技术从来都没有优劣之分，真正让技术体现出差异的是使用工具的人。Unity3D 本质上并非是一个差劲的游戏引擎，所以以引擎来论仙剑六的成败是不恰当而且不应该的，因为使用 Unity3D 开发的游戏目前已经相当的多了，比如《蒸汽之城》、《太空冒险》、《新仙剑 OL》等等以及无数的手机游戏。可是能将这个引擎用到如此地步的恐怕只有仙剑六吧！目前 Unity3D 的授权方式是收入超过 3 万美元即 18 万人民币左右需要按照一定的比例向 Unity3D 官方支付费用，我不知道这次的优化问题是否会对仙剑六的销量产生影响，可是我觉得恐怕官方都会认为这款游戏存在影响 Unity3D 引擎声誉的问题吧！\n画面 首先我们来说说游戏画面，我不知道有多少玩家可以在这款游戏中保证特效全开，总之在我的电脑上看起来整个画面有一种糊糊的感觉，远景看起来比较唯美壮丽，然而当我将镜头拉近的时候我觉得我还是不要计较仙剑奇侠传这个游戏的画面了吧！可是我真的不理解，作为仙剑奇侠传系列的好兄弟轩辕剑系列在使用 Unity3D 制作了两款游戏这样的背景下，北软为什么就不肯向 DOMO 小组学习哪怕借鉴相关的经验呢？虽然《轩辕剑六》恶名在外、《穹之扉》销量不佳，可是最起码人家的游戏的画面做得相对仙剑六要好很多好不好，况且人家在处理人物和场景时刻意加强了人物面部的特征，通过渲染景深和整体的光照使得画面透露出一种唯美的风格。可是仙剑六呢？仙剑六中做得最好看的永远都是人物的脸，我不知道北软是不是在有意告诉大家：这是一个看脸、靠脸的时代，所以当我们面对这个游戏的时候我们就发现整个游戏除了脸比较漂亮以外基本没有什么优点，可是事实上不同的人物在不同的场合、年龄他的面部应该都是不一样的啊，北软你把所有人的脸都做的这么漂亮，是想告诉玩家都不用去努力工作、只要拼脸就可以了吗？我不知道使用国外的 3A 级别的游戏引擎如 UE、CE 等来表现这种中国古典美的场景是否合适，因为这种类型的引擎更适合写实的渲染，而无疑中国的古风游戏需要的是一种意蕴上的美感的渲染。然而《古剑奇谭》和《轩辕剑》在表现这种场景时处理的相对来说是比较好的，这次的所有场景中我比较喜欢的是与青山，因为这个场景的色彩运用可以让玩家很明显的区分开场景中的不同的区域，反观忘尘寰、归墟、天晴之海、饮马河等场景因为使用的色彩较为接近，因为在玩家探索迷宫的时候常常搞晕，再辅以本次游戏中那个神奇的相机视角，探索迷宫的乐趣真是大大的增强啊！我不理解为什么北软连全局光照这种只需要简单设置下参数的东西都不愿意用，却要花大力气在角色的头顶上放置点光源，你告诉我，放置点光源就是为了让角色的头顶亮一下、脸白一点吗？更不要说启魂邪教总坛里那些支持实时反射的水晶石了？难道你宁可要这种华而不实的效果，都要让玩家的计算机耗费资源去支持它吗？景安正武盟门前的那条河的果冻绿材质就不能让美术想办法替换下吗？难道怕玩家不知道你是使用的 Unity3D 里的标准材质？\n下面是游戏中主角及配角的面部截图，颜值爆表啊！\n果然还是看脸的\r下面是《古剑奇谭二》、《穹之扉》、《仙剑六》三部游戏在特效全开的情况下的画面表现，相信高下立判了吧！\n古剑奇谭2最高画质\r穹之扉最高画质\r仙剑六最高画质\r建模 好了，下面说说建模的问题，我使用 disunity 对仙剑六的部分.unity3d 文件进行了解包，然后发现每个模型文件的包大概在 10M 左右，像太空步、循环动画、穿模这种问题我就不说了，反正每次说了你们又不打算改。我就来说说这个游戏里的模型吧？对三个模型进行了解包，然后发现这三个模型的单位都是不一样的，就是说在 scale 为 1、1、1 的时候三个模型的大小是不一样的；其次模型的角度需要手动改为-90,180,0，我觉得建模的时候难道不应该制定相关的规范吗？我觉得从 Max、Maya 里导出到 FBX 到 Unity3D 的时候难道不应该规范单位、角度和中心点位置吗？我从来不认为游戏引擎就是美术把模型做好了给程序用就行了，我觉得美术在建模的时候更应该去关注模型在这个引擎下的渲染效果，如怎么调整材质、怎么打灯光等等的问题，这些问题不应该推给程序而且不能推给程序。从模型贴图来看，美术想到了诸如法线贴图等等的次世代特性，可是到了实际使用的时候，我看到的结果的是整个游戏里基本清一色的使用了 Diffuse 着色器，那请问这样做这些贴图有什么意义？既然你根本用不到为什么还要放到游戏里？而且我在模型文件中经常看到诸如 Object01 或者 A_toufa、B_yifu 这样的命名，我是一个程序员，对命名比较敏感，我觉得出现汉语拼音式的命名，说明建模的人是特别不专业的。然后我想说的是这次整体美术风格的问题，难道大家不觉得天晴之海的建筑风格偏欧式了吗？这是一个中国的古风游戏啊！难道大家不觉得盈辉堡的道路和房子都是一样的颜色吗？我在地图里转了半天才找到路啊！此次的配角如赢旭危和朔漩的建模普遍要比主角团好看多了，难道你们要开始学《古剑奇谭二》在游戏中潜伏隐藏主角团吗？我不知道一个 2015 年的游戏出现 NPC 配音时嘴巴不动是出于什么考虑？NPC 不重要吗？NPC 戏份没有主角团多可以忍、长得没有主角团帅可以忍，可是你剥夺人家说话的权利是什么鬼？还有骆驼移动的时候没有移动动画直接悬空移动又是想干什么？一个骆驼值得你使用刚体这样的移动方式去移动吗？关于游戏读条慢的情况，我自己测试了下、同时找了相关的资料去查阅，Unity3D 场景的异步加载的确有坑存在，可是我相信只要运用合适的方法是可以规避这个问题的，因为目前仅仅解了部分 AssetBundle 包的内容，所以对程序内部的一些东西还有待确定，等确定后会继续更新到这里。\n朔璇模型\r赢旭危模型\r剧情 剧情、配音、配乐这里放到一起说，因为这是仙剑六引以为豪的地方，此次的剧情主线有两条，即双越身世之谜和洛家双生子早逝之谜，将这两条线交织在一起的是横道众和柷敔间的矛盾冲突，这样的设定明显是继承了仙五前的多线程叙事方式，这样的叙事方式应该是值得肯定的。但是我不能理解整个游戏到底是以谁为叙事中心的，正如仙剑五是以主角姜云凡为叙事中心的，他所看到的一切推进着整个剧情的深入，再入仙剑五前传是以主角夏侯瑾轩为叙事中心的，围绕着为姜承洗刷冤屈、为瑕妹治病两条主线将所有相关的人或事联系了起来。可是仙剑六我真没看出来是以谁为中心的，整个主角团是仙剑史上最冷漠、最分裂的团队，将大家联系到一起的唯一理由就是存在感爆表的神农九泉，然而这并没有什么卵用，大家都是站在自己的立场上做着自己关心的事情。\n比如越今朝是霸道总裁“只有我一个人可以叫你祈”。一路上不是摸头就是捏脸，可惜手压根没有放到脸上去；再比如越祈是天真傻“我听今朝的”。一路上吃面吃得我都饿了，可是那碗鸡蛋面就是一张贴图啊，吃半天空气最后居然吃完了，我要向仙剑六的四位程序员致敬；再比如闲卿是典型的双标狗，一面要讨好老婆洛昭言和世侄小绣儿，一面还要做出一副闲适淡泊的样子，我都忍不住要为你的演技点赞；再比如耳光绣明绣，我觉得要么是美术和策划有仇，故意将这样一个凶狠的角色画成甜美可人的女神范儿，要么就是编剧经常看琼瑶剧比较热衷于打人耳光，一个武侠游戏有什么不满直接亮兵器不就好了，要是当年月如被逍遥在扬州城外欺负了直接打李逍遥一个耳光，我觉得这个角色恐怕要失去不少忠实粉丝吧;再比如说技术宅居士方这货总是一副“你们都是对的，怪我咯”的态度，我至今都想不明白他有什么不对的地方，既然大家都不拿你当朋友，你凭什么要为这样一群人牺牲豆包啊。我一直喜欢仙剑营造的那种朋友间比较温暖的情感，比如仙剑四里小紫英一句“承君此诺必守一生”就会让人觉得温暖，即使以后大家分开了彼此的心中还可以相互牵挂。可是仙剑六呢，那晚大家做一起赏月喝酒本来应该是彼此相互了解和认识的机会，结果大家都忙着去约会了，留下居十方一个人在哪里喝闷酒，甚至他喝醉了酒吐露心事主角团中竟然无一人听见，我严重怀疑编剧每次和同学聚会的时候都是那个抢着麦克风嘶吼却从来不会有人去安慰他的那个人，编剧啊，己所不欲勿施于人啊。我一直认为一个 RPG 游戏的核心在于代入感，就是说你要让玩家觉得他就是游戏中的主角。比如我们玩仙剑一的时候就感觉自己是李逍遥，仙五前谢叔单挑姜世离的时候我们就感觉自己是谢叔，这就是代入感。\n可是仙剑六呢，居然巧妙的避开了这一点，搞得从头到尾都像在看电影，不，应该是叫做在看幻灯片。我不知道仙六是不是借鉴了《古剑奇谭二》的叙事方式，整个叙述视角更像是以上帝俯视人间的视角在讲整个故事，如果说《古剑奇谭二》成就了流月城，那么仙剑六便成就了衡道众，而且编剧觉得为了和《古剑奇谭二》拉开差距，刻意让站在对立面的衡道众认识到自己的错误并对主角一行人提供了补偿。我承认，这让仙剑六在立意上有了深度，可是我接下来要说的就是你们的不对了。我们玩仙剑一的时候比武招亲、蜀山剑法、林家绝学、苗疆蛊术、五灵仙术我们从来不会觉得存在违和感，因为这些东西都是东方文化中已有或者说可以找到起源追朔的东西，可是仙剑六的编剧你告诉我整个仙剑六除了鲲鹏能够在庄子的《逍遥游》中找到记载以外，其他的这些是中国传统文化存在的吗？是，时空穿越早就有了，可是回魂仙梦和血濡回魂都无法改变已经发生的事情；是，在天上飞早就有了，可是蜀山仙剑派御剑飞行早在武侠小说、志怪小说中有记载，所以蜀山的御剑术不会存在丝毫的违和感，可是你搞个二十一世纪都未必有的飞行器是什么鬼，古时候尝试上天的人最多是在一个椅子上捆满火药，希望通过反冲力飞到天上去，结果为科学事业献身了，编剧你告诉我这是什么鬼。我真傻，我单单知道黑科技会在仙剑剧里出现，却不知道有一天会在仙剑游戏里出现，你告诉我御界枢的人都是外星人吗？我们使用智能手机、平板电脑不过四五年的样子，编剧你告诉我衡道众里的人是怎么做到的，他们是从未来穿越过去的嘛？好了，下面请允许我替历代仙剑中因为剧情需要而牺牲的各位男主角、女主角、男配角、女配角、小怪以及 Boss 说句公道话，为什么六代的人可以通过交换实现“不死”的愿望，而六代以前的就只能领便当？我知道编剧一定会说，因为这次我们采用了全新的以神农为中心的世界观，可是编剧好像忘了神农和女娲差不多是同时在宇宙中产生的吧？我觉得五代的 Boss 魔翳比较冤枉，冒着做坏人的危险、拼着命为魔界找来了水源，结果你说九泉之一的热海同样可以产生水源，我原本只要伤害洛埋名一人就可以取得水源，结果就因为你这奇怪的设定，五代造就了仙剑史上最大的牺牲，编剧啊编剧原来你是真正的幕后黑手，神马黑包子各种连携技全都弱爆了好吗？你把仙剑六的故事设定到仙五前的五六十年里难道不担心这个世界的变化跟不上你的节奏吗？编剧你一句话就让蜀山派这样的神权天授、依靠盘古之心存在于世间的正派组织荡然无存啊，你告诉我御剑术都在江湖上失传了，这是摆明了以后不会再出现蜀山或者御剑术的节奏吗？我乐意看到仙剑六在世界观上的变化，可是这个新的世界观应该是原来以女娲为中心的世界观的一种补充而非推翻啊，你提出了神农九泉的概念，我觉得这个设定可以让仙剑的题材变得新颖些，然并卵这一次就把九泉的故事差不多都讲完了，是想等下一部游戏立项的时候再次推翻这次的设定，编剧啊，你到底是来挖坑的还是扩展仙剑的游戏世界观的啊！\n仙剑六黑科技\r游戏性 下面我们来重点说说游戏性。你问我为什么要说游戏性啊？一个游戏、一个商业游戏不提游戏性你觉得提什么呢？首先我想强调一个观点，认为仙剑六只要剧情好就行了的朋友请向姚仙建议将仙剑做成一个动漫或者电影，这样大家连自动战斗都不用点了对吧！仙剑六的突破挺多的，可惜注重了量而不注重质，这样平均下来仙剑六的突破其实很少很少。首先，我们来说说开放地图的问题，因为地图开放了玩家可以自由探索的地方就多了，可是你要真的想做好开放地图，就应该认真的去设计空气墙而不是等玩家掉坑里出来的时候打开游戏菜单重新回到原点。因为你们在设置空气墙的时候不用心，在过饮马河和去落日部的路上，比如祈妹的隔空移物和今朝的凌空飞剑，我不会告诉你我是直接从两边的石头上跳过去的。抓猫是挺好玩的，可是你告诉我玩家站在树顶纹丝不动、走绳子如履平地是什么鬼，在没有对 Unity3D 内部集成的 Physic 物理引擎进行完全充分的了解的情况下，贸然使用这样的技术你确定你能驾驭得好吗？浮金堂我跳了一个下午没有跳过去，然后跳出各种 Bug，我终于明白这次为什么有人能玩到 70 个小时以上啦，恭喜北软你们终于知道了怎样延长一个游戏的时间。这次的开机关让居十方都觉得郁闷，因为每次需要开机关的时候都会提示“开机关这种事情还是让十方去做吧!”，当我终于庆幸有用得着十方的时候，这下轮到我郁闷了，难道小游戏就不能给点提示吗？我总得知道自己要做什么吧！划船我再转了不知多久以后才明白过来怎么控制船的移动方向和角度，这种小问题难道每次都要让大家说吗？这次的迷宫设计有了层次感和立体感，比如启魂邪教总坛的迷宫和机关设计得都不错，天晴之海得迷宫设计得比较好，然而我最喜欢的是与青山这个场景！\n我就放个图，不说话！\n浮金堂跳跃Bug\r好了，说完这些小游戏，我们来说说这次仙剑六的战斗系统。我想知道，究竟是什么样的一种考量让你们选择去模仿 FF13 的战斗模式，难道是为了刻意和《古剑奇谭二》有所不同？然并卵，这次的战斗系统糟糕透了。首先，我是希望仙剑的战斗系统慢慢地向着即时制的方向发展的，因为这是现在的大势所趋吧，尤其是《古剑奇谭》、《雨血》和《御天降魔传》这类游戏正在引领着大家的兴趣往即时方向转变，在这样的背景下仙剑积极地向即时制转变从某种程度上来讲是一种不得不采取的防御性措施。可是我有教过你用一个伪即时制的战斗系统来欺骗大家的感情吗？以前大家对排站好在每个回合里我们可以依次控制多名角色，然后依次释放技能，通过不同角色间策略的调整来将游戏进行下去；现在大家迈着太空步，在每个回合里我们可以控制一名角色，每次可以发动多次行动，其他角色由 AI 控制，然后场景中各种粒子特效乱飞。当我看到粒子特效贴图的矩形边框时，我的内心是奔溃的。你告诉我这样的战斗系统和回合制有什么区别？当初主企划说为了让大家更好的观赏战斗画面特意将 UI 做到了右下角，可是你告诉我在一个即时制的游戏里用眼角的余光扫视右下角然后用滚动条从一堆物品中选择需要的物品该有多蛋疼，等你选好了，队友或者玩家可能已经死了。所以我们的数值策划为明绣配置逆天的治疗数值，这样一来大家就不用吃药了。呵呵，是你们该吃药了吧，你告诉我一个游戏玩到现在我都没记住几个技能的名称，以后问起来大家提到万剑诀、天剑、酒神、真元护体、天罡战气、五灵归宗、乾坤一掷、气疗术、气指剑、万物归烬、仙风云体、千方残光剑等等经典招式的时候，我希望你们不要说我们厚此薄彼就好。我相信有好多妹子已经习惯开着自动战斗直接看剧情的习惯了吧，如果这样仙剑还不如买小说或者拍动漫呢，正好这次所有的过场动画都是 2D 动画的形式，可是你告诉我 2D 动画是一种风格、3D 建模是一种风格、小剧情表情是一种风格，一个游戏里三种风格，你是打算同时照顾动漫和游戏两个不同群体的玩家吗？然并卵，你这样做了不见得人家会领情，人家会说你抄袭、撞梗，你说你又是何苦呢？我给仙剑六战斗系统提点意见吧，希望可以支持玩家自定义快捷键，比如玩家可以挑选自己喜欢的技能和常用的物品，每次使用都会消耗行动点数，行动点数目消耗完了就触发技能动画，这样至少可以让点鼠标变得高端些，就像英雄联盟说白了就是 Q、W、E 三个键各种按，可是你同样可以装 X 地说这里面涉及到走位和意识。对了，灵脉系统界面能不能点击了以后不要放大，你觉得那样真的好看吗？再说三种培养方式我非得一条路走到黑？\n战斗系统截图1\r战斗系统截图2\r好了，熬夜到凌晨三点写完这篇文章，我对仙剑绝对是真爱，我知道一定会有许多人来吐槽我写的这篇吐槽，可我想说的是：你要真的爱它就别总是惯着它，真正的爱从来都不是溺爱！在官方放出第三版补丁后，整个游戏的优化得到了较好的提升，从感官上像个游戏了，如果有朋友还在徘徊不定，不妨在这个时候尝试下吧！以上观点，一家之言，不足为据！\n",
            "tags": ["仙剑奇侠传","RPG","Unity3D","游戏"],
            "categories": ["单机游戏"],
            "thumbnail": ""
        },
        {
            "title": "Unity3D 游戏开发之从 Unity3D 项目版本控制说起",
            "date": "2015-07-02",
            "permalink": "http://localhost:1313/posts/1320325685/",
            "summary": "各位朋友，大家好，欢迎大家关注我的博客，我是秦元培，我的独立博客地址是：http://blog.yuanpei.me、CSDN 博客地址是：http://blog.csdn.net/qinyuanpei。今天我想和大家聊聊 Unity3D 游戏项目的版本控制。\n",
            "content": "各位朋友，大家好，欢迎大家关注我的博客，我是秦元培，我的独立博客地址是：http://blog.yuanpei.me、CSDN 博客地址是：http://blog.csdn.net/qinyuanpei。今天我想和大家聊聊 Unity3D 游戏项目的版本控制。\n为什么要进行版本控制 当我一个人写代码的时候，在我的脑海中是不存在版本控制这个概念的，因为我对整个项目的代码如数家珍。可是当我和一群人在一起写代码的时候，我可能并不会清楚团队中有谁修改了哪一行代码，即使是一个变量的名称或者是一个函数的名称，在我毫不知情的情况下，可能这样的修改会使得程序无法运行，这个时候我需要版本控制；尽管 Unity3D 是一个适合小团队开发的游戏引擎，可是即使再小的团队同样会有不同的分工，当大家需要将各自的工作合并到一个完整的项目中的时候，这个时候我需要版本控制；当我需要了解团队成员实际的编程能力的时候，最好的方法是让他们参与到一个项目的开发中，这样我可以从他提交代码的情况了解他的工作能力，这个时候我需要版本控制；当我希望时时刻刻对项目进行备份，并在某一个关键的时刻将项目恢复到一个正确的状态的时候，复制、黏贴不会让这个工作变得简单，这个时候我需要版本控制。\n怎样在 Unity3D 中进行版本控制 在 Unity3D 中进行版本控制主要针对 Assets 和 ProjectSetting 这两个文件夹，因为除此以外的文件和文件夹都是 Unity3D 在运行过程中产生的临时文件，这些文件会在使用 Unity3D 打开项目后重新生成，因此无需对这些文件或文件夹进行版本控制。好了，在了解了 Unity3D 版本控制中需要关注的主要内容后，接下来我们要关注的是怎样让版本控制的软件对我们提交的内容进行差异化识别，我们知道版本控制的一个核心任务就是将服务器上的文件和本地的文件进行比对，找出哪些文件是最新生成的、哪些文件是被修改过的等等。因此为了方便版本控制软件对文件进行比对，常常需要项目变动的这些因素转化为文本形式，如果熟悉 Github 的朋友应该知道，Github 中判断两个文件的差异就是根据文本(代码)来比较的，因此在 Unity3D 中使用版本控制同样需要遵循这个原则，好在 Unity3D 在管理 Unity3D 项目时已经考虑到了这一点，通常在对 Unity3D 项目进行版本控制的时候，我们需要做这样的事情：\n通过 Edit-\u0026gt;Project Settings-\u0026gt;Editor 菜单打开编辑器设置选项，将 Version Control 选项下的 Mode 设为 Visual Meta Files，这样 Unity3D 将为项目中的每个文件或者每个文件夹生成对应的.Meta 文件。该文件是一个文本文件，记录了对应文件的相关信息，版本控制软件可以以此来对文件版本进行对比和合并操作。\nUnity3D 中的资源默认是以二进制的形式进行组织的，这种组织方式对版本控制来说是不合适的，因此需要通过通过 Edit-\u0026gt;Project Settings-\u0026gt;Editor 菜单打开编辑器设置选项，将 Asset Serialization 下的 Mode 设为 Force Text。\n通过 Edit-\u0026gt;Prefences-\u0026gt;External Tools 找到 Revision Control Diff/Merge 选项，在安装了版本控制软件后可以在这里找到相关的选项，以博主为例，博主使用的是 TortoiseSVN，这里的选项是 TortoiseMegre。目前 Unity3D 支持的版本控制软件有 SourceGear DiffMerge、TKDiff、P4Megre、TortoiseMegre、WinMegre、PlasticSCM Megre。\n编辑器设置\r编辑器设置\r好了，在完成以上准备工作后，我们就可以开始进行 Unity3D 项目的版本控制了，目前在 Unity3D 中我们主要有以下三种方式来对 Unity3D 项目进行版本控制：\n使用 Asset Server 进行版本控制 Unity3D 的Asset Server是一个 Unity3D 内部集成的版本控制软件，它和我们熟知的 SVN 类似，适合在小团队内进行版本控制，这是一个收费软件，尽管在某些方面它甚至比 SVN 还要方便，不过在实际的项目中使用这个的还是比较少的，所以如果大家对这个感兴趣，可以从这里了解它的具体情况，这里我们不打算介绍这个软件的使用。\nUnity3D 游戏制作（四）——Asset Server 搭建\n【教程】Asset Server（联合开发）\n使用 Github 进行版本控制 使用 Github 进行版本控制时可以在 Git 仓库中添加一个.gitignore 文件来对项目中需要同步的文件进行过滤，在文章开始我们已经知道 Unity3D 项目的版本控制主要针对 Assets 和 ProjectSetting 这两个文件，因此.gitignore 的内容可以这样填写:\nLibrary/ Temp/ *.sln *.csproj *.sln *.userprefs *.unityproj *.DS_Store 这样每次提交文件的时候 Github 将忽略这些文件的更改。关于 Github 的使用及其相关命令可以查看这里：\n总结自己的 Git 常用命令\nGit 远程操作详解\nGithub 中每个仓库的容量限制为 1G，适合小项目的版本控制，对于大型项目的版本控制应该考虑使用 SVN。\n使用 SVN 进行版本控制 使用 SVN 进行版本控制时可以通过右键菜单将某些文件和文件夹添加到忽略的文件列表中，这样 SVN 在每次提交文件的时候将忽略这些文件的更改。这块儿其实和 Github 的.gitignore 是相同的。SVN 常用的软件组合是 TortoiseSVN(客户端)+VisualSVN Server(服务端)，具体内容请参考这 2 篇文章：SVN 使用教程总结和客户端 TortoiseSVN 的安装及使用方法\n小结 不管使用什么版本控制软件，建立相关的代码提交规范和流程控制规范都是必要的，因此在团队中应该有一个人负责对团队成员提交的代码进行审核和规范化，这样可以减少因为因为代码提交而产生的各种问题。好了，今天这篇文章先写到这里了，希望大家喜欢！\n",
            "tags": ["版本控制","Unity3D","SVN","Github"],
            "categories": ["Unity3D"],
            "thumbnail": ""
        },
        {
            "title": "从「复活」和「暂停/恢复」谈游戏数据配置管理",
            "date": "2015-03-27",
            "permalink": "http://localhost:1313/posts/3356910090/",
            "summary": "随着游戏制作技术的不断发展，在经历了从 2D 到 3D、从单机到网游、从 PC 游戏到移动游戏的种种演变后，玩家对于游戏质量的要求越来越高，游戏制作的难度相应地增加，整个游戏研发的体系开始变得庞大而复杂，由此就产生了游戏数据配置和管理的相关问题。本文将从游戏中的\u0026quot;复活\u0026quot;和\u0026quot;暂停/恢复\u0026quot;这两个应用场景的角度来谈谈在游戏开发中如何对游戏中的数据进行管理和配置。\n",
            "content": "随着游戏制作技术的不断发展，在经历了从 2D 到 3D、从单机到网游、从 PC 游戏到移动游戏的种种演变后，玩家对于游戏质量的要求越来越高，游戏制作的难度相应地增加，整个游戏研发的体系开始变得庞大而复杂，由此就产生了游戏数据配置和管理的相关问题。本文将从游戏中的\u0026quot;复活\u0026quot;和\u0026quot;暂停/恢复\u0026quot;这两个应用场景的角度来谈谈在游戏开发中如何对游戏中的数据进行管理和配置。\n为什么要谈游戏数据的配置和管理 不知道大家是不是会和博主有一样的想法，就是当你回头来思考游戏开发的时候，你常常会发现，如果忽略游戏的画面、情节、特效等等这些游戏中的可视化的东西，那么其实游戏从本质上来说就是一个大型的有限状态机(FSM)，而我们通常所做的事情基本就是在维护这个有限状态机里面的各种状态，从游戏加载到游戏开始、从游戏开始到游戏中各种事件的发生再到各种事件影响到整个有限状态机的状态，我们通常所做的事情无外乎是在维护各种状态。这种感觉在 RPG 游戏中可能会更明显些，因为在 RPG 中玩家可能是在场景中行走或者奔跑、可能是在和场景中的某个 NPC 进行对话、可能是在和面前的敌人进行战斗、可能是在和杂货店的老板讨价还价……可以说在整个游戏当中无时无刻不在进行游戏状态的切换，那么在不同的状态间切换的时候，什么最为重要呢？答案是数据。什么是数据呢？玩家的生命值、魔法值、战斗力、防御力，物品的用途、价格、数量，游戏的剧情、对话、音乐等等这些都是数据。当我们在状态间进行切换的时候，其实真正改变的就是这些数据。由此可见，面对复杂而庞大的游戏体系，如何对游戏中的数据进行配置和管理是一件值得我们去思考的问题。\n从应用场景来看游戏数据的配置与管理 首先我们来从游戏当中的两个常见的应用场景:\u0026ldquo;复活\u0026quot;和\u0026quot;暂停/恢复\u0026quot;来看看游戏数据配置和管理的重要性。 这里以博主的一款跑酷游戏为例：\n游戏截图\r应用场景——\u0026ldquo;复活\u0026rdquo; \u0026ldquo;复活\u0026quot;是一个在游戏中特别常见的功能，复活这一设定的好处在于无需重新开始游戏就能再次回到游戏当中，当然这只是我们最为直观的一个感受，更为深刻的原因是，游戏者巧妙地利用了玩家在游戏任务失败那一刻的心理。现在生活中每一个人都喜欢胜利，这种心理到了游戏世界中同样是适用的，因为游戏的目的无非就是让玩家有种成就感以获得快乐。可是当游戏任务失败的时候，玩家会竭尽全力不断尝试去打败 Boss 以获得游戏的胜利，因此在游戏中有这样一个设定，可以引导玩家在游戏中形成消费的习惯，这样游戏就能从玩家身上盈利。好了，我们来看看一个基本的\u0026quot;复活\u0026quot;的逻辑吧！\nprivate void Update() { //如果玩家的生命值大于0则游戏正常进行 if(Player.Hp\u0026gt;0) { //游戏状态为Normal GameManager.Instance.GameState=GameStateEnum.Normal; //执行正常的游戏逻辑 DoNormalEvent(); }else { //游戏状态为Over GameManager.Instance.GameState=GameStateEnum.Over; //显示GameOver ShowGameOver(); //玩家复活 ReLive() } } 玩家复活需要做两件事情：\n将游戏的状态从 Over 调整到 Normal 将玩家的状态从死亡调整到正常 调整游戏的状态特别容易，因为 GameManager 是一个典型的单例模式，因此我们可以直接将 GameState 从 Over 变成 Noral。可是对于玩家状态的调整，我们却遇到了困难。问题出在什么地方呢？问题出在我们将玩家的生命值等一系列属性都写在了 PlayerController 这个类中，如果我们将玩家的属性全部都设为 Private，那么我们将无法从外部来调整这些属性。比如我们想让玩家满血复活，可是因为这些属性都是私有的，我们无法从外部访问，所以我们在给玩家恢复生命值的时候，无法获得玩家当前的生命值以及最大生命值。可是如果我们将玩家的属性全部都设为 Public，我们可能不得不去面对在编辑器窗口中为每一个属性去赋值，因为一旦我们试图调整游戏双方力量的平衡时，这将是我们不得不去面对的问题，更为致命的玩家的属性并不是永远不变的，比如在 RPG 游戏中玩家的生命值等属性会随着角色等级的提升而不断增加。因此不管我们将这些属性设为 Public 还是 Private，我们都无法保证每次访问到的这些数据都是最新的数据。换句话说，我们不能想当然地在脚本中将玩家的属性写成一个不变的值，因为这些数据随时都在发生着变化，当然如果像敌人和 Boss 这种数值相对稳定的情况，我们可以直接在脚本中将其写成一个固定值，不过我并不推荐大家这样做。由此可见，游戏中数据配置和管理的一个重要作用是维持各个状态间的正常切换。如图是雨血前传.蜃楼中的复活界面，每次复活需要消耗一个复活玉：\n雨血\r那么博主在这款跑酷游戏里面是怎样做这个复活的呢？因为博主当时在设计这个游戏的时候考虑不周，直接将玩家的生命值写成了 100，所以在复活玩家时候，同样是先将游戏的状态调整过来，然后再将相关的 GUI 窗口隐藏，然后将玩家的生命值重新设置为 100，重新生成玩家就好了。正是因为感觉这段时间做游戏缺乏一种良好的游戏架构，所以每次游戏做到最后都是自己把逼到了绝路上，留给了自己一个自己都不想再去维护的烂摊子，这样显然是不好的，所以以后需要在正式动手写代码前做好规划，相信这样就能够保证游戏的质量了吧！任何东西学习到一定阶段都会遭遇瓶颈，尽管打破这种瓶颈的过程是痛苦的，可是如果不去打破它，那么你永远都只能停留在这个位置。\n应用场景——\u0026ldquo;暂停/恢复\u0026rdquo; 和\u0026quot;复活\u0026quot;一样，\u0026ldquo;暂停/恢复\u0026quot;同样是一个在游戏中常见的功能，该功能是给了玩家暂时离开游戏的一种选择，可以保证玩家在做其它事情的时候不会影响到游戏的进程。比如在仙剑奇侠传、古剑奇谭等游戏中，玩家可以按下 ESC 键调出游戏设置界面，在玩家进入游戏设置界面的这段时间，游戏世界里的时间似乎是静止的，场景中的敌人不会因为玩家在查看系统设置界面就去主动偷袭玩家，因为这种情况下游戏是暂停的。而当玩家退出系统设置界面后，游戏恢复为正常状态。到了移动互联网时代，游戏中出现\u0026quot;暂停/恢复\u0026quot;的情况更为普遍，这是由移动互联网时代人们玩游戏更注重休闲和娱乐这样的性质来决定的。记得天天酷跑刚刚在微信上线的那段时间，我身边好多同学都在上课的时候玩，可是因为这游戏一跑起来就根本停不下来，所以经常是一次游戏玩下来一节课就结束了。博主不提倡这样啊，玩游戏归玩游戏，可是什么事情都要有个度啊，不然就会变成玩物丧志。好了，我们分析这个案例的目的无非就是想告诉大家在游戏里增加这样一个\u0026quot;暂停/恢复\u0026quot;的功能还是十分必要的。好了，现在我们来分析下在这个应用场景中发生状态转换的时候都会牵扯到那些数据吧！\n首先，游戏暂停后，场景内所有的物体都会停止运动，此时游戏中每个物体的状态都发生了变化，不过因为在 Unity3D 中控制游戏暂停/的恢复主要是通过调整 Time.timeScale 的值来实现的。当 Time.timeScale 取值为 0 时，游戏暂停；当 Time.timeScale 取值为 1 时，游戏恢复正常。不过需要注意的是 Time.timeScale 会对 Unity3D 中所有的时间产生影响如 FixedUpdate()、协程、Destroy()、动画组件等等，所以如果对暂停后的游戏状态有特殊要求的话，建议还是通过其它的方法来实现吧！这里没有提到 Update() 和 LaterUpdate() 这是因为这两个方法不会受到影响。我们来看这样一段代码：\n//游戏是否暂停 private bool isPause = false; //暂停/恢复游戏的方法 private void Resume() { if (!isPause) { Time.timeScale = 0; isPause = true; } else { Time.timeScale = 1; isPause = false; } } 通过这段代码我们就能够实现一个基本的游戏\u0026quot;暂停/恢复\u0026quot;的功能。在游戏管理类 GameManager 中我们定义了一个玩家的得分。正常情况下，当玩家没有死亡的时候会在 GUI 中更新玩家的得分，而玩家的得分是直接采用在 Update()中累加的方式实现的，因此玩家的得分会在游戏暂停后继续更新，这当然是不符合实际情况的，因此可以在这个增量前乘上一个 Time.deltaTime 就可以解决这个问题了。博主举这个例子无非就是想告诉大家使用这种方法来暂停游戏会存在这样的问题，希望大家以后注意啊！\n跑酷游戏复活界面\r游戏数据配置和管理的思路和方法 既然我们在今天的的文章中主要阐述的就是游戏数据配置和管理，那么下面我们就来说说游戏数据配置和管理的常见的思路和方法。根据游戏中数据变动的相对大小，我们将游戏中的数据分为静态数据和动态数据两类。\n静态数据 静态数据是指在游戏中基本不变或者不需要变动的数据。比如游戏中 Boss 的等级和生命值一般都是确定的，因此这种类型的数据可以称为静态数据。同样地，游戏中 NPC 对话的内容是一种静态数据，因为 NPC 的对话内容是在设计剧情的时候就设计好的无需再对它进行修改。那么对于静态数据，我们可以考虑下列方法：\n将静态数据作为常量定义在一个类中，这样做的好处是无需对每一个脚本进行修改。 将静态数据存储在文件当中，这样做的好处是可以对数据进行管理，缺点是需要针对不同的文件编写解析接口，游戏开发中常用的数据存储形式有：Json、Xml、Excel、CSV 等。 将静态数据存储在数据库当中，如 SQLIite 等，可是这样做的缺点同样很明显，从本地读取数据库会消耗大量的资源，而且数据库文件一旦丢失，整个游戏都将无法运行。 动态数据 动态数据是指在游戏中会不断变化的数据，比如玩家的得分、玩家的生命值、玩家的经验值等等。动态数据的处理方式除作为常量写在类中以外，其它的都和静态数据是一样的，在此就不再多说了。\n总结 可能今天这篇文章显得唠叨些，甚至从技术的角度来看，这篇文章都没有讲到什么有价值的技术要点。可是在博主看来，不管一项技术有多么伟大，如果没有良好的架构或者说结构，那么当这个项目的规模到了一定程度以后，这个项目就会出现问题。因为根据破窗户理论，当你看到窗户破了而不去及时修补的话，那么时间一长你破掉的就是整个房子了。回顾博主这么长时间的游戏开发，其实做过的好多游戏到最后之所以没有做完，都是因为到最后项目基本失控、变成了一个连自己都不愿意去维护的项目，这样的情况是可怕的。平时是你一个人做项目，可能你觉得这些都没有什么，可是当你和别人一起去完成这样一个项目的时候，你的这些问题都会成为整个团队的问题。博主一直想知道自己做游戏和团队在一起做游戏会有什么不同，因为博主感觉自己在这一块确实不是掌握得很好。虽然说架构这种事情你做多了才会有经验，可是你现在发现了问题，为什么不在现在改掉呢？架构真的很重要，致那些因为架构死去的项目，真正的项目应该死在实践中，因为架构的问题最终变得不可收拾的，这件事情本身就是可耻的。好了，今天就说这么多了。\n",
            "tags": ["游戏","数据","状态"],
            "categories": ["游戏开发"],
            "thumbnail": ""
        },
        {
            "title": "使用 Love2D 引擎开发贪吃蛇游戏",
            "date": "2015-03-10",
            "permalink": "http://localhost:1313/posts/426338252/",
            "summary": "今天来介绍博主最近捣腾的一个小游戏“贪吃蛇”。“贪吃蛇”这个游戏相信大家都不会感到陌生吧。今天博主将通过Love2D这款游戏引擎来为大家实现一个简单的贪吃蛇游戏,在本篇文章当中我们将会涉及到“贪吃蛇”的基本算法、Lua 语言编程等基本的内容，希望能够对大家开发类似的游戏提供借鉴和思考，文章中如有不足之处，还希望大家能够谅解，因为博主的游戏开发基本就是这样慢慢摸索着学习，所以难免会有不足的地方。\n",
            "content": "今天来介绍博主最近捣腾的一个小游戏“贪吃蛇”。“贪吃蛇”这个游戏相信大家都不会感到陌生吧。今天博主将通过Love2D这款游戏引擎来为大家实现一个简单的贪吃蛇游戏,在本篇文章当中我们将会涉及到“贪吃蛇”的基本算法、Lua 语言编程等基本的内容，希望能够对大家开发类似的游戏提供借鉴和思考，文章中如有不足之处，还希望大家能够谅解，因为博主的游戏开发基本就是这样慢慢摸索着学习，所以难免会有不足的地方。\n游戏算法 我们首先来看看贪吃蛇是怎么移动的？ 贪吃蛇游戏算法演示1\r贪吃蛇游戏算法演示2\r贪吃蛇游戏算法演示3\r贪吃蛇游戏算法演示4\r通过这四张图的演示，我们可以发现这样一个规律：\n蛇的移动其实是将蛇身体的最后一个元素移动到第一个元素的位置\n那么完成这样一个工作需要两个步骤：\n1、将在蛇头位置插入一个新的元素 2、移除蛇尾位置的最后一个元素\n好了，了解了蛇的移动后我们再来考虑一个问题，怎样判断蛇吃到了食物？思路和蛇的移动类似，主要考虑在蛇头插入的这个元素和食物的关系，如果这个元素的坐标和食物的坐标是相同的，那么就可以认为蛇吃到了食物，此时蛇的身体应该是变长的，所以只要在蛇头位置插入一个元素就可以了。反之，如果蛇没有吃到食物，那么蛇应该是移动的，所以就可以按照移动的方法来处理了。那么在蛇头位置插入的这个元素该如何确定呢？我们来看下面这段程序：\n--计算下一个目标点 function getNextPoint() --计算下一个目标点 snake = {} if (dir == 0) then snake.x = snakes[1].x snake.y = snakes[1].y - 20 end if (dir == 1) then snake.x = snakes[1].x snake.y = snakes[1].y + 20 end if (dir == 2) then snake.x = snakes[1].x - 20 snake.y = snakes[1].y end if (dir == 3) then snake.x = snakes[1].x + 20 snake.y = snakes[1].y end return snake end 这里定义了 getNextPoint()的方法，目的是计算在蛇头位置添加的下一个元素，这里我们注意到根据蛇的移动方向(dir)的不同，其中 0 表示上、1 表示下、2 表示左、3 表示右，计算出下一个元素的位置，因为在这个游戏中网格大小是 20，所以这里可以直接根据坐标来计算一个元素的位置。snakes 是一个 table，保存的是当前的蛇的全部元素的坐标。通过维护这个 table，我们就可以利用绘图的函数绘制出蛇的身体，这样蛇就可以移动起来了。我们来看看蛇是怎样移动的：\n--核心算法——蛇的移动 function SnakeUpdate() --获取元素个数 local n = table.maxn(snakes) if (table.maxn(snakes) \u0026gt; 0) then if (getNextPoint().x == foodX and getNextPoint().y == foodY) then --将下一个目标点的位置插入表中 table.insert(snakes, 1, getNextPoint()) --将食物状态设置为BeEated foodState=\u0026#34;BeEated\u0026#34; else --将下一个目标点的位置插入表中 table.insert(snakes, 1, getNextPoint()) --移除最后一个元素 table.remove(snakes,n+1) end end end 在这里我们定义了一个 foodState 变量以保存食物的状态，当食物的状态为 BeEated 的时候表示食物被蛇吃掉了，此时应该重新生成一个食物的坐标，此时事物的状态将变成 WaitToEat。食物的坐标保存在 foodX 和 foodY 这两个变量中，大家可以到完整的代码中去查看。\n游戏状态 我们知道蛇碰到四周墙壁的时候就会死亡，此时游戏结束。这个比较简单，只要判断蛇头的坐标和屏幕的关系就可以了。因为在这个游戏中屏幕的尺寸为 640X640，所以判断游戏是否结束的代码可以这样写：\n--判断游戏状态 if(snakes[1].x \u0026lt;= 0 or snakes[1].x \u0026gt;= 640 or snakes[1].y \u0026lt;= 0 or snakes[1].y \u0026gt;= 640) then gameState = 0 else gameState = 1 end 这里 gameState 为 0 表示游戏结束，gameState 为 1 表示游戏正常进行。\n完整代码 在完成了这些核心的算法以后，剩下的事情就交给 Love2D 引擎来绘制吧，最后给出完整的程序代码：\n--定义窗口宽度和高度 local w\t= 640 local h\t= 640 --定义网格单元大小 local unitSize = 20; --方块的初始位置 local initX\t= 320 local initY\t= 320 --移动方向 local dir = 1 --贪吃蛇集合 local snakes = {} --食物状态 --WaitToEat：绘制食物 --BeEated：随机生成食物 local foodState = \u0026#34;WaitToEat\u0026#34; --游戏状态 --0：游戏结束 --1：游戏正常 local gameState = 1 --食物的位置 local foodX = 0 local foodY = 0 --Love2D加载事件 function love.load() --设置窗口标题 love.window.setTitle(\u0026#34;Love2D-贪吃蛇游戏\u0026#34;) --设置窗口大小 love.window.setMode(w,h) --定义字体 local myFont = love.graphics.newFont(30) --设置字体 love.graphics.setFont(myFont) --设置背景色 love.graphics.setBackgroundColor(255,255,255,255) --设置线条类型为平滑 love.graphics.setLineStyle(\u0026#34;smooth\u0026#34;) --设置线宽 love.graphics.setLineWidth(0.1) --蛇的初始化(蛇的长度为5) for i=1,5 do local snake = { } snake.x = initX + (i-1) * 20 snake.y = initY snakes[i] = snake end --食物初始化 foodX = love.math.random(32-1)*20 foodY = love.math.random(32-1)*20 end --Love2D绘制事件 function love.draw() --绘制竖线 love.graphics.setColor(0,0,0,255) for i = 0, w, unitSize do love.graphics.line(0,i,h,i) end --绘制横线 for j = 0, h, unitSize do love.graphics.line(j,0,j,w) end --绘制蛇 for i = 1,table.maxn(snakes) do love.graphics.setColor(0,0,255,255) love.graphics.rectangle(\u0026#34;fill\u0026#34;,snakes[i].x,snakes[i].y,20,20) end --绘制食物 if(foodState == \u0026#34;WaitToEat\u0026#34;) then love.graphics.setColor(255,0,0,255) love.graphics.rectangle(\u0026#34;fill\u0026#34;,foodX,foodY,20,20) end --如果游戏结束则显示GameOver if(gameState == 0) then love.graphics.setColor(255,0,0,255) love.graphics.print(\u0026#34;Game Over\u0026#34;,250,300) end end -- function love.update(dt) --判断游戏状态 if (snakes[1].x \u0026lt;= 0 or snakes[1].x \u0026gt;= 640 or snakes[1].y \u0026lt;= 0 or snakes[1].y \u0026gt;= 640) then gameState = 0 else gameState = 1 end --如果游戏状态为正常 if (gameState == 1) then SnakeUpdate() FoodUpdate() end end --核心算法——蛇的移动 function SnakeUpdate(dt) --获取元素个数 local n = table.maxn(snakes) if(table.maxn(snakes) \u0026gt; 0) then if(getNextPoint().x == foodX and getNextPoint().y == foodY) then --将下一个目标点的位置插入表中 table.insert(snakes, 1, getNextPoint()) --将食物状态设置为BeEated foodState=\u0026#34;BeEated\u0026#34; else --将下一个目标点的位置插入表中 table.insert(snakes, 1, getNextPoint()) --移除最后一个元素 table.remove(snakes,n+1) end end end --随机生成食物 function FoodUpdate() --如果食物被蛇吃掉则重新生成食物 if(foodState == \u0026#34;BeEated\u0026#34;) then foodX=love.math.random(32-1)*20 foodY=love.math.random(32-1)*20 foodState = \u0026#34;WaitToEat\u0026#34; end end --根据玩家按下的键位定义不同的方向 function love.keypressed(key) if (key == \u0026#34;a\u0026#34;) then dir = 2 end if (key == \u0026#34;d\u0026#34;) then dir = 3 end if (key == \u0026#34;w\u0026#34;) then dir = 0 end if (key==\u0026#34;s\u0026#34;) then dir = 1 end end --计算下一个目标点 function getNextPoint() --计算下一个目标点 local snake = {} if(dir == 0) then snake.x = snakes[1].x snake.y = snakes[1].y - 20 end if(dir == 1) then snake.x = snakes[1].x snake.y = snakes[1].y + 20 end if(dir == 2) then snake.x=snakes[1].x - 20 snake.y=snakes[1].y end if(dir == 3) then snake.x = snakes[1].x + 20 snake.y = snakes[1].y end return snake end 将代码压缩成.love 文件后就可以运行了，我们来看看最终的效果：\nLove2D 贪吃蛇游戏示例1\rLove2D 贪吃蛇游戏示例2\r本文的项目作为开源项目托管在 Github 上，可以通过Github来获取项目源代码。谢谢大家，今天的内容就是这样了。\n",
            "tags": ["Love2D","游戏开发","贪吃蛇"],
            "categories": ["游戏开发"],
            "thumbnail": ""
        }]
} 